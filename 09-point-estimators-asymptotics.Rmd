# Point Estimators: Asymptotics {#point-estimators-asymptotics}

Evaluating the asymptotic properties of point estimators.

## Consistency

An important asymptotic property of an estimator is that it converges in probability to the true value being estimated as $n \rightarrow \infty$. This is called **consistency**. 

### Weak Law of Large Numbers
The most common strategy for proving consistency is to use the Weak Law of Large Numbers.
 
::: {.theorem name="Weak Law of Large Numbers" #wlln}

**iid version**: If $Z_i$ are iid with finite mean, then

$$\bar{X} = \frac{1}{n}\sum_{i=1}^nX_i\overset{p}{\rightarrow}E(X_i)$$

**Non-iid version**: Alternatively, we can drop the iid assumption; if $X_i$ have finite mean and variance, $Cov(X_i, X_j) = 0$, and $\lim_{n\rightarrow\infty}\sum_{i=1}^n\frac{\sigma_i^2}{n^2} = 0$, then

$$\frac{1}{n}\sum_{i=1}^nX_i - \frac{1}{n}\sum_{i=1}^nE(X_i) \overset{\mathcal{p}}{\rightarrow} 0$$
:::

If a statistic can be rewritten in the form of a sample mean, the WLLN can be combined with the Continuous Mapping Theorem to show a variety of estimators are consistent. One hint that the WLLN (or later, the Central Limit Theorem) may be employed is that the expression to be proven contains a sum.

### Direct Proof via Convergence in Probability
Sometimes, a statistic may not consistent of a sum of random variables. This often arises in the case of [order statistics](#order-statistics). In this case, the definition of convergence in probability must be used directly. To do this, a variety of arguments may be employed.

::: {.example name="How To Prove Convergence of a Sample Maximum"}
Let $X_i \overset{iid}{\sim} U(a, b)$. We can prove that $X_{(n)}\overset{p}{\rightarrow}b$ via a [direct probability](#direct-manipulation) argument using [disjointification](#disjointify). By the definition of the order statistic cdf,

$$
P(|X_{(n)} - b| > \varepsilon) = P(X_{(n)} > \varepsilon + b) + P(X_{(n)} < b - \varepsilon) \\
= 0 + (F_{X_i}(x))^n = (\frac{b - \varepsilon - a}{b - a})^n = (1 -\frac{\varepsilon}{b - a})^n
$$
The 0 arises because $X_{(n)} < b$ by the definition of the Uniform. As $\varepsilon$ is taken to be small, $\lim_{n\rightarrow \infty}(1 -\frac{\varepsilon}{b - a})^n = 0$. Therefore, since $P(|X_{(n)} - b| > \varepsilon) = 0$, we have proven $X_{(n)}\overset{p}{\rightarrow}b$

:::

## Asymptotic Efficiency

In addition to checking that an estimator converges to the correct value, we are also often concerned with the estimator's *variance* as $n \rightarrow \infty$. Often, estimators converge asymptotically to a normal distribution. If an estimator $T_n(X)$ has the property $k_n(T_n(X) - \tau(\theta)) \overset{\mathcal{D}}{\rightarrow} N(0,\sigma^2)$, then $\sigma^2$ is called the *asymptotic variance* [@Casella1990].

Furthermore, $T_n(X)$ is **asymptotically efficient* if $\sigma^2$ achieves the Cramer-Rao Lower Bound. If $T_n(X)$ is one-dimensional, then the **asymptotic efficiency** of $T_n(X)$ can be computed as the ratio

$$AE(\theta, T_n) = \frac{(\tau'(\theta))^2}{\mathcal{I}(\theta)\sigma^2}$$
If $T_n(X)$ is $k$-dimensional, let $d = \begin{bmatrix}\frac{\partial}{\partial\theta_1}\tau(\theta),...,\frac{\partial}{\partial\theta_k}\tau(\theta)\end{bmatrix}$. Then, the asymptotic efficiency is 

$$AE(\theta, T_n) = \frac{d'\mathcal{I}(\theta)^{-1}d}{\sigma^2}$$

We can also compare estimators via their **asymptotic relative efficiency (ARE)**, which for estimators $S_n$ and $T_n$ is 

$$ARE(\theta, S_n, T_n) = \frac{\sigma_T^2}{\sigma_S^2}$$

Let's explore first how we might prove that an estimator converges asymptotically to a normal distribution to show this directly.


### Central Limit Theorems

These theorems are used to show asymptotic normality. There are many different types; let us focus on the three most common.


::: {#clt .theorem name="Central Limit Theorem (iid)"}

If $X_i$ are iid with finite first and second moments ($E(X_i), Var(X_i) < \infty$)  then

$$\sqrt{n}\Big(\frac{1}{n}\sum_{i=1}^nX_i - E(X_i)\Big) \overset{\mathcal{D}}{\rightarrow} N(0,Var(X_i))$$

:::

::: {#clt .theorem name="Lyapunov's Central Limit Theorem (non-iid)"}

Consider $X_i$ that are independent with finite first and second moments ($E(X_i), Var(X_i) < \infty$). Let $S_n^2 = \sum_{i=1}^n Var(X_i)$. Then, for some $\delta > 0$, if the condition

$$\lim_{n\rightarrow \infty} \frac{1}{S_n^{2 + \delta}}\sum_{i=1}^n E(|X_i - E(X_i)|^{2 + \delta}) = 0$$

holds, then we know

$$\frac{1}{S_n}\sum_{i=1}^n (X_i - E(X_i)) \overset{\mathcal{D}}{\rightarrow} N(0,1)$$
:::

In practice, to prove that Lyapunov's CLT holds true, we typically take $\delta = 1$ and compute the third moments contained in the Lyapunov condition. Alternatively, we can use Lindeberg's CLT for non-iid data:

::: {#clt .theorem name="Lindeberg's Central Limit Theorem (non-iid)"}
Like the Lyapunov CLT, consider $X_i$ that are independent with finite first and second moments ($E(X_i), Var(X_i) < \infty$). Let $S_n^2 = \sum_{i=1}^n Var(X_i)$. Then if

$$\lim_{n\rightarrow \infty} \frac{1}{S_n^{2}}\sum_{i=1}^n E((X_i - E(X_i))^{2})\cdot I(|X_i - E(X_i) > \varepsilon S_n) = 0$$
holds, then we know

$$\frac{1}{S_n}\sum_{i=1}^n (X_i - E(X_i)) \overset{\mathcal{D}}{\rightarrow} N(0,1)$$

:::


### The Delta Method

What if our estimator is not a sample mean? In this case, if it is a function of a sample mean, we can still use the Delta Method to prove that it converges asymptotically to either a Normal or a $\chi^2$ distribution.

:::{#delta-method .theorem name="Delta Method"}

If $\sqrt{n}(\hat{\theta}_n - \theta) \overset{\mathcal{D}}{\rightarrow} N(0,\sigma^2)$, then for a continuous $g$ with continuous nonzero derivative in an interval containing $\theta$...

- $\sqrt{n}\Big(g(\hat{\theta}_n) - g(\theta)\Big) \overset{\mathcal{D}}{\rightarrow} N\Big(0, \sigma^2\cdot (\frac{d}{d\theta}g(\theta))^2\Big)$ (First-Order Delta Method)
- $n\Big(g(\hat{\theta}_n) - g(\theta)\Big) \overset{\mathcal{D}}{\rightarrow} \frac{1}{2}\sigma^2\cdot g''(\theta)\cdot\chi^2(1)$ (Second-Order Delta Method)

:::

Generally, the Second-Order Delta Method is required if $g'(\theta) = 0$.

:::{#multivariate-delta-method .theorem name="Multivariate Delta Method"}

If $\sqrt{n}(\hat{\theta}_n - \theta) \overset{\mathcal{D}}{\rightarrow} N(0,V)$, where $V$ is a $k \times k$ matrix, then for a continuous real-valued function $g(x)$ of $k$ variables with continuous nonzero first partial derivatives,

$$\sqrt{n}\Big(g(\hat{\theta}_n) - g(\theta)\Big) \overset{\mathcal{D}}{\rightarrow} MVN_k\Big(0, u'Vu\Big)$$

where $u = \begin{bmatrix}\frac{\partial}{\partial \theta_1}g(\theta),...,\frac{\partial}{\partial \theta_k}g(\theta)\end{bmatrix}$

:::

### Cramer-Wold Device

One final technique for proving convergence in distribution is to use the Cramer-Wold Device.

:::{#cramer-wold-device .theorem name="Cramer Wold Device"}

For a sequence of random vectors $X_n$, a random vector $X$, and a vector $a$ of constants,

$$X_n \overset{\mathcal{D}}{\rightarrow} \iff a'X_n \overset{\mathcal{D}}{\rightarrow}a'X, \forall a$$
In other words, we can show convergence in distribution of a random vector by showing that every linear combination of that random vector converges in distribution to the same linear combination of $X$.

:::

The Cramer-Wold Device allows us to convert a problem involving convergence of random vectors into a problem involving convergence of a single random variable.

### Asymptotic Distribution in Practice.

Statistical problems are often more complicated than simply applying the CLT or Delta Method. Often, we may need to consider the convergences of other random variables. To do this, we can combine the CLT/Delta Method with Slutsky's Theorem or the Continuous Mapping Theorem to prove desired results.


## Asymptotic Properties of MLEs

While the CLT and Delta Method are extremely useful, if you are working with MLEs, it can often be faster to rely on their known properties. For $X_i$ satisfying a certain set of [regularity conditions](https://myweb.uiowa.edu/pbreheny/7110/wiki/regularity-conditions.html) (where? better link), the MLE $\hat{\theta}_n has the two import properties discussed above:

- **Consistency**: $\hat{\theta}_n \overset{p}{\rightarrow} \theta$
- **Asymptotic Efficiency**: $\sqrt{n}(\hat{\theta}_n - \theta) \overset{\mathcal{D}}{\rightarrow} N(0, \mathcal{I}(\theta_0)^{-1})$, the [Cramer-Rao Lower Bound](#crlb)

This holds true even for multi-parameter MLEs. Note that the regularity conditions are met by the MLE of all exponential families for which $\nu \in \Theta \subset \mathbb{R}$ is an open set, which can be useful in problem-solving.

Estimating $\mathcal{I}(\theta_0)$ can be performed in two ways: (is this correct?)

1. $-\frac{1}{n} \frac{\partial^2}{\partial\theta^2}\ell(\hat{\theta}_n | X_n) \overset{p}{\rightarrow} \mathcal{I}(\theta_0)$
2. $\frac{1}{n} I_n(\hat{\theta}_n) \overset{p}{\rightarrow} \mathcal{I}(\theta_0)$


## Variance Stabilizing Transformations

Sometimes, the asymptotic variance of a distribution may depend on the value of a particular parameter. This may be undesirable because the parameter might be unknown. In this case, we can perform a **variance-stabilizing transformation** to remove the variance.

This is done by setting $f'(\theta) = \frac{c}{\tau(\theta)}$. Then, for $T_n(X)$ such that $\sqrt{n}(T_n - \theta) \overset{\mathcal{D}}{\rightarrow} N(0, \tau^2(\theta))$, by the Delta Method we get

$$\sqrt{n}(f(T_n) - f(\theta)) \overset{\mathcal{D}}{\rightarrow} N(0, f'(\theta)^2 \cdot\tau^2(\theta)) = N(0, c)$$

## Asymptotic Confidence Intervals


