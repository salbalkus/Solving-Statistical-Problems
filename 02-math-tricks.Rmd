# Math Tricks {#math-tricks}

This chapter will cover highly specific mathematical techniques used to solve stats problems but themselves require no statistical knowledge, i.e...

- Exponential function as limits/series
- Inductive integration by parts
- substitution tricks for integration
- binomial theorem
- Gamma function properties
- Matrices (determinants, jacobians, etc.)
- Taylor Series (univariate and multivariate)
- Lagrange multipliers? (Maybe save for MLEs)
- Open versus closed intervals

## Combinatorics

## Geometric Series {#geometric-series}

The **geometric series** is a useful series convergence, defined as follows:

$$\sum_{x=0}^\infty ar^x = \frac{a}{1-r} \text{ for } |r| < 1$$

One place it arises is computing the moments of the [Geometric Distribution](#geometric-distribution).

## Exponential Taylor Series {#exponential-taylor}
Some distributions such as the Poisson rely on infinite sums. Often, we can simplify these infinite sums to an exponential by rewriting them using the following series convergence:

$$\sum_{x=0}^\infty \frac{\lambda^x}{x!} = \exp(\lambda)$$

This is the Taylor series for the exponential function, sometimes called the "exponential series".

## Exponential Limit {#exponential-limit}
Another way to express the exponential function is by the limit

$$\exp(x) = \lim_{n\rightarrow \infty}(1 + \frac{x}{n})^n$$

## Integration by Parts {#ibp}

Another technique, especially for computing moments, is integration by parts, defined by:

$$\int udv = uv - \int vdu$$

Integration by parts is typically used for products of functions. By setting $u$ equal to a function which has a finite number of nonzero derivatives (for example, $x^k$), and $v$ equal to a function which does not (such as $e^x$), this technique can be repeatedly applied to compute integration.

Of course, take caution: If both functions can be repeatedly differentiated infinitely, this may not work! However, it may be possible to use **inductive integration by parts** to prove that if we apply integration by parts infinitely, the result will yield a series which converges to some value *(where?)*

## Leibniz's Rule {#leibniz-rule}

::: {.theorem name="Leibniz's Rule: Simple Version"}

If $a, b$ are constant and $f(x, \theta)$ is differentiable w.r.t to $\theta$, then

$$\frac{d}{d\theta} \int_{a}^{b} f(x,\theta)dx = \int_a^b \frac{\partial}{\partial\theta}f(x,\theta)dx$$
:::

::: {.theorem name="Leibniz's Rule: Complicated Version"}


If $a(\theta)$, $b(\theta)$, and $f(x, \theta)$ are differentiable w.r.t to $\theta$, then 

$$\frac{d}{d\theta}\int_{a(\theta)}^{b(\theta)}f(x,\theta)dx = f(b(\theta), \theta)\frac{d}{d\theta}b(\theta) - f(a(\theta), \theta)\frac{d}{d\theta}a(\theta) + \int_{a(\theta)}^{b(\theta)} \frac{\partial}{\partial\theta}f(x,\theta)dx$$
:::

## Gamma Function {#gamma-function}

Defined as 

$$\Gamma(z) = \int_{0}^\infty t^{z-1}e^{-t}dt$$

the Gamma function commonly appears in the probability density function of several random variables, including the Gamma (duh!) and the Beta.

The most important property of $\Gamma(z)$ is that 

$$\Gamma(z + 1) = z\Gamma(z)$$
This is because we can substitute $\Gamma(z) = \frac{\Gamma(z + 1)}{z}$ to perform the [Kernel Technique](#kernel-technique) on distributions involving the Gamma function in their pdf

As a corollary, when $n$ is an integer, $\Gamma(n) = (n-1)!$. 

## Triangle Inequality {#triangle-inequality}

The triangle inequality is defined as 

$$|x + y| \leq |x| + |y|$$
with $|x + y| \leq |x| + |y|$ unless $x, y \geq 0$. This inequality is useful for proving [moment bounds](#moment-bounds) as well as asymptotic convergence in [Chapter 9](point-estimators-asymptotics) and [Chapter 11](hypothesis-tests-asymptotics).

A general trick for proving the triangular inequality that may be useful in other problems involving absolute value can be termed the "square trick" - noting $x^2 = |x|^2$, we can first show the equality of a square, then take the square root to obtain $|x|$. Here is a proof of the triangle inequality using this trick:

1. Note $(a + b)^2 = |a + b|^2 \leq 0$

2. Rearrange $(a + b)^2$ and 

$$
(a + b)^2 = a^2 + 2ab + b^2 = |a|^2 + 2ab + |b|^2\\
\leq |a|^2 + 2|a||b| + |b|^2 = (|a| + |b|)^2\\
\implies |a + b|^2 \leq (|a| + |b|)^2\\
$$
3. Take the square root of the final implication to get $|a + b| \leq |a| + |b|$


## Fubini's Theorem

One useful problem-solving technique is exchanging the order of integrals or infinite series to more easily solve an equation. But when can we do this?

::: {.theorem name="Fubini's Theorem: Integrals"}

$\int_\mathcal{X}\int_\mathcal{Y}f(x,y)dydx = \int_\mathcal{Y}\int_\mathcal{X}f(x,y)dxdy$ if $\int_\mathcal{X}\int_\mathcal{Y}|f(x,y)|dydx < \infty$

that is, if the integral of its absolute value is finite.

:::

::: {.theorem name="Fubini's Theorem: Infinite Series"}

$\sum_{m=1}^\infty\sum_{n=1}^\infty a_{m,n} = \sum_{n=1}^\infty\sum_{m=1}^\infty a_{m,n}$ if $\sum_{m=1}^\infty\sum_{n=1}^\infty |a_{m,n}| < \infty$ 

that is, if the original series is absolutely convergent

:::



