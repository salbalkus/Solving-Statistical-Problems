# Math Tricks {#math-tricks}

This chapter will cover highly specific mathematical techniques used to solve stats problems but themselves require no statistical knowledge, i.e...

- Exponential function as limits/series
- Inductive integration by parts
- substitution tricks for integration
- binomial theorem
- Gamma function properties
- Matrices (determinants, jacobians, etc.)
- Taylor Series (univariate and multivariate)
- Lagrange multipliers? (Maybe save for MLEs)
- Open versus closed intervals

## Combinatorics

## Geometric Series (#geometric-series)

The **geometric series** is a useful series convergence, defined as follows:

$$\sum_{x=0}^\infty ar^x = \frac{a}{1-r} \text{ for } |r| < 1$$

One place it arises is computing the moments of the [Geometric Distribution](#geometric-distribution).


## Leibniz's Rule {#leibniz-rule}

::: {.theorem name="Leibniz's Rule: Simple Version"}

If $a, b$ are constant and $f(x, \theta)$ is differentiable w.r.t to $\theta$, then

$$\frac{d}{d\theta} \int_{a(\theta)}^{b(\theta)} f(x,\theta)dx = \int_a^b \frac{\partial}{\partial\theta}f(x,\theta)dx$$
:::

::: {.theorem name="Leibniz's Rule: Complicated Version"}


If $a(\theta)$, $b(\theta)$, and $f(x, \theta)$ are differentiable w.r.t to $\theta$, then 

$$\int_{a(\theta)}^{b(\theta)}f(x,\theta)dx = f(b(\theta), \theta)\frac{d}{d\theta}b(\theta) - f(a(\theta), \theta)\frac{d}{d\theta}a(\theta) + \int_{a(\theta)}^{b(\theta)} \frac{\partial}{\partial\theta}f(x,\theta)dx$$
:::

## Gamma Function {#gamma-function}

Defined as 

$$\Gamma(z) = \int_{0}^\infty t^{z-1}e^{-t}dt$$

the Gamma function commonly appears in the probability density function of several random variables, including the Gamma (duh!) and the Beta.

The most important property of $\Gamma(z)$ is that 

$$\Gamma(z + 1) = z\Gamma(z)$$
This is because we can substitute $\Gamma(z) = \frac{\Gamma(z + 1)}{z}$ to perform the [Kernel Technique](#kernel-technique) on distributions involving the Gamma function in their pdf

As a corollary, when $n$ is an integer, $\Gamma(n) = (n-1)!$. 

## Triangle Inequality {#triangle-inequality}

The triangle inequality is defined as 

$$|x + y| \leq |x| + |y|$$
with $|x + y| \leq |x| + |y|$ unless $x, y \geq 0$. This inequality is useful for proving [moment bounds](#moment-bounds) as well as asymptotic convergence in [Chapter 9](point-estimators-asymptotics) and [Chapter 11](hypothesis-tests-asymptotics)
