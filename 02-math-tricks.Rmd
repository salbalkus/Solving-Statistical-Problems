# Math Tricks {#math-tricks}

This chapter will cover highly specific mathematical techniques used to solve stats problems but themselves require no statistical knowledge, i.e...

- Exponential function as limits/series
- Inductive integration by parts
- substitution tricks for integration
- binomial theorem
- Gamma function properties
- Matrices (determinants, jacobians, etc.)
- Taylor Series (univariate and multivariate)
- Lagrange multipliers? (Maybe save for MLEs)
- Open versus closed intervals

## Combinatorics

## Geometric Series {#geometric-series}

The **geometric series** is a useful series convergence, defined as follows:

$$\sum_{x=0}^\infty ar^x = \frac{a}{1-r} \text{ for } |r| < 1$$

One place it arises is computing the moments of the [Geometric Distribution](#geometric-distribution).

## Exponential Taylor Series {#exponential-taylor}
Some distributions such as the Poisson rely on infinite sums. Often, we can simplify these infinite sums to an exponential by rewriting them using the following series convergence:

$$\sum_{x=0}^\infty \frac{\lambda^x}{x!} = \exp(\lambda)$$

This is the Taylor series for the exponential function, sometimes called the "exponential series".

## Exponential Limit {#exponential-limit}
Another way to express the exponential function is by the limit

$$\exp(x) = \lim_{n\rightarrow \infty}(1 + \frac{x}{n})^n$$

## Integration by Parts {#ibp}

Another technique, especially for computing moments, is integration by parts, defined by:

$$\int udv = uv - \int vdu$$

Integration by parts is typically used for products of functions. By setting $u$ equal to a function which has a finite number of nonzero derivatives (for example, $x^k$), and $v$ equal to a function which does not (such as $e^x$), this technique can be repeatedly applied to compute integration.

Of course, take caution: If both functions can be repeatedly differentiated infinitely, this may not work! However, it may be possible to use **inductive integration by parts** to prove that if we apply integration by parts infinitely, the result will yield a series which converges to some value *(where?)*


## Leibniz's Rule {#leibniz-rule}

::: {.theorem name="Leibniz's Rule: Simple Version"}

If $a, b$ are constant and $f(x, \theta)$ is differentiable w.r.t to $\theta$, then

$$\frac{d}{d\theta} \int_{a(\theta)}^{b(\theta)} f(x,\theta)dx = \int_a^b \frac{\partial}{\partial\theta}f(x,\theta)dx$$
:::

::: {.theorem name="Leibniz's Rule: Complicated Version"}


If $a(\theta)$, $b(\theta)$, and $f(x, \theta)$ are differentiable w.r.t to $\theta$, then 

$$\int_{a(\theta)}^{b(\theta)}f(x,\theta)dx = f(b(\theta), \theta)\frac{d}{d\theta}b(\theta) - f(a(\theta), \theta)\frac{d}{d\theta}a(\theta) + \int_{a(\theta)}^{b(\theta)} \frac{\partial}{\partial\theta}f(x,\theta)dx$$
:::

## Gamma Function {#gamma-function}

Defined as 

$$\Gamma(z) = \int_{0}^\infty t^{z-1}e^{-t}dt$$

the Gamma function commonly appears in the probability density function of several random variables, including the Gamma (duh!) and the Beta.

The most important property of $\Gamma(z)$ is that 

$$\Gamma(z + 1) = z\Gamma(z)$$
This is because we can substitute $\Gamma(z) = \frac{\Gamma(z + 1)}{z}$ to perform the [Kernel Technique](#kernel-technique) on distributions involving the Gamma function in their pdf

As a corollary, when $n$ is an integer, $\Gamma(n) = (n-1)!$. 

## Triangle Inequality {#triangle-inequality}

The triangle inequality is defined as 

$$|x + y| \leq |x| + |y|$$
with $|x + y| \leq |x| + |y|$ unless $x, y \geq 0$. This inequality is useful for proving [moment bounds](#moment-bounds) as well as asymptotic convergence in [Chapter 9](point-estimators-asymptotics) and [Chapter 11](hypothesis-tests-asymptotics)
