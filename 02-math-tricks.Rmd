# Math Tricks {#math-tricks}

This chapter will cover highly specific mathematical techniques used to solve stats problems but themselves require no statistical knowledge, i.e...

- Exponential function as limits/series
- Inductive integration by parts
- substitution tricks for integration
- binomial theorem
- Gamma function properties
- Matrices (determinants, jacobians, etc.)
- Taylor Series (univariate and multivariate)
- Lagrange multipliers? (Maybe save for MLEs)
- Open versus closed intervals

## Combinatorics

## Binomial and Multinomial Theorems

The [binomial theorem](https://en.wikipedia.org/wiki/Binomial_theorem) states that

$$(x + y)^n = \sum_{k=0}^n {n\choose k}x^ky^{n-k}$$
It can be used to prove that the pmf of a [Binomial Random Variable](#binomial-distribution) sums to 1.

Analogously, the [multinomial theorem](https://en.wikipedia.org/wiki/Multinomial_theorem) states

$$(x_1 + x_2 + ... + x_m)^n = \sum_{k_1, k_2,...,k_m \geq 0}\frac{n!}{k_1!k_2!...k_m!}\prod_{i=1}^nx_i^{k_i}$$
provided $\sum_{i=1}^m k_i = n$. It can be used to prove that the pmf of a [Multinomial Random Variable](#multinomial-distribution) sums to 1.

## Geometric Series {#geometric-series}

The **geometric series** is a useful series convergence, defined as follows:

$$\sum_{x=0}^\infty ar^x = \frac{a}{1-r} \text{ for } |r| < 1$$

One place it arises is computing the moments of the [Geometric Distribution](#geometric-distribution).

## Taylor Series for Exponential Function {#exponential-taylor}
Some distributions such as the Poisson rely on infinite sums. Often, we can simplify these infinite sums to an exponential by rewriting them using the following series convergence:

$$\sum_{x=0}^\infty \frac{\lambda^x}{x!} = \exp(\lambda)$$

This is the Taylor series for the exponential function, sometimes called the "exponential series".

## Taylor's Formula

If the $(n+1)$th derivative of a function $f$ exists for all $\xi \in (c, x)$, then

$$
f(x) = f(c) + f'(c)(x-c) + \frac{f''(c)}{2!}(x-c)^2 + ...\\
+ \frac{f^{(n)}(c)}{n!}(x - c)^n + R_n(x)
$$
where 

$$R_n(x) = \frac{f^{(n+1)}(\xi)}{(n+1)!}(x - c)^{n+1}$$

This can be found on page 550 of @Keisler2013. Taylor's formula is key to proofs of several important statistical theorems, including the [Delta Method](#delta-method) and the [asymptotic properties of the MLE](#point-estimators-asymptotics). Do note the following points of confusion:
- Although it is based on Taylor's Formula, this is not identical to the infinite "Taylor Series" used to define functions like the exponential as mentioned previously. Taylor's Formula terminates with $R_n(x)$, a function defining the remaining error when approximating a function by a finite number of derivatives. This property is key, because showing that $R_n(x) \rightarrow 0$ allows asymptotic proofs to be developed.
- The aforementioned asymptotic proofs typically only rely on $n = 1$ or $n = 2$, with the remainder appearing only after the second or third term.


## Exponential Limit {#exponential-limit}
Another way to express the exponential function is by the limit

$$\exp(x) = \lim_{n\rightarrow \infty}(1 + \frac{x}{n})^n$$

## Integration by Parts {#ibp}

Another technique, especially for computing moments, is integration by parts, defined by:

$$\int udv = uv - \int vdu$$

Integration by parts is typically used for products of functions. By setting $u$ equal to a function which has a finite number of nonzero derivatives (for example, $x^k$), and $v$ equal to a function which does not (such as $e^x$), this technique can be repeatedly applied to compute integration.

Of course, take caution: If both functions can be repeatedly differentiated infinitely, this may not work! However, it may be possible to use **inductive integration by parts** to prove that if we apply integration by parts infinitely, the result will yield a series which converges to some value *(where?)*

## Leibniz's Rule {#leibniz-rule}

::: {.theorem name="Leibniz's Rule: Simple Version"}

If $a, b$ are constant and $f(x, \theta)$ is differentiable w.r.t to $\theta$, then

$$\frac{d}{d\theta} \int_{a}^{b} f(x,\theta)dx = \int_a^b \frac{\partial}{\partial\theta}f(x,\theta)dx$$
:::

::: {.theorem name="Leibniz's Rule: Complicated Version"}


If $a(\theta)$, $b(\theta)$, and $f(x, \theta)$ are differentiable w.r.t to $\theta$, then 

$$\frac{d}{d\theta}\int_{a(\theta)}^{b(\theta)}f(x,\theta)dx = f(b(\theta), \theta)\frac{d}{d\theta}b(\theta) - f(a(\theta), \theta)\frac{d}{d\theta}a(\theta) + \int_{a(\theta)}^{b(\theta)} \frac{\partial}{\partial\theta}f(x,\theta)dx$$
:::

## Fubini's Theorem

One useful problem-solving technique is exchanging the order of integrals or infinite series to more easily solve an equation. But when can we do this?

::: {.theorem name="Fubini's Theorem: Integrals"}

$\int_\mathcal{X}\int_\mathcal{Y}f(x,y)dydx = \int_\mathcal{Y}\int_\mathcal{X}f(x,y)dxdy$ if $\int_\mathcal{X}\int_\mathcal{Y}|f(x,y)|dydx < \infty$

that is, if the integral of its absolute value is finite.

:::

::: {.theorem name="Fubini's Theorem: Infinite Series"}

$\sum_{m=1}^\infty\sum_{n=1}^\infty a_{m,n} = \sum_{n=1}^\infty\sum_{m=1}^\infty a_{m,n}$ if $\sum_{m=1}^\infty\sum_{n=1}^\infty |a_{m,n}| < \infty$ 

that is, if the original series is absolutely convergent

:::

## Gamma Function {#gamma-function}

Defined as 

$$\Gamma(z) = \int_{0}^\infty t^{z-1}e^{-t}dt$$

the Gamma function commonly appears in the probability density function of several random variables, including the Gamma (duh!) and the Beta.

The most important property of $\Gamma(z)$ is that 

$$\Gamma(z + 1) = z\Gamma(z)$$
This is because we can substitute $\Gamma(z) = \frac{\Gamma(z + 1)}{z}$ to perform the [Kernel Technique](#kernel-technique) on distributions involving the Gamma function in their pdf

As a corollary, when $n$ is an integer, $\Gamma(n) = (n-1)!$. 

## Triangle Inequality {#triangle-inequality}

The triangle inequality is defined as 

$$|x + y| \leq |x| + |y|$$
with $|x + y| \leq |x| + |y|$ unless $x, y \geq 0$. This inequality is useful for proving [moment bounds](#moment-bounds) as well as asymptotic convergence in [Chapter 9](point-estimators-asymptotics) and [Chapter 11](hypothesis-tests-asymptotics).

A general trick for proving the triangular inequality that may be useful in other problems involving absolute value can be termed the "square trick" - noting $x^2 = |x|^2$, we can first show the equality of a square, then take the square root to obtain $|x|$. Here is a proof of the triangle inequality using this trick:

1. Note $(a + b)^2 = |a + b|^2 \leq 0$

2. Rearrange $(a + b)^2$ and 

$$
(a + b)^2 = a^2 + 2ab + b^2 = |a|^2 + 2ab + |b|^2\\
\leq |a|^2 + 2|a||b| + |b|^2 = (|a| + |b|)^2\\
\implies |a + b|^2 \leq (|a| + |b|)^2\\
$$
3. Take the square root of the final implication to get $|a + b| \leq |a| + |b|$

## Constrained Optimization

Consider a vector $\theta = (\theta_1, \theta_2, ..., \theta_n)$. Suppose we want to maximize some function $\mathcal{L}(\theta)$, *but* subject to a constraint $g(\theta) = c$. How can this be accomplished using calculus? To do this, we rely on **Lagrange Multipliers**. The Lagrange function consists of the equation

$$L(\theta, \lambda) = \mathcal{L}(\theta) - \lambda(g(\theta) - c)$$
Rather than optimizing $\mathcal{L}(\theta)$ using the First/Second Derivative test as we would have before, we instead optimize $L(\theta, \lambda)$ over $\theta$ and $\lambda$, finding where $\nabla L(\theta, \lambda) = 0$.  We can even add multiple $\lambda_i, i = 1, 2,...$ to account for multiple constraints.

Why does this work? Suppose we have some $L(\theta_1, \lambda_2)$ with $g(\theta_1, \theta_2) = c$. Then, the location where $L_x = L_y = L_\lambda = 0$ occurs where

$$
\frac{\partial}{\partial\theta_1}L = \frac{\partial}{\partial \theta_1}\mathcal{L}(\theta) - \lambda\frac{\partial}{\partial \theta_1}g(\theta_1, \theta_2)\\
\frac{\partial}{\partial\theta_2}L = \frac{\partial}{\partial \theta_2}\mathcal{L}(\theta) - \lambda\frac{\partial}{\partial \theta_2}g(\theta_1, \theta_2)\\
\frac{\partial}{\partial\lambda}L = -g(\theta_1, \theta_2) + c = 0\\
$$
That last equation guarantees that the constraint is satisfied during maximization. Within a problem, a system of equations will form in $\lambda$; simply solve for each $\theta_i$ in $\lambda$, solve for $\lambda$, and plug in the solution for $\lambda$ into the earlier equations for $\theta$ to get the solution.

Within statistics, this technique arises in [Maximum Likelihood Estimation](#mle).


