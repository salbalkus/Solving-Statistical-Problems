# Statistics {#statistics}

One of the most important tasks we will be performing as statisticians is inference. Inference is the area of statistics which uses sample data to estimate a population parameter. For this it is necessary to *reduce* or *summarize* the recollected data into a measurement that we will refer to as **statistic**. 

::: {.definition name="Statistic"} 
Let $X \sim f(x|\theta)$, where both $X, \theta$ can be vectors. A statistic is a function $T=T(\textbf{X})$ of the sample $\textbf{X}$ from $X$. 
:::

For example, the $T= T(\textbf{X}) = \frac{1}{n} \sum_{i=1}^{n} X_i$ reduces the sample $\textbf{X}$ to a single measurement. 

In general, we would like to choose statistics that satisfy principles that will make inference about the population parameter *easier*. One of these principles is **sufficiency**. The idea behind **sufficiency** is to retain information about the population parameter, say $\theta$, while reducing the data. 

## Sufficient Statistics {#sufficient-stats}

**Sufficiency Principle**. If $T$ is sufficient, then any information about the parameter $\theta$ should depend on $X$ only through $T$. 

::: {.definition name="Sufficient Statistic"} 
Let $X \sim f(x|\theta)$. We say $T=T(X)$ is a sufficient statistic (SS) for $\theta$ if $\mathcal{L}(\theta|X)$ is independent of $\theta$, i.e. $f(x|T;\theta) = g(x)$. 
:::

Let's see some examples. 

::: {.example}
Let $X = (X_1, ..., X_n), X_i$ iid N($\theta$, 1) and $T = \bar{X}$. Recall that $\bar{X} \sim N(\theta, 1/n)$. 
$$
\begin{aligned}
f(x|T, \theta) = n^{-1/2}(2\pi)^{-(\frac{n-1}{2})} exp\{-\sum_{i=1}^{n}(x_i - \bar{x})^2\}
\end{aligned}
$$
Observe that the conditional density is independent of $\theta$. By definition, $T = \bar{X}$ is a sufficient statistic for $\theta$. 
:::


::: {.example} 

Let $X_1, X_2$ iid Poisson($\lambda$). The joint distribution of $X_1, X_2$ is $P(X_1=x_1, X_2 = x_2) = \frac{\lambda^{x_1+x_2}exp(-2\lambda)}{x_1! x_2!}$. Let $T(X_1, X_2) = X_1+X_2$. Observe that $X_1+X_2 \sim$ Poisson(2$\lambda$). Thus, 

$$
\begin{aligned}
P(X_1=x_1, X_2 = x_2|X_1+X_2=t) &= \frac{P(X_1=x_1, X_2 = t - x_1)}{P(X_1 + X_2 = t)} \\
&=\frac{exp(-\lambda)\lambda^{x_1} exp(-\lambda)\lambda^{t -x_1}t!}{x_1!(t-x_1)! exp(-2\lambda)(2\lambda)^t}\\
&= { t \choose x_1} \bigg(\frac{1}{2}\bigg)^t
\end{aligned}
$$
which is independent of $\lambda$ and $X_1+X_2$ is sufficient for $\lambda$. 
:::

As can be seen from the example, we need a candidate statistic to prove sufficiency using the definition. Furthermore, checking sufficiency of a statistics is difficult because we need to compute the conditional distribution. 

::: {.theorem name="Factorization Theorem"}
$T(X)$ is sufficient for $\theta \Longleftrightarrow \exists g(t|\theta)$ and $h(x)$, such that 

$$f(x|\theta)=g(t|\theta)h(x)$$
$\forall x, \theta$.
:::

Note that the factorization theorem tells us that if we can manipulate $f(x|\theta)$ as above, we have a sufficient statistic. 

::: {.example} 
Let $\textbf{X}= (X_1, ..., X_n), X_i$ iid $Poisson(\lambda)$. 
$$
\begin{aligned}
P(X_1=x_1,..., X_n = x_n) &= \frac{exp(-n\lambda)\lambda^{\sum x_i}}{\prod x_i!}\\
&= h(x)g(\sum x_i|\lambda)
\end{aligned}
$$
$\Rightarrow T(X) = \sum X_i$ sufficient for $\lambda$.
:::

::: {.example} 
Let $\textbf{X}= (X_1, ..., X_n), X_i$ iid $Uniform(0,\theta)$.

$$
\begin{aligned}
P_{\theta}(x_1, ..., x_n) &= \frac{1}{\theta^n} \prod_{i=1}^{n}I(0<x_i<\theta)\\
& = \frac{1}{\theta^n} I(x_{(1)}>0) I(x_{(n)}<\theta)
\end{aligned}
$$
$\Rightarrow T(X) = X_{(n)}$ sufficient for $\theta$.
:::

#### Important facts about SS

- Sufficient statistics may or may not reduce the data. 
  - Original data are always sufficient. 
  - In iid sample, the order statistics are sufficient. 
- Sufficient statistics are never unique. 
  - Suppose $X \sim N(0,\sigma^2)$. Then by the factorization theorem $T(X)=X^2, |X|, X^4, exp(X^2)$ are all sufficient. 
  - Any 1-1 function $g$ of a sufficient statistic is also sufficient. 
  
    **Proof**. Let $T^*(X)= g(T(X))$. By assumption $g^{-1}$ exists since g is 1-1. 
    
  $$
  \begin{aligned}
  f(x|\theta) &= g(T(X)|\theta)\\
      &=g(r^{-1}(T^*(x))|\theta)h(x)\\
  \end{aligned}
  $$
    By the factorization theorem, $T^*(X)$ is sufficient for $\theta$. 
  
### Important exponential family result

Consider a sample $X=(X_1,...,X_n)$ from $f_X(x|\theta)= h(x)c(\theta)\exp\Big(\sum_{i=1}^k w_i(\theta)t_i(x)\Big)$. Then $T(X) = \Big(\sum_{i=1}^n t_1(x), ..., \sum_{i=1}^n t_k(x)\Big)$ is a [sufficient statistic](#statistics). Note that this result follows directly from the [factorization theorem](# sufficient-stats). 


### A Note on Distributions of Sufficient Statistics

Recall a convenient property of [exponential families](#exponential-family): that their maximum likelihood estimate is a function of their sufficient statistic. Because of this, in order to prove [Finite Sample Properties](#point-estimators-finite-samples) of an estimator or construct [Hypothesis Tests](#hypothesis-tests-finite-samples), it is often useful to understand their distributions. This is why the distributions of each of the sufficient statistics are included in the fourth column of the table below These distributions are mostly derived from additive, location-scale, and other properties in [Chapter 4 - Known Distributions](#known-distributions)

### Moments of the Sufficient Statistic

As stated by @Casella1990, if $X$ is an exponential family, the moments of its exponential family can be easily computed using certain properties.

::: {.theorem name="I Don't Know What To Call This"}
If $X$ is an [exponential family](#exponential-family), then

$$E\Big(\sum_{i=1}^k \frac{\partial w_i(\theta)}{\partial \theta_j}t_i(X)\Big) = -\frac{\partial}{\partial\theta_j} \log c(\theta)$$
and

$$Var\Big(\sum_{i=1}^k \frac{\partial w_i(\theta)}{\partial \theta_j}t_i(X)\Big) = -\frac{\partial}{\partial\theta_j} \log c(\theta) - E\Big(\sum_{i=1}^k \frac{\partial^2w_i(\theta)}{\partial \theta_j^2}t_i(X)\Big)$$

:::

If $X$ is a [natural exponential family](#natural-exponential-family), then these identities simplify even further.

::: {.theorem name="I Don't Know What To Call This 2"}
If $X$ is a natural exponential family, then$

$$E(t_j(X)) = -\frac{\partial}{\partial\eta_j}\log(c^*(\eta))$$
and

$$Var(t_j(X)) = -\frac{\partial^2}{\partial\eta_j^2}\log(c^*(\eta))$$

:::

Using these theorems, we can calculate the moments of the sufficient statistics of exponential families directly. This is because if $T(X)$ is a sufficient statistic, then $T(X) = \Big(\sum_{i=1}^nt_1(X), ..., \sum_{i=1}^nt_k(X)\Big)$. Suppose, for simplicity, that $T(X)$ is one-dimensional and $X_i$ are iid. Then, if $X_i$ is a natural exponential family,

$$E(T(X)) = E\Big(\sum_{i=1}^nt(X_i)\Big) = nE(t(X_i)) = n\cdot-\frac{\partial}{\partial \eta}\log(c^*(\eta))$$
In fact, letting $A(\eta) = -\log(c^*(\eta))$, we can obtain all of the moments of $T(X)$ by simply differentiating $A(\eta)$

### Table of Sufficient Statistics

Translated from https://en.wikipedia.org/wiki/Exponential_family

| Distribution | Parameter | Sufficient Statistic | S.S. Distribution |
| :---: | :---: | :---: | :---: |
| Bernoulli | p | $\sum_{i=1}^n x_i$ | $Binomial(n, p)$ | 
| Binomial | p | $\sum_{i=1}^n x_i$ | $Binomial(nm, p)$ | 
| Poisson | $\lambda$ | $\sum_{i=1}^n x_i$ | $Poisson(n\lambda)$ | 
| Negative Binomial | p | $\sum_{i=1}^n x_i$ | | 
| Exponential | $\lambda$ | $\sum_{i=1}^n x_i$ | $Gamma(n, \lambda)$ | 
| Normal (known $\sigma^2$) | $\mu$ | $\frac{1}{n}\sum_{i=1}^n x_i$ | $Normal(\mu, \sigma^2/n)$ | 
| Normal | $\mu$, $\sigma^2$ | $(\sum_{i=1}^n x_i, \sum_{i=1}^n x_i^2)$ | |
| Chi-Squared | $\nu$ | $\sum_{i=1}^n \log(x_i)$ | |
| Pareto (known min $x_m$) | $\alpha$ | $\sum_{i=1}^n \log(x_i)$ | | 
| Gamma | $\alpha, \beta$ | $(\sum_{i=1}^n\log(x_i), \sum_{i=1}^n x_i)$ | | 
| Beta | $\alpha, \beta$ | $(\sum_{i=1}^n\log(x_i), \sum_{i=1}^n\log(1 - x_i))$ |  |
| Weibull (known shape $k$) | $\lambda$ | $\sum_{i=1}^n x^k$ |  |


## Minimal Sufficiency {#minimal-sufficiency}

In any setting there are many sufficient statistics. However, we should aim at dealing with the statistic that summarizes the data as concisely as possible. Let $S$ be any a sufficient statistic for $\theta$. In principle, $W=(S,T)$ is also a sufficient statistic, but we rather deal with $S$ reduces the data to one-dimension. When no further reduction from a sufficient statistic is possible, then that statistic is **minimal sufficient**. 

::: {.definition name="Minimal Sufficient Statistic"} 
If $T$ is sufficient for $\theta$, then it is a **minimal sufficient statistic** (MSS) if for any other sufficient statistic $T^*, T$ is a function of $T^*$. 
:::

**Remark:** Of all sufficient statistics, a minimal sufficient statistic offers the maximal reduction of the data. 

Some "intuition" behind this definition goes as follows. A minimal sufficient statistic $T(X)$ creates a partition of the sample space, $\Omega$ into sets $A_t$, where $t \in \mathcal(T) = \{t : t = T(x) \text{ for some } x\in \Omega\}$.  Now consider another sufficient statistic $T^*(X)$ that creates another partition of the sample space such that $A^*_s = \{x: T^*(x)=s\}$. Then for ever $s$ there is a $t$ such that $A^*_s \subset A_t$. Thus the partition associated with the minimal sufficient statistic is the coarsest possible partition for a sufficient statistic. 

As before, the definition for MSS is conceptually useful but it does not help us find a MSS, or how to prove a statistic is a MSS. For this task, we invoke Casella-Berger's Theorem 6.2.13. 

::: {.theorem name="Casella-Berger's Theorem 6.2.13"}
Let $X \sim f(x|\theta)$. Suppose the exists a statistic $T=T(X)$ such that for every $x,y$ in the support of $X$

$$(*) \frac{f(x|\theta)}{f(y|\theta)} = g(x,y) \Longleftrightarrow T(x)=T(y)$$
then $T$ is a MSS. 
:::

In practice this theorem is used both to find a MSS and to prove that a given statistic is a MSS. Let us see some examples of applications of this theorem.

::: {.example}
Let $\textbf{X} = (X_1,..., X_n), X_i$ iid $N(\mu, \sigma^2), \theta = (\mu, \sigma^2)$. Consider $T=(\bar{X}, S^2)$ where $S^2$ is the sample variance. Let $\textbf{x} = (x_1, ..., x_n), \textbf{y} = (y_1, ..., y_n)$. Then, 

$$\frac{f(\textbf{x}|\theta)}{f(\textbf{y}|\theta)} = ... = exp\{-\frac{1}{2\sigma^2}[n(\bar{x}-\bar{y})^2 + 2 n\mu(\bar{x}-\bar{y}) -(n-1)(s^2_x - s^2_y)]\}.$$
$\Rightarrow$ Suppose the ratio is independent of $\theta = (\mu, \sigma^2)$. Then we must have that $\bar{x} = \bar{y}, s^2_x = s^2_y$. 

$\Leftarrow$ Suppose that $T(x)=T(y)$. Then  $\bar{x} = \bar{y}, s^2_x = s^2_y$ and the ratio is 1. 

By the previous theorem we have that $T$ is a MSS.
:::

::: {.example}
Let $\textbf{X} = (X_1,..., X_n), X_i$ iid $U(\theta, \theta + 1)$. 
$$f(\textbf{x}|\theta) = \frac{1}{2^n} I(X_{(1)} > \theta) I(X_{(n)} < \theta + 1)$$
Then, 

$$\frac{f(\textbf{x}|\theta)}{f(\textbf{y}|\theta)} = \frac{I(x_{(1)} > \theta ) I(x_{(n)} < \theta + 1)}{I(y_{(1)} > \theta) I(y_{(n)} < \theta + 1)}$$
and the ratio is independent of $\theta$ if and only if $(x_{(1)}, x_{(n)}) = (y_{(1)}, y_{(n)})$. Thus $T = (X_{(1)}, X_{(n)})$ is MSS. 
:::

Note that in the previous example, $dim(T(X))>dim(\theta)$. This means that there is no estimator of $\theta$ that is sufficient. 

::: {.theorem} 
If $X_1,..., X_n (n\ge 1)$ are iid with $X_i \sim k$-parameter exponential family, then $T(X) = \Big(\sum_{i=1}^n t_1(x), ..., \sum_{i=1}^n t_k(x)\Big)$ is a MSS. 
:::

**Fact**: Like sufficient statistics, MSSs are not unique. Any 1-1 function of a MSS is also a MSS. 

## Ancillary Statistics {#ancillary-stats}

Sufficiency describes where all the information in the data is contained. Ancillarity is the dual of sufficiency, describing where there is no information. 

::: {.definition name="Ancillary Statistic"}
Let $X \sim f(x|\theta)$. The statistic $S(X)$ is **ancillary** for $\theta$ if $\mathcal{L}(S), g(s|\theta)$ are independent of $\theta$, i.e. for any $\theta_1, \theta_2 \in \Theta$, 
$$g(s|\theta_1)=g(s|\theta_2) \text{ for all } s.$$ 
:::

::: {.example}
Let $\textbf{X} = (X_1,..., X_n), X_i$ iid $U(\theta, \theta + 1)$. Consider the order statistics of the sample $\textbf{X}$. The range statistic $R = X_{(n)} - X_{(1)}$ is ancillary for $\theta$ by showing that the pdf of $R$ is independent of $\theta$. Intuitive, the range does not tell anything about the location of $\theta$ in the real line. In this case the ancillarity of $R$ does not depend on the uniformity of the observations, but on the parameter of the distribution being a location parameter. 
:::

::: {.theorem name="Location family ancillary statistic"}
Let $\textbf{X} = (X_1,..., X_n), X_i$ iid $f_{\mathcal{L}}(x|\theta)$, a [location family](#location-scale), then $R = X_{(n)} - X_{(1)}$ is ancillary for $\theta$. 
:::

::: {.theorem name="Scale family ancillary statistic"}
Let $\textbf{X} = (X_1,..., X_n), X_i$ iid $f_{\mathcal{S}}(x|\sigma)$, a [scale family](#location-scale), and let $S_j = X_j/X_n$. Then $S=(S_1, S_2,..., S_n)$ is ancillary for $\sigma$. 
:::

**Note:** Since $S$ is ancillary, any function of $S=(S_1, S_2,..., S_n)$ is also ancillary. For example, $S_1 + ... + S_n$ is also ancillary. 


::: {.theorem name="Location-Scale family ancillary statistic"}
Let $\textbf{X} = (X_1,..., X_n), X_i$ iid $f_{\mathcal{L,S}}(x|\mu,\sigma)$, a [location-scale family](#location-scale) where $\theta = (\mu, \sigma)$. Let $T_1= T_1(\textbf{X})$ and $T_2= T_2(\textbf{X})$ be any two statistics such that

$$ (*) \quad T_j(aX_1 + b, ..., aX_n+b) = aT_j(X_1,..., X_n) \quad j = 1,2.$$

Then, $T_1/T_2$ is ancillary for $\theta$
:::

::: {.example}
Consider the range $R = X_{(n)} - X_{(1)}$ and $S = \sqrt{\frac{1}{n-1}\sum(X_1 -\bar{X})^2}$ satisfy (*) so $R/S$ is ancillary for $\theta = (\mu, \sigma)$. 
:::

### Why are we interested in ancillary statistics?

There is some relationship between ancillarity and minimal sufficiency. Suppose there is a statistic $c=C(\textbf{X})$ and an ancillary statistic $S=S(\textbf{X})$ such that $T=(S,C)$ is minimal sufficient. The **ancillarity principle** states that inference on the parameter should be based on the conditional distribution of $C$ given the ancillary statistic $S$. Furthermore, we will see later that ancillary statistics are independent of complete and sufficient statistics. 


**Remark:** Recall the $U(\theta, \theta + 1)$ example in which we found that the MSS, T, is two dimensional, and therefore no sufficient estimator for $\theta$ exists. We can write $T = (S,C)$ where $C$ has a marginal distribution that is ancillary (independent of the parameter), and then $S$ is conditionally sufficient, i.e. sufficient conditional on $C$. 

::: {.example}
Let $N$ be a random variable with known distribution, $P(N=n) = p_n$, and let $X_1, ..., X_N$ be iid with exponential family density. Then the likelihood of the data, $(N, X_1,..., X_N)$ is

$$p_n (\prod_{i=1}^{n}h(x_i))c(\theta)^{n} exp\bigg\{\sum_{j=1}^{k} w_k(\theta) \sum_{i=1}^{n}t_j(x_i)\bigg\} $$
and thus $\bigg\{N, \{\sum_{i=1}^{N}t_j(x_i)\}_{j=1}^{k}\bigg\}$ is sufficient for $\theta$. Observe that $N$ is ancillary (independent of $\theta$) and $\{\sum_{i=1}^{N}t_j(x_i)\}_{j=1}^{k}$ is sufficient for $\theta$ conditional on $N$. 
:::

**Remark:** Ancillary statistics may not be unique, and there is no general method for constructing them.

::: {.example name="Bivariate normal with unknown correlation"}
Suppose that $(X_1, Y_1), ..., (X_n, Y_n)$ are iid bivariate normal with zero means and unit variances, and unknown correlations $\rho$. Thus their joint pdf is

$$\frac{1}{(2\pi)^{n}(1-\rho^2)^{n/2}} exp \bigg\{ -\frac{\sum(x_i^2+y_i^2)}{2(1-\rho^2)} + \frac{\rho \sum x_iy_i}{(1-\rho^2)}\bigg\}.$$
Since this is an exponential family distribution,it is easy to see that the MSS is $T(X,Y) = (\sum(x_i^2+y_i^2), \sum x_iy_i )$. Observe that there is no ancillary statistic that is a function of $T$. However, if we allow ancillary statistics that are not functions of $T$, then both $\sum x_i^2$ and $\sum y_i^2$ are ancillary for $\rho$. But which one do we choose? Any one of them!
:::

## Complete Statistics {#complete-stats}

One of the most important properties for a statistic to have is completeness. Let $X\sim f(x|\theta), \theta \in \Theta$. 

::: {.example name="Complete statistics"}
A statistic $T=T(X)$ (not necessarily sufficient) is **complete** if for any function $g(\cdot)$, 
$$E_{\theta}(g(T)) = 0 \quad \forall \theta \Rightarrow P_{\theta}(g(T) = 0) = 1 \quad \forall \theta.$$
:::

**Note:** The family of pdfs of $T$ is called complete. 

**Remark:** The definition says that of $T$ is complete, then only function of $T$ that can have zero expectation on $\theta$ is the zero function, i.e. there can't be any non-zero unbiased estimates of 0 based on $T$. 

::: {.example name="Polynomial technique"}
Let $\textbf{X}=(X_1,..., X_n), X_i$ iid $Bern(p)$. Consider $T = \sum_{i=1}^{n}X_i$. Note that $T\sim Bin(n,p)$ as it is the sum of idd Bernoulli random variables. Suppose we have a $g(\cdot)$ function that satisfies the definition of complete statistic. Then $T$ would be complete. 

Now what does $E_{\theta}(g(T)) = 0 \quad \forall p$ mean?

$$ 0 = \sum_{t=0}^{n} g(t) {n \choose t} p^t (1-p)^{n-t} = (1-p)^n \sum_{t=0}^{n} g(t) {n \choose t} \bigg(\frac{p}{1-p}\bigg)^t$$
Observe that for any $t, {n \choose t} \bigg(\frac{p}{1-p}\bigg)^t > 0$. Furthermore $(1-p)^n>0$ for all $n, p, t$. Observe that $g(t)$ is a polynomial of degree $n$ and the only option to satisfy the definition for T to be complete is for $g(t) = 0 \forall t$. Now $P(g(T) = 0)=1 \Rightarrow T$ is complete.
:::

::: {.example}
Let $\textbf{X}=(X_1,..., X_n), X_i$ iid $N(\theta, 1)$. Let $T(X)=(X_1,X_2)$ be a statistic. Observe that if $g(T) = X_1 - X_2$ then $E(g(T)) = E(X_1 - X_2) = 0$ but $P(X_1 - X_2=0)\ne 1$ for all $\theta$. Thus $T$ is not a complete statistic. 
:::

::: {.example }
Let $\textbf{X}=(X_1,..., X_n), X_i$ iid $U(\theta, \theta + 1)$. Recall that $T=T(X) = (X_{(1)}, X_{(n)})$ is MSS for $\theta$. Consider the ancillary statistic $ X_{(n)} -  X_{(1)}$ and $E(X_{(n)} -  X_{(1)}) = c$ for some $c$ independent of $\theta$. Thus, $E(X_{(n)} -  X_{(1)}-c) = 0$ but $P(X_{(n)} -  X_{(1)}-c=0) \ne 1$ for all $\theta$. Therefore $T$ is not a complete statistic. 
:::

**Remark:** If $T$ is sufficient, then it contains all the information regarding $\theta$. If $T$ is also [complete](#complete-stats), then $T$ contains no *irrelevant* information about $\theta$. Intuitively, suppose that $T$ is sufficient and let $T = (T_1, S_1)$ where $S_1$ is ancillary. Then $T$ cannot be complete because it contains irrelevant information about the parameter (recall that ancillary statistics describe where there is no information about $\theta$). 

::: {.theorem}
If $T$ is complete and sufficient and a MSS exists, then $T$ is MSS. 
:::

::: {.example}
Let $\textbf{X}=(X_1,..., X_n), X_i$ iid $U(0, \theta ), \theta >0$. Consider $T = T(X)= X_{(n)}$. Is $T$ complete? Yes. To prove this we will employ the *polynomial technique*. By definition of expectation,
$$ E(g(X_{(n)})) = \int_{0}^{\theta}g(x)f_{X_{(n)}}(x)dx.$$
Recall [order statistics](#order-stats) results, specifically the pdf formula for a given order statistic. 

$$f_{X_{(n)}}(x) = n x^{n-1}\theta^{-n} I(0\le x \le \theta)$$
Now, we have that $E(g(X_{(n)})) = \int_{0}^{\theta}g(x)n x^{n-1}\theta^{-n}dx =n\theta^{-n} \int_{0}^{\theta}g(x)x^{n-1} = 0 \Rightarrow \int_{0}^{\theta}g(x)x^{n-1} = 0$. By applying [Leibnitz' Rule](#math-tricks) wrt $\theta$ we obtain that $g(\theta)\theta^{n-1}=0$, which implies that $g(\cdot)$ must be identically zero because we assumed that $\theta >0$. Therefore $T$ is a complete statistic. 
:::

**Remark:** If the density of $T$ satisfies 
$$f(t|\theta)=h(t)c(\theta)I(0 \le t \le \theta)$$
then $T$ is complete. 

Certainly, the previous remark provides a useful and fast strategy to prove that a statistic is complete. However, as we have mentioned before, exponential family distributions provide what some people would call "easy and fast" strategies. 

::: {.theorem name="Complete statistics in exponential family distributions"}
If $X_1,..., X_n, iid, X_i\sim$ exponential family with pdf/pmf of the form 

$$h(x)c(\theta)\exp\Big(\sum_{i=1}^k w_i(\theta)t_i(x)\Big)$$

and $\mathcal{H}=\{(w_1(\theta), ..., w_n(\theta))|\theta \in \Theta\}$ contains an open set (or open rectangle) in $\mathbb{R}^{k}$, then $T=(T_1,..., T_k)$ is a complete and sufficient statistic, where $T_j = \sum_{i=1}^{n}t_j(x_i)$. 
:::

At a glance, this exponential family theorem on complete and sufficient statistics is great. However, there's a catch: for most of us it is hard to grasp the concept of open set. In this part we will try to provide intuition behind the open set concept. 

- An open $k-$ dimensional ball of radius $r>0$ (also called epsilon-balls) centered at a point $x \in \mathbb{R}^k$ is defined as the set $B(x,r) = \{y| d(x,y) < r\}$, where $d(x,y)$ is a distance function.
    - Observe that what makes the ball *open* is the fact that those points $y$ whose distance to $x$ is exactly $r$ are not in the set.  
    
However, we are interested in a set that contains an open set, like an open ball. Let $A \subset \mathbb{R}^{k}$. We say that the set $A$ contains an open set in $\mathbb{R}^{k}$ if and only is $A$ contains a $k-$dimensional ball B(x,r), i.e. there exists $x \in \mathbb{R}^{k}$ and $r>0$ such that $B(x,r) \subset A$. 

Observe that we do not need our set $A$ to be open, we just need $A$ to *contain* at least one open set in the relevant space. 

For example, in the real line any open interval contains an open set, i.e. $(a,b)$ contains an open subset of $\mathbb{R}$ because for at least point between $a$ and $b$ we can fit an infinitesimally small open ball centered at the point. 

Let us see an example. 

::: {.example}
Recall that the Bernoulli pmf is an exponential family distribution:

$$f(x|\theta) = \theta^x (1-\theta)^{1-x} = (1-\theta)exp\bigg\{ x\text{ }  log\bigg(\frac{p}{1-p}\bigg)\bigg\}, p \in (0,1), x \in \{0,1\}$$
For an iid $Bern(p)$ sample, previous theorems say that $T = \sum_{i=1}^{n} x_i$ is a MSS for $p$. Furthermore, $\mathcal{H} = \{log(p/(1-p))|p \in (0,1)\}$. Does $\mathcal{H}$ contain an open set? Yes. Think about the range of $log(p/(1-p))$ for $p \in (0,1)$. First observe that $p/(1-p) \in (0, \infty)$ which implies that $log(p/(1-p)) \in (-\infty, \infty) = \mathbb{R}$, which certainly contains an open set, $(-1,1)$ for example. Therefore $\mathcal{H}$ contains an open set in $\mathbb{R}$ and we conclude that $T = \sum_{i=1}^{n} x_i$ is a complete statistic.
:::

Most of the time when the statistic T is in fact complete, we can think about the range of the $w_i(\theta)$ functions. If we can find any open set within the range, then we are good to go. However, the task becomes more difficult when the set $\mathcal{H}$ does not contain an open set, because this does **not** necesarily mean that the statistic is not complete. 

::: {.example}
Consider the following pdf

$$f(x|\theta) = \frac{2 exp \{-(x-\theta)^4\}}{\Gamma(1/4)}, -\infty < x < \infty, -\infty < \theta< \infty.$$
Observe that this distribution belongs to the exponential family as we can express the pdf in exponential family form, 

$$f(x|\theta) = \frac{2 exp \{-x^4\}}{\Gamma(1/4)} exp(\theta^4) exp\{(4\theta)x^3 - (6\theta^2) x^2 + (4\theta^3)x\}$$
where $t_1(x) = x^3, t_2(x) = x^2, t_3(x) = x$ and $w_1(\theta) = 4\theta, w_2(\theta)= 6\theta^2, w_3(\theta)=4\theta^3$. Therefore, this is a 3-parameter exponential family, with $dim(\theta)=1$. Now 
$$
\begin{aligned}
\mathcal{H}&=\{(w_1(\theta), w_2(\theta), w_3(\theta)) | w_1(\theta)  = 4\theta, w_2(\theta)= 6\theta^2, w_3(\theta)=4\theta^3, -\infty < \theta< \infty \}\\
& = \{(w_1(\theta), w_2(\theta), w_3(\theta)) | w_1(\theta) \in \mathbb{R}, w_2(\theta)= -(6/16)w_1(\theta)^2, w_3(\theta)=w_1(\theta)^3/16 \}
\end{aligned}
$$
where in the last equality we have that $w_2(\theta), w_3(\theta)$ can be expressed in terms of $w_1(\theta)$. This implies that the set $\mathcal{H}$ is a curve in $\mathbb{R}^3$ and a sphere (open set in $\mathbb{R}^3$) cannot fit in a curve and $\mathcal{H}$ does not contain an open set in $\mathbb{R}^3$. This means that we cannot use the theorem to prove that $T$ is complete (or not complete). In this case we would have to use the definition of [complete statistic](#complete-stat) to verify completeness. 
:::

::: {.example}
Consider the $N(\mu, \sigma^2)$ family with $\theta = (\mu, \sigma^2) \in \Theta$. This family has pdf that can be expressed as an exponential family with $w_1(\theta) = \mu/\sigma^2, w_2(\theta) = -1/2\sigma^2, t_1(x)= x, t_2(x)=x^2$. Let $\textbf{X} = (X_1,...,X_n)$ be iid $N(\mu, \sigma^2)$. By theorem, $T(\textbf{X}) = \sum X_i, \sum X_i^2)$ is a sufficient statistic regardless of the parameter set $\Theta$. Furthermore, observe that $U(\textbf{X}) = (\bar{x}, s^2)$ is a 1-1 function of $T(\textbf{X})$. Let us explore if $T$ (or $U$) is complete. We will see that the application of the CSS for exponential family theorem will depend on the parameter set $\Theta$ and on $\mathcal{H}$.

1. $\Theta_1=\{(\mu, \sigma^2)|\sigma^2>0\} \Rightarrow  \mathcal{H}= \{(\mu/\sigma^2, -1/2\sigma^2) | \sigma^2>0 \}$. Observe that in this case there are no constraints on $\mu$. Thus $\mathcal{H} = \mathbb{R}^2$ which trivially contains an open set of  $\mathbb{R}^2$. In this case we can use the theorem and conclude that $T$ is complete.

2. $\Theta_2=\{(\mu, \sigma^2)|\sigma^2=\sigma_0^2\} \Rightarrow  \mathcal{H}= \{(\mu/\sigma_0^2, -1/2\sigma_0^2)\} = \{(x,-1/2\sigma_0^2)|x\in \mathbb{R}\}$. Now $\sigma^2$ can only take one value, which implies that $\mathcal{H}$ is a line in $\mathbb{R}^2$. Can a disk fit in a line? No, thus we can't use the theorem to assess completeness of $T$. Using the definition of complete statistic one can prove that, in fact, $T$ is not complete in this case.

3. $\Theta_3 = [1,3] \times [4,6]$ implies that $\mu, \sigma^2$ can take values within the closed square that is $\Theta_3$. Does a disk fit into a square? Yes. Thus we can conclude that $T$ is complete.
:::

Hopefully that discussion clarifies when and how to use the theorem on complete statistics in exponential family distributions. Now, let us step back a bit and summarize what we have discussed until now as we present Basu's Lemma, one of the most important theorems in statistical inference.

Let $X \sim f(x|\theta), \theta \in \Theta$.

::: {.theorem name="Basu's Lemma"}
If $T(X)$ is a complete and sufficient statistic (for $\theta \in \Theta$), and $S(X)$ is ancillary, then $T(X)$ and $S(X)$ are independent for all $\theta \in \Theta$. 
:::

**Remark:** This result plainly says that CSSs are independent of ancillary statistics. 

::: {.example}
Let $X_1, ..., X_n$ iid $N(\mu, \sigma^2)$. Suppose that $\sigma^2$ is known. We want to show that $\bar{X}$ and $S^2$ are independent using Basu's Lemma. 
- First, let us show that $\bar{X}$ is a CSS for $\mu$. Observe that the pdf for a $N(\mu, \sigma^2)$ random variable where $\sigma^2$ is known is an exponential family distribution where $t_1(x) = x, w_1(\theta)= \mu/\sigma^2$. Here $\mathcal{H} = \{\mu/\sigma^2 | \mu \in \mathbb{R}\} = \mathbb{R}$ which trially contains an open set. Thus $T(X) = \{\sum X_i\}$ is a CSS. Furthermore consider $S(X) = T(X)/n = \bar{X}$ is a 1-1 function of a CSS and thus is also a CSS. 
- Second, we must prove that $S^2$ is ancillary for $\mu$. By definition, 
$$S^2 = \frac{1}{n-1}\sum_{i=1}^{n}(X_i - \bar{X})^2$$. 
Let $X_i = Z_i + \mu$ where $Z \sim N(0,1)$. Now,

$$
\begin{aligned}
S^2 &= \frac{1}{n-1}\sum_{i=1}^{n}(X_i - \bar{X})^2\\
& = \frac{1}{n-1}\sum_{i=1}^{n}(Z_i + \mu - \frac{\sum(Z_i + \mu)}{n})^2\\
&= \frac{1}{n-1}\sum_{i=1}^{n}(Z_i + \mu - \frac{\sum Z_i}{n} - \mu)^2\\
&= \frac{1}{n-1}\sum_{i=1}^{n}(Z_i - \bar{Z} )^2
\end{aligned}
$$
which is independent of $\mu$. Therefore, $S^2$ is ancillary for $\mu$.

By Basu's theorem we conclude that $\bar{X}$ is independent of $S^2$. Now what happens when $\sigma^2$ is also unknown? Observe that the previous work assumed that $\sigma ^2$ as known, fixed, and *arbitrary*. Thus $\bar{X}$ is independent of $S^2$ even in the case where both $\mu, \sigma^2$ are unknown. 
:::

::: {.example}
In this [example](https://ani.stat.fsu.edu/~debdeep/complete.pdf) we will use Basu's theorem to prove that a statistic is *not* complete. 
Let $\textbf{X} = (X_1,..., X_n), X_i$ iid $U(\theta, \theta + 1)$. In a previous example we saw that $T(X) = (X_{(1)}, X_{(n)})$ is a MSS for $\theta$. Furthermore, we also saw that $R=X_{(n)} - X_{(1)}$ is an ancillary statistic for $\theta$. Observe that $R$ is a function of $T$ and cannot be independent. Thus, by the contrapositive of Basu's Lemma, $T$ cannot be complete. 
:::




