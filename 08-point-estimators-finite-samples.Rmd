# Point Estimators: Finite Samples {#point-estimators-finite-samples}

The goal of parametric statistical inference is to estimate a parameter - or a function of said parameter - of a given distribution using a random sample. To do this, we must construct an [estimator](https://en.wikipedia.org/wiki/Estimator):

::: {.definition name="Estimator"}

An **estimator** is a [statistic](#statistics) $T(X)$ - a function of data - used to obtain an estimate of a parameter $\theta$ or function of a parameter $\tau(\theta)$ in a distribution. 

- An **estimate** is the value of an estimator when computed on a set of data - that is, $T(X)$ for a particular realization of $X$.
- An **estimand** is the parameter $\theta$ or function $\tau(\theta)$ needed to be estimated.

:::

Point estimators estimate scalar or vector real values - as opposed to interval estimators, which estimate intervals. Estimators have various qualities - some are good, and some are bad. This chapter will discuss how to construct and evaluate the quality of estimators.

## Identifiability

First of all, how do we know if it even possible to estimate a given parameter in a distribution? Such a question gives rise to the concept of [identifiability](https://en.wikipedia.org/wiki/Identifiability):

::: {.definition name="Identifiability"}

For a random variable $X$, a parameter $\theta$ in a parameter space $\Theta$ is said to be **identifiable** if, for all $\theta, \theta' \in \Theta$,

$$\theta \neq \theta' \implies F_X(x|\theta) \neq F_X(x|\theta')$$
Alternatively, $\theta$ is *not* identifiable if there *exists* $\theta, \theta' \in \Theta$ such that, for all $x$, 

$$F_X(x|\theta) = F_X(x|\theta')$$
:::

If $\theta$ is non-identifiable, that means two distributions from the same family will be identical under different $\theta$ values. Hence, even if we knew the true underlying distribution of $X$, it would be impossible to know its parameters. Given a random sample, it would be impossible to estimate the parameter accurately.

Non-identifiability usually arises when a distribution is parameterized by some function of multiple parameters. For instance, if $\lambda = (\lambda_1, \lambda_2)$, and $X \sim \text{Exp}(\lambda_1 + \lambda_2)$, it would be impossible to estimate $\lambda_1$ or $\lambda_2$ given $X$ alone since $\lambda_1 + \lambda_2$ can take on the same value for different individual values of $\lambda_1$ and $\lambda_2$. 

Generally, we can assume parameters are identifiable. But how might we show a parameter is not identifiable? One clue to look for is linear dependence.

::: {.example name="Non-Identifiability Via Linear Dependence"}

Non-identifiability can arise in a natural exponential family. Suppose we've parametrized a [natural exponential family](#exponential-family) into the form

$$f(x | \eta) = h(x)c(\theta)\exp(\sum_{i=1}^k \eta_it_i)$$
where the vector $t = (t_1, ..., t_k)$ is linearly dependent. Linear dependence implies that there exists $\alpha = (\alpha_1, ..., \alpha_k)$ not equal to the 0-vector such that $\alpha t = 0$. Then, $\eta$ is not identified?

How do we know? Since $\alpha t = \sum_{i=1}^k \alpha_it_i = 0$, we can add it on the interior of the exponential function:

$$
f(x | \eta) = h(x)c(\theta)\exp(\sum_{i=1}^k \eta_it_i + \sum_{i=1}^k \alpha_it_i)\\
= h(x)c(\theta)\exp(\sum_{i=1}^k (\eta_i + \alpha_i)t_i)

$$
Letting $\eta = (\eta_1 + \alpha_1,..., \eta_k + \alpha_k)$, clearly $f(x|\eta) = f(x|\eta')$ and $\nu$ is not identified.

:::

Non-identifiability can also arise in more practical situations, such as the following:

::: {.example name="Non-Identifiability with Measurement Error"}

Suppose we want to construct a simple linear regression model $Y = \beta X + \varepsilon$ - but there's a twist. Both $Y$ and $X$ are measured with unknown error, so we only observe

$$
Y^* = Y + \epsilon_1\\
X^* = X + \epsilon_2
$$

If we try to fit this linear regression on $Y^*$ and $X^*$ instead, we get

$$
Y + \epsilon_1 = \beta(X + \epsilon_2) + \varepsilon\\
\implies Y = \beta X + (\beta\epsilon_2 - \varepsilon_1 + \varepsilon)
$$

Now, since $\epsilon_1$ and $\epsilon_2$ are unknown, $\beta$ has become unidentifiable. Consider $(\beta', \epsilon_2') \neq (\beta, \epsilon_2)$. Subtracting $Y = \beta' X + (\beta'\epsilon_2' - \varepsilon_1 + \varepsilon)$ from $Y = \beta X + (\beta\epsilon_2 - \varepsilon_1 + \varepsilon)$, we get

$$0 = (\beta - \beta')X + (\beta\epsilon_2 - \beta'\epsilon_2')$$
Multiple values of $\beta$ and $\beta'$ can be chosen to satisfy this equation (since $\epsilon \neq \epsilon'$) meaning that $F_X(x|\beta) = F_X(x|\beta')$ for $\beta \neq \beta'$, proving $\beta$ is not identifiable.

:::

(can we use the MGF to show this as well?)

## Method of Moments

## MLE Theory

:::{.definition name="Likelihood"}

Let $X$ be a random variable or random vector, and $\theta$ a parameter or vector of parameters describing the distribution of $X$. Then, the **likelihood** is

$$\mathcal{L}(\theta | X) = f_X(x|\theta)$$

:::

:::{.definition name="Log-Likelihood"}
Let $X$ be a random variable or random vector, and $\theta$ a parameter or vector of parameters describing the distribution of $X$. Then, the **log-likelihood** is
$$\ell(\theta|X) = \log\mathcal{L}(\theta | X) = \log f_X(x|\theta)$$
:::

The log-likelihood is often easier to use. Furthermore, When $X$ is an iid sample $X_1, X_2, ... X_n$, by the properties of logarithms, $\ell(\theta|X) = \log\Big(\prod_{i=1}^nf_{X_i}(x|\theta)\Big) = \sum_{i=1}^n \log f_{X_i}(x|\theta)$. Working with a sum is much easier than with a product.

Of course, the whole reason this is permissible in finding a maximum likelihood estimate is that the $\log$ function is monotonic - finding the maximum of a function is equivalent to finding the maximum of its logarithm.

Maximizing this likelihood often requires calculus - specifically, the First Derivative Test. In statistics, the gradient of the log-likelihood has a special name: the **score**.

:::{.definition name="Score Equations"}
If $\theta$ is a vector of parameters, the **score equations** are defined as the gradient of the log-likelihood (as described above). That is,

$$U(\theta | X) = \begin{bmatrix}\frac{\partial}{\partial\theta_1}\ell(\theta_1|X) & ... & \frac{\partial}{\partial\theta_k}\ell(\theta_k|X) \end{bmatrix}$$
Of course, if $\theta$ is a single parameter, then $U(\theta|X) = \frac{d}{d\theta}\ell(\theta|X)$

:::

Therefore, the first step in finding an MLE is to set the score to 0, and solve for the desired parameter. Let's see an example.

:::{.example name="MLE of _______"}
example
:::


The score has two useful properties in MLE theory. First, under [regularity conditions](#regularity-conditions), $E(U(\theta|X)) = 0$ - its mean is 0. This is because the regularity conditions imply [Leibniz's rule](#leibniz-rule); that is,

\begin{align}
E(U(\theta|X)) = \int_{\mathcal{X}}\frac{\partial}{\partial\theta}\log\mathcal{L}(\theta|X)f(x|\theta)dx \\
= \int_{\mathcal{X}}\frac{1}{f(x|\theta)}\frac{\partial}{\partial\theta}f(x|\theta)f(x|\theta)dx \\ 
= \frac{\partial}{\partial\theta}\frac{f(x|\theta)}{f(x|\theta)}f(x|\theta)dx = \frac{\partial}{\partial\theta}1 = 0
\end{align}

The second useful property is that, under the same regularity conditions, the variance of the score is

\begin{align}
Var(U(\theta|X)) = E(U(\theta|X)U(\theta|X)^\top) \\
= -E\Big(\frac{\partial^2}{\partial\partial^\top}\ell(\theta|X)\Big) 
\end{align}

If $\theta$ is one-dimensional, $Var(U(\theta|X)) = E(U(\theta|X)^2) = -E\Big(\frac{\partial^2}{\partial\theta^2}\ell(\theta|X)\Big)$. The rather insightful derivation of this is available [here](https://en.wikipedia.org/wiki/Score_(statistics)#Variance). This variance has a special name, the **information** (or sometimes, "Fisher Information")

:::{.definition name="Information"}

In statistics, **information** refers to the amount of information that a random variable $X$ contains about a parameter $\theta$. It is defined mathematically as the variance of the score, which is

$$I(\theta) = E\Big((\frac{\partial}{\partial\theta}\log f(X|\theta))^2\Big|\theta\Big)$$
Under MLE regularity conditions, 

$$I(\theta) = -E\Big(\frac{\partial^2}{\partial\theta^2}\log f(X|\theta) \Big|\theta\Big)$$
:::

As the mean value of a second derivative, the information measures the curve of $\ell(\theta|X)$ *(expand?)*

The negative second derivative of the log-likelihood also has a special name itself: the **observed information**.

:::{.definition name="Observed Information"}
The observed information is defined as

$$\mathcal{J}(\theta|X) = -\frac{\partial^2}{\partial\theta^2}\ell(\theta|X)$$
When $\theta$ is a $k$-dimensional vector, the observed information is the Hessian of the log-likelihood:

$$\mathcal{J}(\theta|X) = -\nabla\nabla^\top\ell(\theta|X) = \begin{bmatrix}\frac{\partial^2}{\partial\theta_1^2} & ... & \frac{\partial^2}{\partial\theta_1\partial\theta_k}\\ \vdots & & \vdots\\ \frac{\partial^2}{\partial\theta_k\partial\theta_1} & ... & \frac{\partial^2}{\partial\theta_k^2}\end{bmatrix}$$

:::

The observed information can be used to estimate the Fisher information for a given sample $X$.

## Unbiasedness

## Minimum Variance (Cramer-Rao Lower Bound)

## Uniform Minimum Variance Unbiased Estimators (UMVUEs)

::: {.definition name="Lehmann-Scheffe Theorem"}
This theorem

:::

## Inferential Properties of Exponential Families Distributions

Suppose we draw a sample of $n$ iid random variables $X_1,...,X_n$ following one of the distributions below. The proceeding tables list the inferential properties of this sample.

### Bernoulli

|   |   |
| :-----------: | :-----------: | 
|  Log-likelihood |  $\ell(\theta|X) = n\log(1-p) + \log(\frac{p}{1-p})\sum_{i=1}^n x_i$ |
|   Score Equations | $U_n(\theta|X) = -\frac{n}{1-p} + \frac{1}{p(1-p)}\sum_{i=1}^n x_i$ |
| Observed Information | $-\frac{\partial^2}{\partial\theta^2}\ell(\theta|X) = \frac{n}{(1-p)^2} + \frac{1-2p}{p^2(1-p)^2}\sum_{i=1}^n x_i$ | 
|  Fisher Information | $I(\theta)= \frac{n}{p(1-p)}$ |
|   MLE  | $\frac{1}{n}\sum_{i=1}^n x_i$ |

### Binomial

Since the Binomial has $n$ as a parameter, notation in problems that involve a *sample* of $n$ iid Binomial random variables can be tricky. To clarify, in the following table let $X_i \sim \text{Binomial}(m, p)$, and let $n$ represent the number of samples. 

|   |   |
| :-----------: | :-----------: | 
|  Log-likelihood |  $\ell(\theta|X) = nm\log(1-p) + \log(\frac{p}{1-p})\sum_{i=1}^n x_i + \log({m\choose x_i})$ |
|   Score Equations | $U_n(\theta|X) = -\frac{nm}{1-p} + \frac{1}{p(1-p)}\sum_{i=1}^n x_i$ |
| Observed Information | $-\frac{\partial^2}{\partial\theta^2}\ell(\theta|X) = \frac{nm}{(1-p)^2} + \frac{1-2p}{p^2(1-p)^2}\sum_{i=1}^n x_i$ | 
|  Fisher Information | $I(\theta)= \frac{nm}{p(1-p)}$ |
|   MLE  | $\frac{1}{nm}\sum_{i=1}^n x_i$ |

### Geometric

|                   Log-likelihood                   |                   Score Equations                   | Fisher Information |              MLE              |
|:----------------:|:----------------:|:----------------:|:----------------:|
| $n\log(p) + \log(1-p)\sum_{i=1}^n x_i$ | $\frac{n}{p} - \frac{1}{1-p}\sum_{i=1}^n x_i$ |  | $\frac{n}{\sum_{i=1}^n x_i}$ |

### Negative Binomial

|                   Log-likelihood                   |                   Score Equations                   | Fisher Information |              MLE              |
|:----------------:|:----------------:|:----------------:|:----------------:|
| $nr\log(\frac{p}{1-p}) + \sum_{i=1}^n x_i\log(1-p) + \log{x_i + r - 1\choose x_i}$ | $-\frac{nr}{p} - \frac{1}{1-p}\sum_{i=1}^n x_i$ | $\frac{r}{(1-p)^2p}$ | $\frac{1}{1 - \frac{1}{nr}\sum_{i=1}^nx_i}$ |

### Poisson

|                   Log-likelihood                   |                   Score Equations                   | Fisher Information |              MLE              |
|:----------------:|:----------------:|:----------------:|:----------------:|
| $n\lambda + \sum_{i=1}^n x_i\log(\lambda) - \log(x_i!)$ | $-n + \frac{1}{\lambda}x_i$ | $\frac{1}{\lambda}$ | $\frac{1}{n}\sum_{i=1}^nx_i$ |

### Normal

|                   Log-likelihood                   |                   Score Equations                   | Fisher Information |              MLE              |
|:----------------:|:----------------:|:----------------:|:----------------:|
| $-\frac{n}{2}\log(2\pi\sigma^2) - \frac{1}{2\sigma^2}\sum_{i=1}^n (x_i - \mu)^2$ | $U(\mu | x, \sigma^2) = -n\mu -\frac{1}{\sigma^2}\sum_{i=1}^n x_i \\ U(\sigma^2 | x, \mu) = -\frac{n}{2\sigma^2} + \frac{1}{2(\sigma^2)^2}\sum_{i=1}^n(x_i-\mu)^2$ | $\begin{bmatrix}\frac{1}{\sigma^2} & 0 \\ 0 & \frac{1}{2\sigma^4}\end{bmatrix}$ | $\frac{1}{n}\sum_{i=1}^nx_i$ |

### Exponential

|                   Log-likelihood                   |                   Score Equations                   | Fisher Information |              MLE              |
|:----------------:|:----------------:|:----------------:|:----------------:|
| $-n\log(\lambda) - \frac{1}{\lambda}\sum_{i=1}^n x_i$ | $-\frac{n}{\lambda} + \frac{1}{\lambda^2}\sum_{i=1}^n x_i$ | $\lambda^2$ | $\frac{1}{n}\sum_{i=1}^nx_i$ |

### Gamma

|                   Log-likelihood                   |                   Score Equations                   | Fisher Information |              MLE              |
|:----------------:|:----------------:|:----------------:|:----------------:|
| $-n\log(\Gamma(k)) - nk\log(\lambda) + (k - 1 - \frac{1}{\lambda})\sum_{i=1}^n x_i$ | $-\frac{nk}{\lambda} + \frac{1}{\lambda^2}\sum_{i=1}^n x_i$ with $k$ known (otherwise, requires differentiating $\Gamma(k)$) | $\begin{bmatrix}\psi^{(1)}(k) & \frac{1}{\lambda}\\ \frac{1}{\lambda} & \frac{k}{\lambda^2}\end{bmatrix}$ |  |

### Pareto

|                   Log-likelihood                   |                   Score Equations                   | Fisher Information |              MLE              |
|:----------------:|:----------------:|:----------------:|:----------------:|
| $n\log{\alpha} + n\alpha\log(x_m) - (\alpha + 1)\sum_{i=1}^n\log(x_i)$ | $U(\alpha | x_i) = \frac{n}{\alpha} + n\log(x_m) - \sum_{i=1}^n \log(x_i)$ | $\frac{n}{\alpha^2}$ | $\frac{n}{\sum_{i=1}^n\log(x_i)}$
