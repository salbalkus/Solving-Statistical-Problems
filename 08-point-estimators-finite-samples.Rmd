# Point Estimators: Finite Samples {#point-estimators-finite-samples}

The goal of parametric statistical inference is to estimate a parameter - or a function of said parameter - of a given distribution using a random sample. To do this, we must construct an [estimator](https://en.wikipedia.org/wiki/Estimator):

::: {.definition name="Estimator"}

An **estimator** is a [statistic](#statistics) $T(X)$ - a function of data - used to obtain an estimate of a parameter $\theta$ or function of a parameter $\tau(\theta)$ in a distribution. 

- An **estimate** is the value of an estimator when computed on a set of data - that is, $T(X)$ for a particular realization of $X$.
- An **estimand** is the parameter $\theta$ or function $\tau(\theta)$ needed to be estimated.

:::

Point estimators estimate scalar or vector real values - as opposed to interval estimators, which estimate intervals. Estimators have various qualities - some are good, and some are bad. This chapter will discuss how to construct and evaluate the quality of estimators.

## Identifiability

First of all, how do we know if it even possible to estimate a given parameter in a distribution? Such a question gives rise to the concept of [identifiability](https://en.wikipedia.org/wiki/Identifiability):

::: {.definition name="Identifiability"}

For a random variable $X$, a parameter $\theta$ in a parameter space $\Theta$ is said to be **identifiable** if, for all $\theta, \theta' \in \Theta$,

$$\theta \neq \theta' \implies F_X(x|\theta) \neq F_X(x|\theta')$$
Alternatively, $\theta$ is *not* identifiable if there *exists* $\theta, \theta' \in \Theta$ such that, for all $x$, 

$$F_X(x|\theta) = F_X(x|\theta')$$
:::

If $\theta$ is non-identifiable, that means two distributions from the same family will be identical under different $\theta$ values. Hence, even if we knew the true underlying distribution of $X$, it would be impossible to know its parameters. Given a random sample, it would be impossible to estimate the parameter accurately.

Non-identifiability usually arises when a distribution is parameterized by some function of multiple parameters. For instance, if $\lambda = (\lambda_1, \lambda_2)$, and $X \sim \text{Exp}(\lambda_1 + \lambda_2)$, it would be impossible to estimate $\lambda_1$ or $\lambda_2$ given $X$ alone since $\lambda_1 + \lambda_2$ can take on the same value for different individual values of $\lambda_1$ and $\lambda_2$. 

Generally, we can assume parameters are identifiable. But how might we show a parameter is not identifiable? One clue to look for is linear dependence.

::: {.example name="Non-Identifiability Via Linear Dependence"}

Non-identifiability can arise in a natural exponential family. Suppose we've parametrized a [natural exponential family](#exponential-family) into the form

$$f(x | \eta) = h(x)c(\theta)\exp(\sum_{i=1}^k \eta_it_i)$$
where the vector $t = (t_1, ..., t_k)$ is linearly dependent. Linear dependence implies that there exists $\alpha = (\alpha_1, ..., \alpha_k)$ not equal to the 0-vector such that $\alpha t = 0$. Then, $\eta$ is not identified?

How do we know? Since $\alpha t = \sum_{i=1}^k \alpha_it_i = 0$, we can add it on the interior of the exponential function:

$$f(x | \eta) = h(x)c(\theta)\exp(\sum_{i=1}^k \eta_it_i + \sum_{i=1}^k \alpha_it_i)\\
= h(x)c(\theta)\exp(\sum_{i=1}^k (\eta_i + \alpha_i)t_i)$$

Letting $\eta = (\eta_1 + \alpha_1,..., \eta_k + \alpha_k)$, clearly $f(x|\eta) = f(x|\eta')$ and $\nu$ is not identified.

:::

Non-identifiability can also arise in more practical situations, such as the following:

::: {.example name="Non-Identifiability with Measurement Error"}

Suppose we want to construct a simple linear regression model $Y = \beta X + \varepsilon$ - but there's a twist. Both $Y$ and $X$ are measured with unknown error, so we only observe

$$
Y^* = Y + \epsilon_1\\
X^* = X + \epsilon_2
$$

If we try to fit this linear regression on $Y^*$ and $X^*$ instead, we get

$$
Y + \epsilon_1 = \beta(X + \epsilon_2) + \varepsilon\\
\implies Y = \beta X + (\beta\epsilon_2 - \varepsilon_1 + \varepsilon)
$$

Now, since $\epsilon_1$ and $\epsilon_2$ are unknown, $\beta$ has become unidentifiable. Consider $(\beta', \epsilon_2') \neq (\beta, \epsilon_2)$. Subtracting $Y = \beta' X + (\beta'\epsilon_2' - \varepsilon_1 + \varepsilon)$ from $Y = \beta X + (\beta\epsilon_2 - \varepsilon_1 + \varepsilon)$, we get

$$0 = (\beta - \beta')X + (\beta\epsilon_2 - \beta'\epsilon_2')$$
Multiple values of $\beta$ and $\beta'$ can be chosen to satisfy this equation (since $\epsilon \neq \epsilon'$) meaning that $F_X(x|\beta) = F_X(x|\beta')$ for $\beta \neq \beta'$, proving $\beta$ is not identifiable.

:::

(can we use the MGF to show this as well?)

## Finding estimators

There are several methods used to find parameter estimators. In this chapter we will discuss two very popular methods, the method of moments and the maximum likelihood method. 

### Method of Moments

Recall that the moments of a distribution are given by $\mu^k = E[X^k]$, where $k \in \mathbb{N}_{+}$. The idea of the method of moments is to equate the population quantity to the sample quantity, i.e.

$$\mu^k = E[X^k] = \frac{1}{n} \sum_{i=1}^{n} X_i^k$$
where $k=1,2,...$ and $\textbf{X}=(X_1,..., X_n)$ is the sample from a distribution $f(x|\theta)$ where $\theta = (\theta_1,..., \theta_k)$. Usually we use find $k$ equations of sample average and moments to estimate $k$ parameters. 

::: {.example}
Suppose we want to find MOM estimators for $\alpha, \beta$ from a $X\sim\Gamma(\alpha,\beta)$ random variable. Let $\textbf{X}=(X_1,...,X_n)$ be and iid sample from the gamma distribution. In this case we know that $E[X]=\alpha/\beta$ and $Var(X)= \alpha/\beta^2$. Since we are estimating two parameters, we will be using the first two moments, namely, $E[X], E[X^2]$. Observe that $Var(X) = E[X^2] - E[X]^2 \Rightarrow  E[X^2] =  \alpha/\beta^2 + (\alpha/\beta)^2$. Now, we can proceed with the method of moments estimation with the following two equations.

$$ \bar{X} = \alpha/\beta\\ \bar{X^2} = \alpha/\beta^2 + (\alpha/\beta)^2$$
Solving for $\alpha, \beta$ we obtain that 

$$ \hat{\beta} = \frac{\bar{X}}{\bar{X^2} - \bar{X}^2},\\ \hat{\alpha} = \hat{\beta}\bar{X} = \frac{\bar{X}^2}{\bar{X^2} - \bar{X}^2}.$$
:::

**Remarks:**

- MOM estimators may not exist. Suppose $Y_1,...,Y_n$ is a random sample from a $Unif(-\theta, \theta)$. Then $E[X] = 0$ and a MOM estimator would solve the following equation, $\bar{Y} = 0$, which has no solutions in $\theta$. Thus a MOM estimator of $\theta$ does not exist. 

- MOM estimators may be outside of the parameter space. 

- MOM estimators are not invariant to transformations. 

- MOM estimators can offer a starting point for iterative methods used to find solutions of likelihood equations. 


### MLE Theory 

:::{.definition name="Likelihood"}

Let $X$ be a random variable or random vector, and $\theta$ a parameter or vector of parameters describing the distribution of $X$. Then, the **likelihood** is

$$\mathcal{L}(\theta | X) = f_X(x|\theta)$$

:::

:::{.definition name="Log-Likelihood"}
Let $X$ be a random variable or random vector, and $\theta$ a parameter or vector of parameters describing the distribution of $X$. Then, the **log-likelihood** is
$$\ell(\theta|X) = \log\mathcal{L}(\theta | X) = \log f_X(x|\theta)$$
:::

The log-likelihood is often easier to use. Furthermore, When $X$ is an iid sample $X_1, X_2, ... X_n$, by the properties of logarithms, $\ell(\theta|X) = \log\Big(\prod_{i=1}^nf_{X_i}(x|\theta)\Big) = \sum_{i=1}^n \log f_{X_i}(x|\theta)$. Working with a sum is much easier than with a product.

Of course, the whole reason this is permissible in finding a maximum likelihood estimate is that the $\log$ function is monotonic - finding the maximum of a function is equivalent to finding the maximum of its logarithm.

Maximizing this likelihood often requires calculus - specifically, the First Derivative Test. In statistics, the gradient of the log-likelihood has a special name: the **score**.

:::{.definition name="Score Equations"}
If $\theta$ is a vector of parameters, the **score equations** are defined as the gradient of the log-likelihood (as described above). That is,

$$U(\theta | X) = \begin{bmatrix}\frac{\partial}{\partial\theta_1}\ell(\theta_1|X) & ... & \frac{\partial}{\partial\theta_k}\ell(\theta_k|X) \end{bmatrix}$$
Of course, if $\theta$ is a single parameter, then $U(\theta|X) = \frac{d}{d\theta}\ell(\theta|X)$

:::

Therefore, the first step in finding an MLE is to set the score to 0, and solve for the desired parameter. Let's see an example.

:::{.example }
Let $\textbf{X} = (X_1,..., X_n)$ be an iid sample from a $Bern(p)$ distribution. To find the MLE for $p$, let us find maximize wrt $p$ the log likelihood function. The likelihood function is $\mathcal{L}(p|\textbf{X}) = \prod_{i=1}^{n} p^X_i(1-p)^{1-X_i} = p^{\sum X_i}(1-p)^{n-\sum X_i}$. Now, the log-likehood is given by $\mathcal{l}(p|\textbf{X}) = log(p) \sum X_i + log(1-p)(n-\sum X_i)$. The score equation goes as follows.

$$ U(p|\textbf{X}) = \frac{\sum X_i}{p} - \frac{(n-\sum X_i)}{1-p} \overset{set}= 0$$
and the solution wrt $p$ is $\bar{X}$. Now, the second derivative test is always negative which implies that $\hat{p}_{MLE} = \bar{X}$. 
:::

The score has two useful properties in MLE theory. First, under [regularity conditions](#regularity-conditions), $E(U(\theta|X)) = 0$ - its mean is 0. This is because the regularity conditions imply [Leibniz's rule](#leibniz-rule); that is,

\begin{align}
E(U(\theta|X)) = \int_{\mathcal{X}}\frac{\partial}{\partial\theta}\log\mathcal{L}(\theta|X)f(x|\theta)dx \\
= \int_{\mathcal{X}}\frac{1}{f(x|\theta)}\frac{\partial}{\partial\theta}f(x|\theta)f(x|\theta)dx \\ 
= \frac{\partial}{\partial\theta}\frac{f(x|\theta)}{f(x|\theta)}f(x|\theta)dx = \frac{\partial}{\partial\theta}1 = 0
\end{align}

The second useful property is that, under the same regularity conditions, the variance of the score is

\begin{align}
Var(U(\theta|X)) = E(U(\theta|X)U(\theta|X)^\top) \\
= -E\Big(\frac{\partial^2}{\partial\partial^\top}\ell(\theta|X)\Big) 
\end{align}

If $\theta$ is one-dimensional, $Var(U(\theta|X)) = E(U(\theta|X)^2) = -E\Big(\frac{\partial^2}{\partial\theta^2}\ell(\theta|X)\Big)$. The rather insightful derivation of this is available [here](https://en.wikipedia.org/wiki/Score_(statistics)#Variance). This variance has a special name, the **information** (or sometimes, "Fisher Information")

:::{.definition name="Information"}

In statistics, **information** refers to the amount of information that a random variable $X$ contains about a parameter $\theta$. It is defined mathematically as the variance of the score, which is

$$I(\theta) = E\Big((\frac{\partial}{\partial\theta}\log f(X|\theta))^2\Big|\theta\Big)$$
Under MLE regularity conditions, 

$$I(\theta) = -E\Big(\frac{\partial^2}{\partial\theta^2}\log f(X|\theta) \Big|\theta\Big)$$
:::

As the mean value of a second derivative, the information measures the curve of $\ell(\theta|X)$ *(expand?)*

The negative second derivative of the log-likelihood also has a special name itself: the **observed information**.

:::{.definition name="Observed Information"}
The observed information is defined as

$$\mathcal{J}(\theta|X) = -\frac{\partial^2}{\partial\theta^2}\ell(\theta|X)$$
When $\theta$ is a $k$-dimensional vector, the observed information is the Hessian of the log-likelihood:

$$\mathcal{J}(\theta|X) = -\nabla\nabla^\top\ell(\theta|X) = \begin{bmatrix}\frac{\partial^2}{\partial\theta_1^2} & ... & \frac{\partial^2}{\partial\theta_1\partial\theta_k}\\ \vdots & & \vdots\\ \frac{\partial^2}{\partial\theta_k\partial\theta_1} & ... & \frac{\partial^2}{\partial\theta_k^2}\end{bmatrix}$$

:::

The observed information can be used to estimate the Fisher information for a given sample $X$.

The MLE theory is essential is the task of finding estimators for a given parameter. By definition, the solution that maximizes the likelihood function is called the maximum likelihood estimator for the parameter. This estimator has very properties, namely, it is a statistic that respects the parameter space. We previously saw an example in which we found an MLE for a one-dimensional parameter. However, we can find multidimensional MLEs for multidimensional parameters. There are two main approaches. 

Let $\theta = (\theta_1, \theta_2)$, a two-dimensional parameter. 
1. Find MLE candidates for $\theta_1, \theta_2$ by setting partial derivative wrt $\theta_1, \theta_2$ respectively to zero and solving.
2. Find second partial derivatives and show that at least one of them is negative. 
3. Find the second order partial derivative matrix and show that the determinant is positive. 
If 2. and 3. are shown to be true, then then candidates from 1. are MLE's for $\theta_1, \theta_2$. 

Let $\theta$ be a $k$-dimensional parameter, ie, $k$ parameters. Then we can employ the profile-likelihood approach to find an MLE candidate. The profile-likelihood approach consists of maximizing wrt a parameter of interest and profiling out the rest of the parameters (which can be considered nuissance parameters). In the case in which we want to find MLE for all $k$ parameters, then we would have to maximize wrt to each parameter and then substitute back our solutions until we prove that the candidates maximize each of the profile likelihoods for each parameter. This method is necessary when we want to find the MLE of an unknown, but fixed, number of parameters. 
<!-- EXPLAIN BETTER -->
<!-- https://web.stat.tamu.edu/~suhasini/teaching613/chapter3.pdf -->


**Remarks:** Suppose $X \sim f(x|\eta)$ is an exponential family with natural parametrization. Then MOM estimators correspond to MLE.

::: {.theorem}
If an MLE is unique and an MSS exists, then the MLE is a function of the MSS. 
:::

::: {.theorem name="Invariance property of MLE"}
If $\hat{\theta}$ is MLE for $\theta$, then $\tau(\hat{\theta})$ is MLE for $\tau(\theta)$.
:::

::: {.example}
invariance of mle example

:::


## Properties of estimators

After we have found our estimators, we are usually interested in choosing the best estimator. This is not always possible, but there are properties we can check to at least guarantee that some characteristics will be satisfied. Let us discuss the finite- sample properties of estimators.

## Unbiasedness
Let $\textbf{X} = (X_1,..., X_n), X_i \sim f(x|\theta), \theta \in \Theta$ and let $\hat{\theta}$ be an estimator of $\theta$

::: {.definition name="Bias of an estimator"}
$$bias(\hat{\theta}) = E[\hat{\theta}] - \theta$$
:::
If $bias(\hat{\theta})=0$ then $E[\hat{\theta}] = \theta$ and we say that $\hat{\theta}$ is an **unbiased** estimator of $\theta$. 

**Note:** Bias is a measure of accuracy. 

::: {.definition name="Variance of an estimator"}
$$Var(\hat{\theta}) = E[(\hat{\theta} - E[\hat{\theta}])^2] = E[\hat{\theta}^2]- E[\hat{\theta}]^2 $$
:::
**Note:** Variance is a measure of precision

::: {.definition name="Mean squared error, MSE"}
$$MSE(\hat{\theta}) = Var(\hat{\theta}) + bias^2(\hat{\theta})$$
:::
**Note:** MSE is a measure of precision and accuracy. 

Ideally we want an unbiased estimator with minimal variance. Often we have a [variance-bias tradeoff](https://en.wikipedia.org/wiki/Bias–variance_tradeoff), a situation in which when bias decreases, variance increases and viceversa. For example, for any distribution we have the following facts:

- $S^2$ is unbiased
- $\hat{\sigma^2}$ is biased
- $Var(\hat{\sigma^2}) < Var(S^2)$

::: {.definition}
Let $\hat{\theta}, \tilde{\theta}$ be two estimators of $\theta$. If $MSE(\hat{\theta}) \le MSE (\tilde{\theta}) \forall \theta$ and  $MSE(\hat{\theta}) < MSE (\tilde{\theta})$ for at least one $\theta$, then we say that $\tilde{\theta}$ is inadmissible. 
:::

::: {.definition}
Let $\hat{\theta}, \tilde{\theta}$ be any unbiased estimators of $\theta$ whose variances exist. Then, 

$$RE(\hat{\theta};\tilde{\theta}) = \frac{Var(\tilde{\theta})}{Var(\hat{\theta})}$$

is the *relative efficiency* of $\hat{\theta}$ to $\tilde{\theta}$. 
:::



## Uniform Minimum Variance Unbiased Estimators (UMVUEs)

::: {.definition name="Uniform minimum variance unbiased estimator (UMVUE)"}
An estimator $W$ of a parameter $\tau(\theta)$ is called *uniform minimum variance unbiased estimator (UMVUE)* of $\tau(\theta)$ if $E_{\theta}(W)= \tau(\theta)$ and $Var_{\theta}(W) \le Var_{\theta}(Q)$ for any other unbiased estimator, Q, of $\tau(\theta)$.
:::

The UMVUE is the unbiased estimator of $\tau(\theta)$ with the minimum variance among all other unbiased estimators of $\tau(\theta)$. Proving that an estimator is UMVUE by the definition alone becomes a difficult task easily, because there might exist an infinite number of unbiased estimators! Thankfully, a plethora of results exist that aids statisticians in the task of proving -or finding- an UMVUE. 

### Cramér-Rao Bound

The Cramér-Rao Bound provides *lower* bound for the variance of all estimators of a parameter $\tau(\theta)$. This result is extremely powerful because if we can show that an *unbiased* estimator attains the *Cramér-Rao Lower Bound* (CRLB), then the estimator is UMVUE. 

::: {.definition name="Cramér-Rao Bound"}
Let $\textbf{X}=(X_1,..., X_n), \textbf{X} \sim f_{\textbf{X}}(x|\theta), \theta \in \Theta$. If $W=W(\textbf{X})$ is some estimator such that
$$
\begin{equation}
\frac{\partial}{\partial\theta} E_{\theta}(W)=\int \frac{\partial}{\partial\theta} \bigg[ W(\textbf{X})f_{\textbf{X}}(x|\theta)\bigg]dx 
(\#eq:reg-cond-1)
\end{equation}
$$
then, 

$$Var(W) \ge \frac{[\frac{\partial}{\partial\theta} E_{\theta}(W)]^2}{E_{\theta}([\frac{\partial}{\partial\theta} ln(f_{\textbf{X}}(x|\theta))]^2)}.$$
:::

**Remark:** If the sample is iid and 

$$
\begin{equation}
\frac{\partial^2}{\partial\theta^2} \int f(x|\theta) dx =  \int \frac{\partial^2}{\partial\theta^2} f(x|\theta) dx
(\#eq:reg-cond-2)
\end{equation}
$$
then, 
 $$Var(W) \ge \frac{[\frac{\partial}{\partial\theta} E_{\theta}(W)]^2}{nE_{\theta}[-\frac{\partial^2}{\partial\theta^2} ln(f_{X_i}(x|\theta))]}.$$
Before continuing, let's discuss conditions \@ref(eq:reg-cond-1) and \@ref(eq:reg-cond-2). First, if \@ref(eq:reg-cond-1) is satisfied then 
$$E_{\theta}\bigg[\frac{\partial}{\partial\theta}ln(f_{\textbf{X}}(X|\theta))\bigg] = 0,  \forall \theta \in \Theta.$$
Furthermore, if \@ref(eq:reg-cond-2) is satisfied, then 

$$E_{\theta}\bigg[\frac{\partial^2}{\partial\theta^2} ln(f(x|\theta))\bigg] = -E_{\theta}\bigg[\bigg(\frac{\partial}{\partial\theta} ln(f(x|\theta))\bigg)^2\bigg]. $$
**Remarks:** 

- \@ref(eq:reg-cond-2) is satisfied for exponential families. 
- If \@ref(eq:reg-cond-1) and \@ref(eq:reg-cond-2) are not satisfied, then the CRLB might not be the lower bound for the variance. 

::: {.example}
CRLB example
:::

::: {.theorem name="Attainment Theorem"}
Let $W(X)$ be an unbiased $\tau(\theta)$ and suppose that \@ref(eq:reg-cond-1) holds. Then $W$ attains CRLB if and only if $\frac{\partial}{\partial \theta} ln (\mathcal{L}(\theta|X)) = a(\theta)[W-\tau(\theta)]$ for some $a(\theta)$.
:::

::: {.example}
attainment theorem example
:::

**Remark:** An estimator that does not attain CRLB can still be UMVUE. For some parameters, CRLB might not be achieved. 

::: {.theorem}
If $E_{\theta}(W(X))= \tau(\theta)$, then $W(X)$ is best unbiased if and only if $W(X)$ is uncorrelated with all unbiased estimators of 0.
:::

::: {.theorem name=Rao-Blackwell}
Suppose $W$ is unbiased for $\tau(\theta)$ and $T$ is a SS for $\tau(\theta)$. Let $Y:=E(W|T)$. Then,

1. $Y$ is a statistic (not a function of $\theta$)
2. $E[Y] = \tau(\theta), \forall \theta$
3. $Var(Y)\le Var(W), \forall \theta$. 
:::

Rao-Blackwell theorem provides a method to improve upon any unbiased estimator, i.e. provides a *better* unbiased estimator. Let us see a result from Lehmann-Scheffé, which provides a method to find an UMVUE. 

::: {.theorem name=Lehmann-Scheffé}
Suppose $W$ is unbiased for $\tau(\theta)$ and $T$ is a *complete* and *sufficient* for $\tau(\theta)$. Then $Y:=E(W|T)$ is UMVUE of $\tau(\theta)$.
:::

::: {.theorem}
Let $W$ be a statistic such that $Var(W)<\infty$, and $W$ is UMVUE for $\tau(\theta)$. Then $W$ is the unique UMVUE of $\tau(\theta)$. 
:::


## Inferential Properties of Exponential Families Distributions

Suppose we draw a sample of $n$ iid random variables $X_1,...,X_n$ following one of the distributions below. The proceeding tables list the inferential properties of this sample.

### Bernoulli

|   |   |
| :-----------: | :-----------: | 
|  Log-likelihood |  $\ell(\theta|X) = n\log(1-p) + \log(\frac{p}{1-p})\sum_{i=1}^n x_i$ |
|   Score Equations | $U_n(\theta|X) = -\frac{n}{1-p} + \frac{1}{p(1-p)}\sum_{i=1}^n x_i$ |
| Observed Information | $-\frac{\partial^2}{\partial\theta^2}\ell(\theta|X) = \frac{n}{(1-p)^2} + \frac{1-2p}{p^2(1-p)^2}\sum_{i=1}^n x_i$ | 
|  Fisher Information | $I(\theta)= \frac{n}{p(1-p)}$ |
|   MLE  | $\frac{1}{n}\sum_{i=1}^n x_i$ |

### Binomial

Since the Binomial has $n$ as a parameter, notation in problems that involve a *sample* of $n$ iid Binomial random variables can be tricky. To clarify, in the following table let $X_i \sim \text{Binomial}(m, p)$, and let $n$ represent the number of samples. 

|   |   |
| :-----------: | :-----------: | 
|  Log-likelihood |  $\ell(\theta|X) = nm\log(1-p) + \log(\frac{p}{1-p})\sum_{i=1}^n x_i + \log({m\choose x_i})$ |
|   Score Equations | $U_n(\theta|X) = -\frac{nm}{1-p} + \frac{1}{p(1-p)}\sum_{i=1}^n x_i$ |
| Observed Information | $-\frac{\partial^2}{\partial\theta^2}\ell(\theta|X) = \frac{nm}{(1-p)^2} + \frac{1-2p}{p^2(1-p)^2}\sum_{i=1}^n x_i$ | 
|  Fisher Information | $I(\theta)= \frac{nm}{p(1-p)}$ |
|   MLE  | $\frac{1}{nm}\sum_{i=1}^n x_i$ |

### Geometric

|                   Log-likelihood                   |                   Score Equations                   | Fisher Information |              MLE              |
|:----------------:|:----------------:|:----------------:|:----------------:|
| $n\log(p) + \log(1-p)\sum_{i=1}^n x_i$ | $\frac{n}{p} - \frac{1}{1-p}\sum_{i=1}^n x_i$ |  | $\frac{n}{\sum_{i=1}^n x_i}$ |

### Negative Binomial

|                   Log-likelihood                   |                   Score Equations                   | Fisher Information |              MLE              |
|:----------------:|:----------------:|:----------------:|:----------------:|
| $nr\log(\frac{p}{1-p}) + \sum_{i=1}^n x_i\log(1-p) + \log{x_i + r - 1\choose x_i}$ | $-\frac{nr}{p} - \frac{1}{1-p}\sum_{i=1}^n x_i$ | $\frac{r}{(1-p)^2p}$ | $\frac{1}{1 - \frac{1}{nr}\sum_{i=1}^nx_i}$ |

### Poisson

|                   Log-likelihood                   |                   Score Equations                   | Fisher Information |              MLE              |
|:----------------:|:----------------:|:----------------:|:----------------:|
| $n\lambda + \sum_{i=1}^n x_i\log(\lambda) - \log(x_i!)$ | $-n + \frac{1}{\lambda}x_i$ | $\frac{1}{\lambda}$ | $\frac{1}{n}\sum_{i=1}^nx_i$ |

### Normal

|                   Log-likelihood                   |                   Score Equations                   | Fisher Information |              MLE              |
|:----------------:|:----------------:|:----------------:|:----------------:|
| $-\frac{n}{2}\log(2\pi\sigma^2) - \frac{1}{2\sigma^2}\sum_{i=1}^n (x_i - \mu)^2$ | $U(\mu | x, \sigma^2) = -n\mu -\frac{1}{\sigma^2}\sum_{i=1}^n x_i \\ U(\sigma^2 | x, \mu) = -\frac{n}{2\sigma^2} + \frac{1}{2(\sigma^2)^2}\sum_{i=1}^n(x_i-\mu)^2$ | $\begin{bmatrix}\frac{1}{\sigma^2} & 0 \\ 0 & \frac{1}{2\sigma^4}\end{bmatrix}$ | $\frac{1}{n}\sum_{i=1}^nx_i$ |

### Exponential

|                   Log-likelihood                   |                   Score Equations                   | Fisher Information |              MLE              |
|:----------------:|:----------------:|:----------------:|:----------------:|
| $-n\log(\lambda) - \frac{1}{\lambda}\sum_{i=1}^n x_i$ | $-\frac{n}{\lambda} + \frac{1}{\lambda^2}\sum_{i=1}^n x_i$ | $\lambda^2$ | $\frac{1}{n}\sum_{i=1}^nx_i$ |

### Gamma

|                   Log-likelihood                   |                   Score Equations                   | Fisher Information |              MLE              |
|:----------------:|:----------------:|:----------------:|:----------------:|
| $-n\log(\Gamma(k)) - nk\log(\lambda) + (k - 1 - \frac{1}{\lambda})\sum_{i=1}^n x_i$ | $-\frac{nk}{\lambda} + \frac{1}{\lambda^2}\sum_{i=1}^n x_i$ with $k$ known (otherwise, requires differentiating $\Gamma(k)$) | $\begin{bmatrix}\psi^{(1)}(k) & \frac{1}{\lambda}\\ \frac{1}{\lambda} & \frac{k}{\lambda^2}\end{bmatrix}$ |  |

### Pareto

|                   Log-likelihood                   |                   Score Equations                   | Fisher Information |              MLE              |
|:----------------:|:----------------:|:----------------:|:----------------:|
| $n\log{\alpha} + n\alpha\log(x_m) - (\alpha + 1)\sum_{i=1}^n\log(x_i)$ | $U(\alpha | x_i) = \frac{n}{\alpha} + n\log(x_m) - \sum_{i=1}^n \log(x_i)$ | $\frac{n}{\alpha^2}$ | $\frac{n}{\sum_{i=1}^n\log(x_i)}$
