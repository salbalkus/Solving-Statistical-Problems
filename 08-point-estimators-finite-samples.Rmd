# Point Estimators: Finite Samples {#point-estimators-finite-samples}

The goal of parametric statistical inference is to estimate a parameter - or a function of said parameter - of a given distribution using a random sample. To do this, we must construct an [estimator](https://en.wikipedia.org/wiki/Estimator):

::: {.definition name="Estimator"}

An **estimator** is a [statistic](#statistics) $T(X)$ - a function of data - used to obtain an estimate of a parameter $\theta$ or function of a parameter $\tau(\theta)$ in a distribution. 

- An **estimate** is the value of an estimator when computed on a set of data - that is, $T(X)$ for a particular realization of $X$.
- An **estimand** is the parameter $\theta$ or function $\tau(\theta)$ needed to be estimated.

:::

Point estimators estimate scalar or vector real values - as opposed to interval estimators, which estimate intervals. Estimators have various qualities - some are good, and some are bad. This chapter will discuss how to construct and evaluate the quality of estimators.

## Identifiability

First of all, how do we know if it even possible to estimate a given parameter in a distribution? Such a question gives rise to the concept of [identifiability](https://en.wikipedia.org/wiki/Identifiability):

::: {.definition name="Identifiability"}

For a random variable $X$, a parameter $\theta$ in a parameter space $\Theta$ is said to be **identifiable** if, for all $\theta, \theta' \in \Theta$,

$$\theta \neq \theta' \implies F_X(x|\theta) \neq F_X(x|\theta')$$
Alternatively, $\theta$ is *not* identifiable if there *exists* $\theta, \theta' \in \Theta$ such that, for all $x$, 

$$F_X(x|\theta) = F_X(x|\theta')$$
:::

If $\theta$ is non-identifiable, that means two distributions from the same family will be identical under different $\theta$ values. Hence, even if we knew the true underlying distribution of $X$, it would be impossible to know its parameters. Given a random sample, it would be impossible to estimate the parameter accurately.

Non-identifiability usually arises when a distribution is parameterized by some function of multiple parameters. For instance, if $\lambda = (\lambda_1, \lambda_2)$, and $X \sim \text{Exp}(\lambda_1 + \lambda_2)$, it would be impossible to estimate $\lambda_1$ or $\lambda_2$ given $X$ alone since $\lambda_1 + \lambda_2$ can take on the same value for different individual values of $\lambda_1$ and $\lambda_2$. 

Generally, we can assume parameters are identifiable. But how might we show a parameter is not identifiable? One clue to look for is linear dependence.

::: {.example name="Non-Identifiability Via Linear Dependence"}

Non-identifiability can arise in a natural exponential family. Suppose we've parametrized a [natural exponential family](#exponential-family) into the form

$$f(x | \eta) = h(x)c(\theta)\exp(\sum_{i=1}^k \eta_it_i)$$
where the vector $t = (t_1, ..., t_k)$ is linearly dependent. Linear dependence implies that there exists $\alpha = (\alpha_1, ..., \alpha_k)$ not equal to the 0-vector such that $\alpha t = 0$. Then, $\eta$ is not identified?

How do we know? Since $\alpha t = \sum_{i=1}^k \alpha_it_i = 0$, we can add it on the interior of the exponential function:

$$f(x | \eta) = h(x)c(\theta)\exp(\sum_{i=1}^k \eta_it_i + \sum_{i=1}^k \alpha_it_i)\\
= h(x)c(\theta)\exp(\sum_{i=1}^k (\eta_i + \alpha_i)t_i)$$

Letting $\eta = (\eta_1 + \alpha_1,..., \eta_k + \alpha_k)$, clearly $f(x|\eta) = f(x|\eta')$ and $\nu$ is not identified.

:::

Non-identifiability can also arise in more practical situations, such as the following:

::: {.example name="Non-Identifiability with Measurement Error"}

Suppose we want to construct a simple linear regression model $Y = \beta X + \varepsilon$ - but there's a twist. Both $Y$ and $X$ are measured with unknown error, so we only observe

$$
Y^* = Y + \epsilon_1\\
X^* = X + \epsilon_2
$$

If we try to fit this linear regression on $Y^*$ and $X^*$ instead, we get

$$
Y + \epsilon_1 = \beta(X + \epsilon_2) + \varepsilon\\
\implies Y = \beta X + (\beta\epsilon_2 - \varepsilon_1 + \varepsilon)
$$

Now, since $\epsilon_1$ and $\epsilon_2$ are unknown, $\beta$ has become unidentifiable. Consider $(\beta', \epsilon_2') \neq (\beta, \epsilon_2)$. Subtracting $Y = \beta' X + (\beta'\epsilon_2' - \varepsilon_1 + \varepsilon)$ from $Y = \beta X + (\beta\epsilon_2 - \varepsilon_1 + \varepsilon)$, we get

$$0 = (\beta - \beta')X + (\beta\epsilon_2 - \beta'\epsilon_2')$$
Multiple values of $\beta$ and $\beta'$ can be chosen to satisfy this equation (since $\epsilon \neq \epsilon'$) meaning that $F_X(x|\beta) = F_X(x|\beta')$ for $\beta \neq \beta'$, proving $\beta$ is not identifiable.

:::

(can we use the MGF to show this as well?)

## Finding estimators

How do we take a guess at what a good estimator might be? There are several methods used to find parameter estimators. In this chapter we will discuss two very popular methods, the method of moments and the maximum likelihood method. 

### Method of Moments

Recall that the moments of a distribution are given by $\mu^k = E[X^k]$, where $k \in \mathbb{N}_{+}$. The idea of the method of moments is to equate the population quantity to the sample quantity, i.e.

$$\mu^k = E[X^k] = \frac{1}{n} \sum_{i=1}^{n} X_i^k$$
where $k=1,2,...$ and $\textbf{X}=(X_1,..., X_n)$ is the sample from a distribution $f(x|\theta)$ where $\theta = (\theta_1,..., \theta_k)$. Usually we use find $k$ equations of sample average and moments to estimate $k$ parameters. 

::: {.example}
Suppose we want to find MOM estimators for $\alpha, \beta$ from a $X\sim\Gamma(\alpha,\beta)$ random variable. Let $\textbf{X}=(X_1,...,X_n)$ be and iid sample from the gamma distribution. In this case we know that $E[X]=\alpha/\beta$ and $Var(X)= \alpha/\beta^2$. Since we are estimating two parameters, we will be using the first two moments, namely, $E[X], E[X^2]$. Observe that $Var(X) = E[X^2] - E[X]^2 \Rightarrow  E[X^2] =  \alpha/\beta^2 + (\alpha/\beta)^2$. Now, we can proceed with the method of moments estimation with the following two equations.

$$ \bar{X} = \alpha/\beta\\ \bar{X^2} = \alpha/\beta^2 + (\alpha/\beta)^2$$
Solving for $\alpha, \beta$ we obtain that 

$$ \hat{\beta} = \frac{\bar{X}}{\bar{X^2} - \bar{X}^2},\\ \hat{\alpha} = \hat{\beta}\bar{X} = \frac{\bar{X}^2}{\bar{X^2} - \bar{X}^2}.$$
:::

**Issues with the MoM Estimators:**  

- MOM estimators may not exist. Suppose $Y_1,...,Y_n$ is a random sample from a $Unif(-\theta, \theta)$. Then $E[X] = 0$ and a MOM estimator would solve the following equation, $\bar{Y} = 0$, which has no solutions in $\theta$. Thus a MOM estimator of $\theta$ does not exist. 

- MOM estimators may be outside of the parameter space. 

- MOM estimators are not invariant to transformations. 

**Advantages of MoM Estimators:**  

- They are guaranteed to be [unbiased](#unbiasedness) and [consistent](#consistency) for the parameter they estimate.

- MOM estimators can offer a starting point for iterative methods used to find solutions of likelihood equations.


### Maximum Likelihood Estimation

Rather than simply matching moments, often a more effective strategy for finding an estimator is by finding the value which maximizes the *entire* likelihood of the sample. That is, for a given $X = X_1, X_2, ... X_n$, we choose the estimator for which this sample is most likely. This is called the *maximum likelihood estimator*. Because of how common this approach is, we introduce several terms to describe the components of MLE.

:::{.definition name="Likelihood"}

Let $X$ be a random variable or random vector, and $\theta$ a parameter or vector of parameters describing the distribution of $X$. Then, the **likelihood** is

$$\mathcal{L}(\theta | X) = f_X(x|\theta)$$

:::

:::{.definition name="Log-Likelihood"}
Let $X$ be a random variable or random vector, and $\theta$ a parameter or vector of parameters describing the distribution of $X$. Then, the **log-likelihood** is
$$\ell(\theta|X) = \log\mathcal{L}(\theta | X) = \log f_X(x|\theta)$$
:::

The log-likelihood is often easier to use. Furthermore, When $X$ is an iid sample $X_1, X_2, ... X_n$, by the properties of logarithms, $\ell(\theta|X) = \log\Big(\prod_{i=1}^nf_{X_i}(x|\theta)\Big) = \sum_{i=1}^n \log f_{X_i}(x|\theta)$. Working with a sum is much easier than with a product.

Of course, the whole reason this is permissible in finding a maximum likelihood estimate is that the $\log$ function is monotonic - finding the maximum of a function is equivalent to finding the maximum of its logarithm.

Maximizing this likelihood often requires calculus - specifically, the First Derivative Test. In statistics, the gradient of the log-likelihood has a special name: the **score**.

:::{.definition name="Score Equations"}
If $\theta$ is a vector of parameters, the **score equations** are defined as the gradient of the log-likelihood (as described above). That is,

$$U(\theta | X) = \begin{bmatrix}\frac{\partial}{\partial\theta_1}\ell(\theta_1|X) & ... & \frac{\partial}{\partial\theta_k}\ell(\theta_k|X) \end{bmatrix}$$
Of course, if $\theta$ is a single parameter, then $U(\theta|X) = \frac{d}{d\theta}\ell(\theta|X)$

:::

Therefore, the first step in finding an MLE is to set the score to 0, and solve for the desired parameter. Let's see an example.

:::{.example }
Let $\textbf{X} = (X_1,..., X_n)$ be an iid sample from a $Bern(p)$ distribution. To find the MLE for $p$, let us find maximize wrt $p$ the log likelihood function. The likelihood function is $\mathcal{L}(p|\textbf{X}) = \prod_{i=1}^{n} p^X_i(1-p)^{1-X_i} = p^{\sum X_i}(1-p)^{n-\sum X_i}$. Now, the log-likehood is given by $\mathcal{l}(p|\textbf{X}) = log(p) \sum X_i + log(1-p)(n-\sum X_i)$. The score equation goes as follows.

$$ U(p|\textbf{X}) = \frac{\sum X_i}{p} - \frac{(n-\sum X_i)}{1-p} \overset{set}= 0$$
and the solution wrt $p$ is $\bar{X}$. Now, the second derivative test is always negative which implies that $\hat{p}_{MLE} = \bar{X}$. 
:::

The score has two useful properties in MLE theory. First, under [regularity conditions](#regularity-conditions), $E(U(\theta|X)) = 0$ - its mean is 0. This is because the regularity conditions imply [Leibniz's rule](#leibniz-rule); that is,

\begin{align}
E(U(\theta|X)) = \int_{\mathcal{X}}\frac{\partial}{\partial\theta}\log\mathcal{L}(\theta|X)f(x|\theta)dx \\
= \int_{\mathcal{X}}\frac{1}{f(x|\theta)}\frac{\partial}{\partial\theta}f(x|\theta)f(x|\theta)dx \\ 
= \frac{\partial}{\partial\theta}\frac{f(x|\theta)}{f(x|\theta)}f(x|\theta)dx = \frac{\partial}{\partial\theta}1 = 0
\end{align}

The second useful property is that, under the same regularity conditions, the variance of the score is

\begin{align}
Var(U(\theta|X)) = E(U(\theta|X)U(\theta|X)^\top) \\
= -E\Big(\frac{\partial^2}{\partial\partial^\top}\ell(\theta|X)\Big) 
\end{align}

If $\theta$ is one-dimensional, $Var(U(\theta|X)) = E(U(\theta|X)^2) = -E\Big(\frac{\partial^2}{\partial\theta^2}\ell(\theta|X)\Big)$. The rather insightful derivation of this is available [here](https://en.wikipedia.org/wiki/Score_(statistics)#Variance). This variance has a special name, the **information** (or sometimes, "Fisher Information")

:::{#information .definition name="Information"}

In statistics, **information** refers to the amount of information that a random variable $X$ contains about a parameter $\theta$. It is defined mathematically as the variance of the score, which is

$$I(\theta) = E\Big((\frac{\partial}{\partial\theta}\log f(X|\theta))^2\Big|\theta\Big)$$
Under MLE regularity conditions, 

$$I(\theta) = -E\Big(\frac{\partial^2}{\partial\theta^2}\log f(X|\theta) \Big|\theta\Big)$$
:::

As the mean value of a second derivative, the information measures the curve of $\ell(\theta|X)$ *(expand?)*

The negative second derivative of the log-likelihood also has a special name itself: the **observed information**.

:::{.definition name="Observed Information"}
The observed information is defined as

$$\mathcal{J}(\theta|X) = -\frac{\partial^2}{\partial\theta^2}\ell(\theta|X)$$
When $\theta$ is a $k$-dimensional vector, the observed information is the Hessian of the log-likelihood:

$$\mathcal{J}(\theta|X) = -\nabla\nabla^\top\ell(\theta|X) = \begin{bmatrix}\frac{\partial^2}{\partial\theta_1^2} & ... & \frac{\partial^2}{\partial\theta_1\partial\theta_k}\\ \vdots & & \vdots\\ \frac{\partial^2}{\partial\theta_k\partial\theta_1} & ... & \frac{\partial^2}{\partial\theta_k^2}\end{bmatrix}$$

:::

The observed information can be used to estimate the Fisher information for a given sample $X$.

The MLE theory is essential is the task of finding estimators for a given parameter. By definition, the solution that maximizes the likelihood function is called the maximum likelihood estimator for the parameter. This estimator has very properties, namely, it is a statistic that respects the parameter space. We previously saw an example in which we found an MLE for a one-dimensional parameter. However, we can find multidimensional MLEs for multidimensional parameters. There are two main approaches. 

Let $\theta = (\theta_1, \theta_2)$, a two-dimensional parameter. 
1. Find MLE candidates for $\theta_1, \theta_2$ by setting partial derivative wrt $\theta_1, \theta_2$ respectively to zero and solving.
2. Find second partial derivatives and show that at least one of them is negative. 
3. Find the second order partial derivative matrix and show that the determinant is positive. 
If 2. and 3. are shown to be true, then then candidates from 1. are MLE's for $\theta_1, \theta_2$. 

Let $\theta$ be a $k$-dimensional parameter, ie, $k$ parameters. Then we can employ the profile-likelihood approach to find an MLE candidate. The profile-likelihood approach consists of maximizing with respect to a parameter of interest and profiling out the rest of the parameters (which can be considered nuisance parameters). In the case in which we want to find MLE for all $k$ parameters, then we would have to maximize with respect to each parameter and then substitute back our solutions until we prove that the candidates maximize each of the profile likelihoods for each parameter. This method is necessary when we want to find the MLE of an unknown, but fixed, number of parameters. 

EXAMPLE FORTHCOMING
<!-- EXPLAIN BETTER -->
<!-- https://web.stat.tamu.edu/~suhasini/teaching613/chapter3.pdf -->


**Remarks:** Suppose $X \sim f(x|\eta)$ is an exponential family with natural parametrization. Then MOM estimators correspond to MLE.

::: {.theorem}
If an MLE is unique and an MSS exists, then the MLE is a function of the MSS. 
:::

::: {.theorem name="Invariance property of MLE"}
If $\hat{\theta}$ is MLE for $\theta$, then $\tau(\hat{\theta})$ is MLE for $\tau(\theta)$.
:::

::: {.example}
invariance of mle example

:::


## Properties of Estimators

How do we know if a particular estimator is considered "good"? There are two main properties by which we measure the quality of an estimator: **bias**, which measures the *accuracy* of the estimator, and **variance**, which measures the *precision*. Let $\textbf{X} = (X_1,..., X_n), X_i \sim f(x|\theta), \theta \in \Theta$ and let $\hat{\theta}$ be an estimator of $\theta$. Then, we have the following:

::: {.definition name="Bias of an estimator"}
$$bias(\hat{\theta}) = E[\hat{\theta}] - \theta$$
:::
If $bias(\hat{\theta})=0$ then $E[\hat{\theta}] = \theta$ and we say that $\hat{\theta}$ is an **unbiased** estimator of $\theta$. 

**Note:** Bias is a measure of accuracy. 

::: {.definition name="Variance of an estimator"}
$$Var(\hat{\theta}) = E[(\hat{\theta} - E[\hat{\theta}])^2] = E[\hat{\theta}^2]- E[\hat{\theta}]^2 $$
:::
**Note:** Variance is a measure of precision

::: {.definition name="Mean squared error, MSE"}
$$MSE(\hat{\theta}) = Var(\hat{\theta}) + bias^2(\hat{\theta})$$
:::
**Note:** MSE is a measure of precision and accuracy. 

Ideally we want an unbiased estimator with minimal variance. Often we have a [variance-bias tradeoff](https://en.wikipedia.org/wiki/Bias–variance_tradeoff), a situation in which when bias decreases, variance increases and viceversa. For example, for any distribution we have the following facts:

- $S^2$ is unbiased
- $\hat{\sigma^2}$ is biased
- $Var(\hat{\sigma^2}) < Var(S^2)$

::: {.definition}
Let $\hat{\theta}, \tilde{\theta}$ be two estimators of $\theta$. If $MSE(\hat{\theta}) \le MSE (\tilde{\theta}) \forall \theta$ and  $MSE(\hat{\theta}) < MSE (\tilde{\theta})$ for at least one $\theta$, then we say that $\tilde{\theta}$ is inadmissible. 
:::

::: {.definition}
Let $\hat{\theta}, \tilde{\theta}$ be any unbiased estimators of $\theta$ whose variances exist. Then, 

$$RE(\hat{\theta};\tilde{\theta}) = \frac{Var(\tilde{\theta})}{Var(\hat{\theta})}$$

is the *relative efficiency* of $\hat{\theta}$ to $\tilde{\theta}$. This allows use to compare variance between estimators
:::



## Uniform Minimum Variance Unbiased Estimators (UMVUEs)

Is it possible for there to be a "best" estimator with respect to MSE? If we restrict consideration only to unbiased estimators, then the answer may be yes. An unbiased estimator with the minimum possible variance among all possible $\theta$ is known as a *Uniform Minimum Variance Unbiased Estimator*, or UMVUE for short. Let's define this mathematically.

::: {.definition name="Uniform minimum variance unbiased estimator (UMVUE)"}
An estimator $W$ of a parameter $\tau(\theta)$ is called *uniform minimum variance unbiased estimator (UMVUE)* of $\tau(\theta)$ if $E_{\theta}(W)= \tau(\theta)$ and $Var_{\theta}(W) \le Var_{\theta}(Q)$ for any other unbiased estimator, Q, of $\tau(\theta)$.
:::

The UMVUE is the unbiased estimator of $\tau(\theta)$ with the minimum variance among all other unbiased estimators of $\tau(\theta)$. First, let's consider when the UMVUE exists - in this case, there are generally two strategies to construct the UMVUE.

### Finding UMVUEs I: Cramér-Rao Bound

If you can find an unbiased estimator of $\tau(\theta)$, one way to show that it is an UMVUE is by showing that it attains the minimum possible variance among all estimators. The Cramér-Rao Bound provides such a *lower* bound for the variance of all estimators of a parameter $\tau(\theta)$. This result is extremely powerful because if we can show that an *unbiased* estimator attains the *Cramér-Rao Lower Bound* (CRLB), then the estimator is UMVUE. 

::: {.definition name="Cramér-Rao Bound"}
Let $\textbf{X}=(X_1,..., X_n), \textbf{X} \sim f_{\textbf{X}}(x|\theta), \theta \in \Theta$. If $W=W(\textbf{X})$ is some estimator such that

\begin{equation}
\frac{\partial}{\partial\theta} E_{\theta}(W)=\int \frac{\partial}{\partial\theta} \bigg[ W(\textbf{X})f_{\textbf{X}}(x|\theta)\bigg]dx 
(\#eq:reg-cond-1)
\end{equation}

then, 

$$Var(W) \ge \frac{[\frac{\partial}{\partial\theta} E_{\theta}(W)]^2}{E_{\theta}([\frac{\partial}{\partial\theta} \log(f_{\textbf{X}}(x|\theta))]^2)}.$$
:::

**Remark:** If the sample is iid and 

$$
\begin{equation}
\frac{\partial^2}{\partial\theta^2} \int f(x|\theta) dx =  \int \frac{\partial^2}{\partial\theta^2} f(x|\theta) dx
(\#eq:reg-cond-2)
\end{equation}
$$
then, 
 $$Var(W) \ge \frac{[\frac{\partial}{\partial\theta} E_{\theta}(W)]^2}{nE_{\theta}[-\frac{\partial^2}{\partial\theta^2} ln(f_{X_i}(x|\theta))]}.$$
 
Before continuing, let's discuss conditions \@ref(eq:reg-cond-1) and \@ref(eq:reg-cond-2). First, if \@ref(eq:reg-cond-1) is satisfied then 
$$E_{\theta}\bigg[\frac{\partial}{\partial\theta}\log(f_{\textbf{X}}(X|\theta))\bigg] = 0,  \forall \theta \in \Theta.$$
Furthermore, if \@ref(eq:reg-cond-2) is satisfied, then 

$$E_{\theta}\bigg[\frac{\partial^2}{\partial\theta^2} \log(f(x|\theta))\bigg] = -E_{\theta}\bigg[\bigg(\frac{\partial}{\partial\theta} \log(f(x|\theta))\bigg)^2\bigg]. $$
**Remarks:** 

- \@ref(eq:reg-cond-2) is satisfied for exponential families. 
- If \@ref(eq:reg-cond-1) and \@ref(eq:reg-cond-2) are not satisfied, then the CRLB might not be the lower bound for the variance.
- Sometimes, especially for multiple parameters, we can also write the CRLB as $Var(W) \geq I(\theta)^{-1}$, the inverse of of the [information](#information) mentioned previously. Thus, if one is computing the MLE, the CRLB follows from the same calculations used for the MLE, which can save time, especially for problems involving common families of distributions.

::: {.example}
CRLB example
:::

Since the CRLB is a lower bound, if the variance of an unbiased estimator $Var(\hat{\theta})$ equals the CRLB, then it must be the UMVUE.  

While it is certainly possible to compute the CRLB directly, and then compute the variance of the estimator $Var(\hat{\theta})$, and check if these values are the same, there is also a simpler method. The *Attainment Theorem* tells us whether a specific estimator attains the CRLB without requiring that the bound actually be computed:

::: {.theorem name="Attainment Theorem"}
Let $W(X)$ be an unbiased $\tau(\theta)$ and suppose that \@ref(eq:reg-cond-1) holds. Then $W$ attains CRLB if and only if 

$$\frac{\partial}{\partial \theta} \log (\mathcal{L}(\theta|X)) = a(\theta)[W-\tau(\theta)]$$

for some $a(\theta)$
:::

As a result, to prove whether an estimator does *or does not* attain the CRLB, we simply must show that the [score](#score) of the sample can or cannot be factorized as above, which is often faster than computing the variance of the estimator.

::: {.example}
attainment theorem example
:::

**Note:** An estimator that does not attain CRLB can still be UMVUE! For some parameters, CRLB might not be achieved. 

### Rao-Blackwell and Lehmann-Scheffé Theorem

The more common approach to finding an UMVUE is to use the Lehmann-Scheffé theorem, which describes specifically how to construct an UMVUE using a complete sufficient statistic (as discussed in [Chapter 7](#statistics)). 

Before introducing the Lehmann-Scheffé theorem, first let us state the more general theorem upon which its proof is based:

::: {.theorem name=Rao-Blackwell}
Suppose $W$ is unbiased for $\tau(\theta)$ and $T$ is a SS for $\tau(\theta)$. Let $Y:=E(W|T)$. Then,

1. $Y$ is a statistic (not a function of $\theta$)
2. $E[Y] = \tau(\theta), \forall \theta$
3. $Var(Y)\le Var(W), \forall \theta$. 
:::

The Rao-Blackwell theorem provides a method to improve upon any unbiased estimator, i.e. provides a *better* unbiased estimator, by conditioning on a sufficient statistic. However, if this statistic is *also* complete, then the Lehmann-Scheffé  provides a method to find an UMVUE. This is because of the following theorem:

::: {.theorem}
If $E_{\theta}(W(X))= \tau(\theta)$, then $W(X)$ is best unbiased if and only if $W(X)$ is uncorrelated with all unbiased estimators of 0.
:::

How do we guarantee that $W$ is uncorrelated with all unbiased estimators of 0? A complete statistic contains no unbiased estimators of 0 (other than 0 itself). This is by definition; $E(g(T) = 0 \implies g(T) = 0$ with probability 1, as established in [Complete Statistics](#complete-stats). Therefore, if we condition on a complete statistic, then the resulting statistic must be uncorrelated with all unbiased estimators of 0, and therefore be an UMVUE. This principle forms the foundation of the Lehmann-Scheffé Theorem:

::: {.theorem name="Lehmann-Scheffé"}
Suppose $W$ is unbiased for $\tau(\theta)$ and $T$ is a *complete* and *sufficient* for $\tau(\theta)$. Then $Y:=E(W|T)$ is UMVUE of $\tau(\theta)$.

In addition, let $W$ be a statistic such that $Var(W)<\infty$, and $W$ is UMVUE for $\tau(\theta)$. Then $W$ is the unique UMVUE of $\tau(\theta)$. 

:::

### Finding UMVUEs II: Using the Lehmann-Scheffé Theorem

How would one use the Lehmann-Scheffé Theorem to construct an UMVUE in practice? Let's consider an example.

::: {.example name="Conditioning on a Complete Statistic"}
Let $X_i \overset{iid}{\sim} \text{Bernoulli}(p)$, and our goal is to find an UMVUE of $p^2$.  

First, we need to find an unbiased estimator of $p^2$. Since all observations are iid, $X_1\cdot X_2$ should suffice, as $E(X_1X_2) = E(X_1) \cdot E(X_2) = p^2$.

Since the Bernoulli is an exponential family with $w(\theta) = \log(\frac{p}{1-p})$ an open set, we know its CSS is $\sum_{i=1}^n X_i$ by Lehmann-Scheffé, the UMVUE is

$$E(X_1X_2 | \sum_{i=1}^n X_i = t)$$
Since the expected value of a Bernoulli can be written as a probability, this equals

$$
P(X_1X_2 = 1 | \sum_{i=1}^n X_i = t) = \frac{P(X_1 = 1, X_2 = 1, \sum_{i=3}^n X_i = t - 2)}{P(\sum_{i=1}^n X_i = t)} \\
= \frac{P(X_1=1) \cdot P(X_2=1) \cdot P(\sum_{i=3}^n X_i = t-2)}{{n\choose t}p^x(1-p)^{n-t}}\\
= \frac{p^2{n-2\choose t-2}p^{t-2}(1-p)^{n-2 - t+2}}{{n\choose t}p^t(1-p)^{n-t}}\\
= \frac{{n-2\choose t-2}}{{n\choose t}} = \frac{t(t-1)}{n(n-1)}\\
$$
using the fact that $\sum_{i=1}^nX_i \sim \text{Binomial}(n,p)$. Therefore, the UMVUE is $\frac{t(t-1)}{n(n-1)}$

:::



### Finding UMVUEs III: Lehmann-Scheffé Corollary

::: {.corollary name="Lehmann-Scheffé"}
Suppose $T$ is a complete sufficient statistic for $\theta$. Let $g(T)$ be an estimator based solely on $T$. Then, $g(T)$ is the UMVUE of its expected value - that is, $E(g(T))$ is an UMVUE.

:::

Rather than playing around with conditional probabilities to construct an UMVUE using Lehmann-Scheffé, it can often be much easier to "work backwards" based on what you know about the CSS and find a function $g$ such that $E(g(T)) = \tau(\theta)$. Here is an algorithm for using the Lehmann-Scheffé corollary for a function of a parameter $\tau(\theta)$ given $X_1, ..., X_n \sim f_{X_i}(x|\theta)$

1. Find the CSS of the distribution $f_{X_i}(x|\theta)$. Call it $T$.
2. Find the distribution of $T$. Often, these are known based on convolution formulas. See Chapter 7 for a [table of sufficient statistic distributions](##table-ss).
3. Set $E(g(T)) = \tau(\theta)$, and rearrange to get $E\Big(\frac{g(T)}{\tau(\theta)}\Big) = 1$.
4. (Optional) Use the linearity of expectation to split the above into multiple integration problems. For example, if $\tau(\theta) = \tau_1(\theta) + \tau_2(\theta) + ... + \tau_k(\theta)$, then we can let $E(g(T)) = E(g_1(T)) + E(g_2(T)) + ... + E(g_k(T))$ with each $E(g_1(T)) = \tau_i(\theta)$
5. Because $E\Big(\frac{g(T)}{\tau(\theta)}\Big)$ must integrate to 1, $\frac{g(T)}{\tau(\theta)}$ multiplied by the integrand must be a pdf. Therefore, you can use the [Kernel Technique](#kernel-technique) to solve for the $g(T)$ that converts the rest of the integrand into a pdf.

That last step probably sounds a bit complicated. Let's look at an example.

::: {.example name="Constructing an UMVUE using a function of a CSS"}

Suppose $X_i \sim \text{Exp}(\lambda)$. Find an UMVUE for $\tau(\lambda) = \lambda^k$ with $k \in \{1, ... n\}$ using the algorithm above.  

Let us proceed step-by-step.  

1. Since the exponential distribution is (trivially) a single-parameter exponential family, $T = \sum_{i=1}^nt_1(X) = \sum_{i=1}^nX_i$ is a sufficient statistic.
2. It is known, [by convolution properties](#table-SS), that $T = \sum_{i=1}^nX_i \sim \text{Gamma}(n, \lambda)$. Hence, we've found the distribution of $T$
3. Let $E\Big(\frac{g(T)}{\tau(\theta)}\Big) = 1$, which implies

$$
E\Big(\frac{g(T)}{\tau(\theta)}\Big) = \int_0^\infty\frac{g(t)}{\lambda^k}\cdot\frac{1}{\Gamma(n)\lambda^n}t^{n-1}\exp(-\frac{t}{\lambda})dt = 1\\
\implies \int_0^\infty g(t)\cdot\frac{1}{\Gamma(n)\lambda^{n+k}}t^{n-1}\exp(-\frac{t}{\lambda})dt = 1
$$

5. Clearly this can be converted into a $\text{Gamma}(n+k, \lambda)$ pdf by setting $g(t)$ appropriately. Let's algebraically manipulate to insert $n+k$ where it is needed, and keep track of our transformations.  

  1. Since $n$ and $k$ are integers, $\frac{1}{\Gamma(n+k)} = \frac{1}{(n+k)(n+k-1)...(n+1)\Gamma(n)}$
  2. $t^{n + k -1} = t^{n - 1}\cdot t^k$
  
These transformations give us the $\text{Gamma}(n+k, \lambda)$ pdf we need. Let $g(T) = \frac{T^k}{(n+k)(n+k-1)...(n+1)}$. Then, by the Lehmann Scheffé Corollary, $g(T)$ must be an UMVUE for $\tau(\lambda)$, since $E(g(T)) = \lambda^k$ and $T$ is a complete sufficient statistic.

:::


### Proving an UMVUE Does Not Exist


## Inferential Properties of Exponential Families Distributions

Suppose we draw a sample of $n$ iid random variables $X_1,...,X_n$ following one of the distributions below. The proceeding tables list the inferential properties of this sample.

### Bernoulli

|   |   |
| :-----------: | :-----------: | 
|  Log-likelihood |  $\ell(\theta|X) = n\log(1-p) + \log(\frac{p}{1-p})\sum_{i=1}^n x_i$ |
|   Score Equations | $U_n(\theta|X) = -\frac{n}{1-p} + \frac{1}{p(1-p)}\sum_{i=1}^n x_i$ |
| Observed Information | $-\frac{\partial^2}{\partial\theta^2}\ell(\theta|X) = \frac{n}{(1-p)^2} + \frac{1-2p}{p^2(1-p)^2}\sum_{i=1}^n x_i$ | 
|  Fisher Information | $I(\theta)= \frac{n}{p(1-p)}$ |
|   MLE  | $\frac{1}{n}\sum_{i=1}^n x_i$ |

### Binomial

Since the Binomial has $n$ as a parameter, notation in problems that involve a *sample* of $n$ iid Binomial random variables can be tricky. To clarify, in the following table let $X_i \sim \text{Binomial}(m, p)$, and let $n$ represent the number of samples. 

|   |   |
| :-----------: | :-----------: | 
|  Log-likelihood |  $\ell(\theta|X) = nm\log(1-p) + \log(\frac{p}{1-p})\sum_{i=1}^n x_i + \log({m\choose x_i})$ |
|   Score Equations | $U_n(\theta|X) = -\frac{nm}{1-p} + \frac{1}{p(1-p)}\sum_{i=1}^n x_i$ |
| Observed Information | $-\frac{\partial^2}{\partial\theta^2}\ell(\theta|X) = \frac{nm}{(1-p)^2} + \frac{1-2p}{p^2(1-p)^2}\sum_{i=1}^n x_i$ | 
|  Fisher Information | $I(\theta)= \frac{nm}{p(1-p)}$ |
|   MLE  | $\frac{1}{nm}\sum_{i=1}^n x_i$ |

### Geometric

|   |   |
| :-----------: | :-----------: | 
|  Log-likelihood |  $\ell(\theta|X) = n\log(p) + \log(1-p)\sum_{i=1}^n x_i$ |
|   Score Equations | $U_n(\theta|X) = \frac{n}{p} - \frac{1}{1-p}\sum_{i=1}^n x_i$ |
| Observed Information | | 
|  Fisher Information | |
|   MLE  | $\frac{n}{\sum_{i=1}^n x_i}$ |


### Negative Binomial

|   |   |
| :-----------: | :-----------: | 
|  Log-likelihood |  $\ell(\theta|X) = nr\log(\frac{p}{1-p}) + \sum_{i=1}^n x_i\log(1-p) + \log{x_i + r - 1\choose x_i}$ |
|   Score Equations | $U_n(\theta|X) = -\frac{nr}{p} - \frac{1}{1-p}\sum_{i=1}^n x_i$ |
| Observed Information | | 
|  Fisher Information | $\frac{r}{(1-p)^2p}$ |
|   MLE  | $\frac{1}{1 - \frac{1}{nr}\sum_{i=1}^nx_i}$ |

### Poisson

|   |   |
| :-----------: | :-----------: | 
|  Log-likelihood |  $\ell(\theta|X) = n\lambda + \sum_{i=1}^n x_i\log(\lambda) - \log(x_i!)$ |
|   Score Equations | $U_n(\theta|X) = -n + \frac{1}{\lambda}x_i$ |
| Observed Information | | 
|  Fisher Information | $\frac{1}{\lambda}$ |
|   MLE  | $\frac{1}{n}\sum_{i=1}^nx_i$ |

### Normal

|   |   |
| :-----------: | :-----------: | 
|  Log-likelihood |  $\ell(\theta|X) = -\frac{n}{2}\log(2\pi\sigma^2) - \frac{1}{2\sigma^2}\sum_{i=1}^n (x_i - \mu)^2$ |
|   Score Equations | $U(\mu | x, \sigma^2) = -n\mu -\frac{1}{\sigma^2}\sum_{i=1}^n x_i \\ U(\sigma^2 | x, \mu) = -\frac{n}{2\sigma^2} + \frac{1}{2(\sigma^2)^2}\sum_{i=1}^n(x_i-\mu)^2$ |
| Observed Information | | 
|  Fisher Information | $\begin{bmatrix}\frac{1}{\sigma^2} & 0 \\ 0 & \frac{1}{2\sigma^4}\end{bmatrix}$ |
|   MLE  | $\frac{1}{n}\sum_{i=1}^nx_i$ |

### Exponential

|   |   |
| :-----------: | :-----------: | 
|  Log-likelihood |  $\ell(\theta|X) = -n\log(\lambda) - \frac{1}{\lambda}\sum_{i=1}^n x_i$ |
|   Score Equations | $U(\mu | x, \sigma^2) = -\frac{n}{\lambda} + \frac{1}{\lambda^2}\sum_{i=1}^n x_i$ |
| Observed Information | | 
|  Fisher Information | $\lambda^2$ |
|   MLE  | $\frac{1}{n}\sum_{i=1}^nx_i$ |

### Gamma

|   |   |
| :-----------: | :-----------: | 
|  Log-likelihood |  $\ell(\theta|X) = -n\log(\Gamma(k)) - nk\log(\lambda) + (k - 1 - \frac{1}{\lambda})\sum_{i=1}^n x_i$ |
|   Score Equations | $U(\mu | x, \sigma^2) = -\frac{nk}{\lambda} + \frac{1}{\lambda^2}\sum_{i=1}^n x_i$ with $k$ known (otherwise, requires differentiating $\Gamma(k)$) |
| Observed Information | | 
|  Fisher Information | $\begin{bmatrix}\psi^{(1)}(k) & \frac{1}{\lambda}\\ \frac{1}{\lambda} & \frac{k}{\lambda^2}\end{bmatrix}$ |
|   MLE  | |

### Pareto

|   |   |
| :-----------: | :-----------: | 
|  Log-likelihood |  $\ell(\theta|X) = n\log{\alpha} + n\alpha\log(x_m) - (\alpha + 1)\sum_{i=1}^n\log(x_i)$ |
|   Score Equations | $U(\mu | x, \sigma^2) = \frac{n}{\alpha} + n\log(x_m) - \sum_{i=1}^n \log(x_i)$ |
| Observed Information | | 
|  Fisher Information | $\frac{n}{\alpha^2}$ | $\frac{n}{\sum_{i=1}^n\log(x_i)}$ |
|   MLE  | |
