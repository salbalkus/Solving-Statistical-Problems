# Point Estimators: Finite Samples {#point-estimators-finite-samples}

Finding point estimators and evaluating their finite sample properties.



## Method of Moments

## MLE Theory

:::{.definition name="Likelihood"}

Let $X$ be a random variable or random vector, and $\theta$ a parameter or vector of parameters describing the distribution of $X$. Then, the **likelihood** is

$$\mathcal{L}(\theta | X) = f_X(x|\theta)$$

:::

:::{.definition name="Log-Likelihood"}
Let $X$ be a random variable or random vector, and $\theta$ a parameter or vector of parameters describing the distribution of $X$. Then, the **log-likelihood** is
$$\ell(\theta|X) = \log\mathcal{L}(\theta | X) = \log f_X(x|\theta)$$
:::

The log-likelihood is often easier to use. Furthermore, When $X$ is an iid sample $X_1, X_2, ... X_n$, by the properties of logarithms, $\ell(\theta|X) = \log\Big(\prod_{i=1}^nf_{X_i}(x|\theta)\Big) = \sum_{i=1}^n \log f_{X_i}(x|\theta)$. Working with a sum is much easier than with a product.

Of course, the whole reason this is permissible in finding a maximum likelihood estimate is that the $\log$ function is monotonic - finding the maximum of a function is equivalent to finding the maximum of its logarithm.

Maximizing this likelihood often requires calculus - specifically, the First Derivative Test. In statistics, the gradient of the log-likelihood has a special name: the **score**.

:::{.definition name="Score Equations"}
If $\theta$ is a vector of parameters, the **score equations** are defined as the gradient of the log-likelihood (as described above). That is,

$$U(\theta | X) = \begin{bmatrix}\frac{\partial}{\partial\theta_1}\ell(\theta_1|X) & ... & \frac{\partial}{\partial\theta_k}\ell(\theta_k|X) \end{bmatrix}$$
Of course, if $\theta$ is a single parameter, then $U(\theta|X) = \frac{d}{d\theta}\ell(\theta|X)$

:::

Therefore, the first step in finding an MLE is to set the score to 0, and solve for the desired parameter. Let's see an example.

:::{.example name="MLE of _______"}
example
:::


The score has two useful properties in MLE theory. First, under [regularity conditions](#regularity-conditions), $E(U(\theta|X)) = 0$ - its mean is 0. This is because the regularity conditions imply [Leibniz's rule](#leibniz-rule); that is,

\begin{align}
E(U(\theta|X)) = \int_{\mathcal{X}}\frac{\partial}{\partial\theta}\log\mathcal{L}(\theta|X)f(x|\theta)dx \\
= \int_{\mathcal{X}}\frac{1}{f(x|\theta)}\frac{\partial}{\partial\theta}f(x|\theta)f(x|\theta)dx \\ 
= \frac{\partial}{\partial\theta}\frac{f(x|\theta)}{f(x|\theta)}f(x|\theta)dx = \frac{\partial}{\partial\theta}1 = 0
\end{align}

The second useful property is that, under the same regularity conditions, the variance of the score is

\begin{align}
Var(U(\theta|X)) = E(U(\theta|X)U(\theta|X)^\top) \\
= -E\Big(\frac{\partial^2}{\partial\partial^\top}\ell(\theta|X)\Big) 
\end{align}

If $\theta$ is one-dimensional, $Var(U(\theta|X)) = E(U(\theta|X)^2) = -E\Big(\frac{\partial^2}{\partial\theta^2}\ell(\theta|X)\Big)$. The rather insightful derivation of this is available [here](https://en.wikipedia.org/wiki/Score_(statistics)#Variance). This variance has a special name, the **information** (or sometimes, "Fisher Information")

:::{.definition name="Information"}

In statistics, **information** refers to the amount of information that a random variable $X$ contains about a parameter $\theta$. It is defined mathematically as the variance of the score, which is

$$I(\theta) = E\Big((\frac{\partial}{\partial\theta}\log f(X|\theta))^2\Big|\theta\Big)$$
Under MLE regularity conditions, 

$$I(\theta) = -E\Big(\frac{\partial^2}{\partial\theta^2}\log f(X|\theta) \Big|\theta\Big)$$
:::

As the mean value of a second derivative, the information measures the curve of $\ell(\theta|X)$ *(expand?)*

The negative second derivative of the log-likelihood also has a special name itself: the **observed information**.

:::{.definition name="Observed Information"}
The observed information is defined as

$$\mathcal{J}(\theta|X) = -\frac{\partial^2}{\partial\theta^2}\ell(\theta|X)$$
When $\theta$ is a $k$-dimensional vector, the observed information is the Hessian of the log-likelihood:

$$\mathcal{J}(\theta|X) = -\nabla\nabla^\top\ell(\theta|X) = \begin{bmatrix}\frac{\partial^2}{\partial\theta_1^2} & ... & \frac{\partial^2}{\partial\theta_1\partial\theta_k}\\ \vdots & & \vdots\\ \frac{\partial^2}{\partial\theta_k\partial\theta_1} & ... & \frac{\partial^2}{\partial\theta_k^2}\end{bmatrix}$$

:::

The observed information can be used to estimate the Fisher information for a given sample $X$.

## Unbiasedness

## Minimum Variance (Cramer-Rao Lower Bound)

## Uniform Minimum Variance Unbiased Estimators (UMVUEs)

::: {.definition name="Lehmann-Scheffe Theorem"}
This theorem

:::

## Inferential Properties of Exponential Families Distributions

Suppose we draw a sample of $n$ iid random variables $X_1,...,X_n$ following one of the distributions below. The proceeding tables list the inferential properties of this sample.

### Bernoulli

|   |   |
| :-----------: | :-----------: | 
|  Log-likelihood |  $\ell(\theta|X) = n\log(1-p) + \log(\frac{p}{1-p})\sum_{i=1}^n x_i$ |
|   Score Equations | $U_n(\theta|X) = -\frac{n}{1-p} + \frac{1}{p(1-p)}\sum_{i=1}^n x_i$ |
| Observed Information | $-\frac{\partial^2}{\partial\theta^2}\ell(\theta|X) = \frac{n}{(1-p)^2} + \frac{1-2p}{p^2(1-p)^2}\sum_{i=1}^n x_i$ | 
|  Fisher Information | $I(\theta)= \frac{n}{p(1-p)}$ |
|   MLE  | $\frac{1}{n}\sum_{i=1}^n x_i$ |

### Binomial

Since the Binomial has $n$ as a parameter, notation in problems that involve a *sample* of $n$ iid Binomial random variables can be tricky. To clarify, in the following table let $X_i \sim \text{Binomial}(m, p)$, and let $n$ represent the number of samples. 

|   |   |
| :-----------: | :-----------: | 
|  Log-likelihood |  $\ell(\theta|X) = nm\log(1-p) + \log(\frac{p}{1-p})\sum_{i=1}^n x_i + \log({m\choose x_i})$ |
|   Score Equations | $U_n(\theta|X) = -\frac{nm}{1-p} + \frac{1}{p(1-p)}\sum_{i=1}^n x_i$ |
| Observed Information | $-\frac{\partial^2}{\partial\theta^2}\ell(\theta|X) = \frac{nm}{(1-p)^2} + \frac{1-2p}{p^2(1-p)^2}\sum_{i=1}^n x_i$ | 
|  Fisher Information | $I(\theta)= \frac{nm}{p(1-p)}$ |
|   MLE  | $\frac{1}{nm}\sum_{i=1}^n x_i$ |

### Geometric

|                   Log-likelihood                   |                   Score Equations                   | Fisher Information |              MLE              |
|:----------------:|:----------------:|:----------------:|:----------------:|
| $n\log(p) + \log(1-p)\sum_{i=1}^n x_i$ | $\frac{n}{p} - \frac{1}{1-p}\sum_{i=1}^n x_i$ |  | $\frac{n}{\sum_{i=1}^n x_i}$ |

### Negative Binomial

|                   Log-likelihood                   |                   Score Equations                   | Fisher Information |              MLE              |
|:----------------:|:----------------:|:----------------:|:----------------:|
| $nr\log(\frac{p}{1-p}) + \sum_{i=1}^n x_i\log(1-p) + \log{x_i + r - 1\choose x_i}$ | $-\frac{nr}{p} - \frac{1}{1-p}\sum_{i=1}^n x_i$ | $\frac{r}{(1-p)^2p}$ | $\frac{1}{1 - \frac{1}{nr}\sum_{i=1}^nx_i}$ |

### Poisson

|                   Log-likelihood                   |                   Score Equations                   | Fisher Information |              MLE              |
|:----------------:|:----------------:|:----------------:|:----------------:|
| $n\lambda + \sum_{i=1}^n x_i\log(\lambda) - \log(x_i!)$ | $-n + \frac{1}{\lambda}x_i$ | $\frac{1}{\lambda}$ | $\frac{1}{n}\sum_{i=1}^nx_i$ |

### Normal

|                   Log-likelihood                   |                   Score Equations                   | Fisher Information |              MLE              |
|:----------------:|:----------------:|:----------------:|:----------------:|
| $-\frac{n}{2}\log(2\pi\sigma^2) - \frac{1}{2\sigma^2}\sum_{i=1}^n (x_i - \mu)^2$ | $U(\mu | x, \sigma^2) = -n\mu -\frac{1}{\sigma^2}\sum_{i=1}^n x_i \\ U(\sigma^2 | x, \mu) = -\frac{n}{2\sigma^2} + \frac{1}{2(\sigma^2)^2}\sum_{i=1}^n(x_i-\mu)^2$ | $\begin{bmatrix}\frac{1}{\sigma^2} & 0 \\ 0 & \frac{1}{2\sigma^4}\end{bmatrix}$ | $\frac{1}{n}\sum_{i=1}^nx_i$ |

### Exponential

|                   Log-likelihood                   |                   Score Equations                   | Fisher Information |              MLE              |
|:----------------:|:----------------:|:----------------:|:----------------:|
| $-n\log(\lambda) - \frac{1}{\lambda}\sum_{i=1}^n x_i$ | $-\frac{n}{\lambda} + \frac{1}{\lambda^2}\sum_{i=1}^n x_i$ | $\lambda^2$ | $\frac{1}{n}\sum_{i=1}^nx_i$ |

### Gamma

|                   Log-likelihood                   |                   Score Equations                   | Fisher Information |              MLE              |
|:----------------:|:----------------:|:----------------:|:----------------:|
| $-n\log(\Gamma(k)) - nk\log(\lambda) + (k - 1 - \frac{1}{\lambda})\sum_{i=1}^n x_i$ | $-\frac{nk}{\lambda} + \frac{1}{\lambda^2}\sum_{i=1}^n x_i$ with $k$ known (otherwise, requires differentiating $\Gamma(k)$) | $\begin{bmatrix}\psi^{(1)}(k) & \frac{1}{\lambda}\\ \frac{1}{\lambda} & \frac{k}{\lambda^2}\end{bmatrix}$ |  |

### Pareto

|                   Log-likelihood                   |                   Score Equations                   | Fisher Information |              MLE              |
|:----------------:|:----------------:|:----------------:|:----------------:|
| $n\log{\alpha} + n\alpha\log(x_m) - (\alpha + 1)\sum_{i=1}^n\log(x_i)$ | $U(\alpha | x_i) = \frac{n}{\alpha} + n\log(x_m) - \sum_{i=1}^n \log(x_i)$ | $\frac{n}{\alpha^2}$ | $\frac{n}{\sum_{i=1}^n\log(x_i)}$
