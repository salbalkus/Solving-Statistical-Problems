<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 7 Statistics | Solving Statistical Problems</title>
  <meta name="description" content="Chapter 7 Statistics | Solving Statistical Problems" />
  <meta name="generator" content="bookdown 0.34 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 7 Statistics | Solving Statistical Problems" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="Chapter 7 Statistics | Solving Statistical Problems" />
  <meta name="github-repo" content="salbalkus/Solving-Statistical-Problems" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 7 Statistics | Solving Statistical Problems" />
  
  <meta name="twitter:description" content="Chapter 7 Statistics | Solving Statistical Problems" />
  

<meta name="author" content="Salvador Balkus, Kimberly Greco, and Mónica Robles Fontán" />


<meta name="date" content="2023-07-22" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="moments.html"/>
<link rel="next" href="point-estimators-finite-samples.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>
<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Solving Statistical Problems</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Introduction</a></li>
<li class="chapter" data-level="2" data-path="math-tricks.html"><a href="math-tricks.html"><i class="fa fa-check"></i><b>2</b> Math Tricks</a>
<ul>
<li class="chapter" data-level="2.1" data-path="math-tricks.html"><a href="math-tricks.html#combinatorics"><i class="fa fa-check"></i><b>2.1</b> Combinatorics</a></li>
<li class="chapter" data-level="2.2" data-path="math-tricks.html"><a href="math-tricks.html#binomial-and-multinomial-theorems"><i class="fa fa-check"></i><b>2.2</b> Binomial and Multinomial Theorems</a></li>
<li class="chapter" data-level="2.3" data-path="math-tricks.html"><a href="math-tricks.html#geometric-series"><i class="fa fa-check"></i><b>2.3</b> Geometric Series</a></li>
<li class="chapter" data-level="2.4" data-path="math-tricks.html"><a href="math-tricks.html#exponential-taylor"><i class="fa fa-check"></i><b>2.4</b> Taylor Series for Exponential Function</a></li>
<li class="chapter" data-level="2.5" data-path="math-tricks.html"><a href="math-tricks.html#taylors-formula"><i class="fa fa-check"></i><b>2.5</b> Taylor’s Formula</a></li>
<li class="chapter" data-level="2.6" data-path="math-tricks.html"><a href="math-tricks.html#exponential-limit"><i class="fa fa-check"></i><b>2.6</b> Exponential Limit</a></li>
<li class="chapter" data-level="2.7" data-path="math-tricks.html"><a href="math-tricks.html#ibp"><i class="fa fa-check"></i><b>2.7</b> Integration by Parts</a></li>
<li class="chapter" data-level="2.8" data-path="math-tricks.html"><a href="math-tricks.html#leibniz-rule"><i class="fa fa-check"></i><b>2.8</b> Leibniz’s Rule</a></li>
<li class="chapter" data-level="2.9" data-path="math-tricks.html"><a href="math-tricks.html#fubinis-theorem"><i class="fa fa-check"></i><b>2.9</b> Fubini’s Theorem</a></li>
<li class="chapter" data-level="2.10" data-path="math-tricks.html"><a href="math-tricks.html#gamma-function"><i class="fa fa-check"></i><b>2.10</b> Gamma Function</a></li>
<li class="chapter" data-level="2.11" data-path="math-tricks.html"><a href="math-tricks.html#triangle-inequality"><i class="fa fa-check"></i><b>2.11</b> Triangle Inequality</a></li>
<li class="chapter" data-level="2.12" data-path="math-tricks.html"><a href="math-tricks.html#constrained-optimization"><i class="fa fa-check"></i><b>2.12</b> Constrained Optimization</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="probability.html"><a href="probability.html"><i class="fa fa-check"></i><b>3</b> Probability</a>
<ul>
<li class="chapter" data-level="3.1" data-path="probability.html"><a href="probability.html#basic-axioms"><i class="fa fa-check"></i><b>3.1</b> Basic Axioms</a></li>
<li class="chapter" data-level="3.2" data-path="probability.html"><a href="probability.html#basic-probability-solving-techniques"><i class="fa fa-check"></i><b>3.2</b> Basic Probability Solving Techniques</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="probability.html"><a href="probability.html#disjointify"><i class="fa fa-check"></i><b>3.2.1</b> Disjointification</a></li>
<li class="chapter" data-level="3.2.2" data-path="probability.html"><a href="probability.html#demorgan"><i class="fa fa-check"></i><b>3.2.2</b> DeMorgan’s Laws</a></li>
<li class="chapter" data-level="3.2.3" data-path="probability.html"><a href="probability.html#proving-inequalities-subsetting"><i class="fa fa-check"></i><b>3.2.3</b> Proving Inequalities: Subsetting</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="probability.html"><a href="probability.html#conditional-probability"><i class="fa fa-check"></i><b>3.3</b> Conditional Probability</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="probability.html"><a href="probability.html#conditional-probability-in-practice"><i class="fa fa-check"></i><b>3.3.1</b> Conditional Probability in Practice</a></li>
<li class="chapter" data-level="3.6.4" data-path="probability.html"><a href="probability.html#important-theorems"><i class="fa fa-check"></i><b>3.6.4</b> Important Theorems</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="known-distributions.html"><a href="known-distributions.html"><i class="fa fa-check"></i><b>4</b> Known Distributions</a>
<ul>
<li class="chapter" data-level="4.1" data-path="known-distributions.html"><a href="known-distributions.html#families-of-distributions"><i class="fa fa-check"></i><b>4.1</b> Families of Distributions</a></li>
<li class="chapter" data-level="4.2" data-path="known-distributions.html"><a href="known-distributions.html#location-scale"><i class="fa fa-check"></i><b>4.2</b> Location and Scale Families</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="known-distributions.html"><a href="known-distributions.html#location-families"><i class="fa fa-check"></i><b>4.2.1</b> Location Families</a></li>
<li class="chapter" data-level="4.2.2" data-path="known-distributions.html"><a href="known-distributions.html#scale-families"><i class="fa fa-check"></i><b>4.2.2</b> Scale Families</a></li>
<li class="chapter" data-level="4.2.3" data-path="known-distributions.html"><a href="known-distributions.html#properties-of-location-scale-families"><i class="fa fa-check"></i><b>4.2.3</b> Properties of Location-Scale Families</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="known-distributions.html"><a href="known-distributions.html#exponential-family"><i class="fa fa-check"></i><b>4.3</b> Exponential Families</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="known-distributions.html"><a href="known-distributions.html#properties"><i class="fa fa-check"></i><b>4.3.1</b> Properties</a></li>
<li class="chapter" data-level="4.3.2" data-path="known-distributions.html"><a href="known-distributions.html#natural-exponential-family"><i class="fa fa-check"></i><b>4.3.2</b> Natural Exponential Families</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="known-distributions.html"><a href="known-distributions.html#known-univariate-exponential-families"><i class="fa fa-check"></i><b>4.4</b> Known Univariate Exponential Families</a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="known-distributions.html"><a href="known-distributions.html#bernoulli"><i class="fa fa-check"></i><b>4.4.1</b> Bernoulli</a></li>
<li class="chapter" data-level="4.4.2" data-path="known-distributions.html"><a href="known-distributions.html#binomial"><i class="fa fa-check"></i><b>4.4.2</b> Binomial</a></li>
<li class="chapter" data-level="4.4.3" data-path="known-distributions.html"><a href="known-distributions.html#geometric-distribution"><i class="fa fa-check"></i><b>4.4.3</b> Geometric</a></li>
<li class="chapter" data-level="4.4.4" data-path="known-distributions.html"><a href="known-distributions.html#negative-binomial"><i class="fa fa-check"></i><b>4.4.4</b> Negative Binomial</a></li>
<li class="chapter" data-level="4.4.5" data-path="known-distributions.html"><a href="known-distributions.html#poisson"><i class="fa fa-check"></i><b>4.4.5</b> Poisson</a></li>
<li class="chapter" data-level="4.4.6" data-path="known-distributions.html"><a href="known-distributions.html#normal"><i class="fa fa-check"></i><b>4.4.6</b> Normal</a></li>
<li class="chapter" data-level="4.4.7" data-path="known-distributions.html"><a href="known-distributions.html#exponential"><i class="fa fa-check"></i><b>4.4.7</b> Exponential</a></li>
<li class="chapter" data-level="4.4.8" data-path="known-distributions.html"><a href="known-distributions.html#gamma"><i class="fa fa-check"></i><b>4.4.8</b> Gamma</a></li>
<li class="chapter" data-level="4.4.9" data-path="known-distributions.html"><a href="known-distributions.html#beta"><i class="fa fa-check"></i><b>4.4.9</b> Beta</a></li>
<li class="chapter" data-level="4.4.10" data-path="known-distributions.html"><a href="known-distributions.html#chi-squared"><i class="fa fa-check"></i><b>4.4.10</b> Chi-squared</a></li>
<li class="chapter" data-level="4.4.11" data-path="known-distributions.html"><a href="known-distributions.html#weibull"><i class="fa fa-check"></i><b>4.4.11</b> Weibull</a></li>
<li class="chapter" data-level="4.4.12" data-path="known-distributions.html"><a href="known-distributions.html#pareto"><i class="fa fa-check"></i><b>4.4.12</b> Pareto</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="known-distributions.html"><a href="known-distributions.html#non-exponential-families"><i class="fa fa-check"></i><b>4.5</b> Non-exponential families</a>
<ul>
<li class="chapter" data-level="4.5.1" data-path="known-distributions.html"><a href="known-distributions.html#uniform"><i class="fa fa-check"></i><b>4.5.1</b> Uniform</a></li>
<li class="chapter" data-level="4.5.2" data-path="known-distributions.html"><a href="known-distributions.html#cauchy"><i class="fa fa-check"></i><b>4.5.2</b> Cauchy</a></li>
<li class="chapter" data-level="4.5.3" data-path="known-distributions.html"><a href="known-distributions.html#studentst"><i class="fa fa-check"></i><b>4.5.3</b> t-distribution</a></li>
<li class="chapter" data-level="4.5.4" data-path="known-distributions.html"><a href="known-distributions.html#f-distribution"><i class="fa fa-check"></i><b>4.5.4</b> F-distribution</a></li>
<li class="chapter" data-level="4.5.5" data-path="known-distributions.html"><a href="known-distributions.html#hypergeometric"><i class="fa fa-check"></i><b>4.5.5</b> Hypergeometric</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="known-distributions.html"><a href="known-distributions.html#multivariate-distributions"><i class="fa fa-check"></i><b>4.6</b> Multivariate Distributions</a>
<ul>
<li class="chapter" data-level="4.6.1" data-path="known-distributions.html"><a href="known-distributions.html#bivariate-normal"><i class="fa fa-check"></i><b>4.6.1</b> Bivariate Normal</a></li>
<li class="chapter" data-level="4.6.2" data-path="known-distributions.html"><a href="known-distributions.html#multivariate-normal"><i class="fa fa-check"></i><b>4.6.2</b> Multivariate Normal</a></li>
<li class="chapter" data-level="4.6.3" data-path="known-distributions.html"><a href="known-distributions.html#multinomial"><i class="fa fa-check"></i><b>4.6.3</b> Multinomial</a></li>
</ul></li>
<li class="chapter" data-level="4.7" data-path="known-distributions.html"><a href="known-distributions.html#medians-and-other-functionals-of-a-distribution"><i class="fa fa-check"></i><b>4.7</b> Medians and Other Functionals of a Distribution</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="new-distributions.html"><a href="new-distributions.html"><i class="fa fa-check"></i><b>5</b> New Distributions</a>
<ul>
<li class="chapter" data-level="5.1" data-path="new-distributions.html"><a href="new-distributions.html#transformations"><i class="fa fa-check"></i><b>5.1</b> Transformations</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="new-distributions.html"><a href="new-distributions.html#theorems"><i class="fa fa-check"></i><b>5.1.1</b> Theorems</a></li>
<li class="chapter" data-level="5.1.2" data-path="new-distributions.html"><a href="new-distributions.html#practical-strategy"><i class="fa fa-check"></i><b>5.1.2</b> Practical Strategy</a></li>
<li class="chapter" data-level="5.1.3" data-path="new-distributions.html"><a href="new-distributions.html#proving-independence-from-a-joint-transformation"><i class="fa fa-check"></i><b>5.1.3</b> Proving Independence From a Joint Transformation</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="new-distributions.html"><a href="new-distributions.html#computing-joint-probabilities"><i class="fa fa-check"></i><b>5.2</b> Computing Joint Probabilities</a></li>
<li class="chapter" data-level="5.3" data-path="new-distributions.html"><a href="new-distributions.html#probability-integral-transform"><i class="fa fa-check"></i><b>5.3</b> Probability Integral Transform</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="new-distributions.html"><a href="new-distributions.html#hierarchical-models-iterated-moments"><i class="fa fa-check"></i><b>5.3.1</b> Hierarchical Models (Iterated Moments)</a></li>
<li class="chapter" data-level="5.3.2" data-path="new-distributions.html"><a href="new-distributions.html#convolutions"><i class="fa fa-check"></i><b>5.3.2</b> Convolutions</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="moments.html"><a href="moments.html"><i class="fa fa-check"></i><b>6</b> Moments</a>
<ul>
<li class="chapter" data-level="6.1" data-path="moments.html"><a href="moments.html#basic-definitions"><i class="fa fa-check"></i><b>6.1</b> Basic Definitions</a></li>
<li class="chapter" data-level="6.2" data-path="moments.html"><a href="moments.html#expected-value"><i class="fa fa-check"></i><b>6.2</b> <span class="math inline">\(E(X)\)</span> Properties</a></li>
<li class="chapter" data-level="6.3" data-path="moments.html"><a href="moments.html#varx-properties"><i class="fa fa-check"></i><b>6.3</b> <span class="math inline">\(Var(X)\)</span> Properties</a></li>
<li class="chapter" data-level="6.4" data-path="moments.html"><a href="moments.html#covariance-and-correlation"><i class="fa fa-check"></i><b>6.4</b> Covariance and Correlation</a></li>
<li class="chapter" data-level="6.5" data-path="moments.html"><a href="moments.html#conditional-expectation"><i class="fa fa-check"></i><b>6.5</b> Conditional Expectation</a></li>
<li class="chapter" data-level="6.6" data-path="moments.html"><a href="moments.html#mgf"><i class="fa fa-check"></i><b>6.6</b> Moment Generating Functions</a></li>
<li class="chapter" data-level="6.7" data-path="moments.html"><a href="moments.html#moment-bounds"><i class="fa fa-check"></i><b>6.7</b> Moment Inequalities</a></li>
<li class="chapter" data-level="6.8" data-path="moments.html"><a href="moments.html#techniques-for-deriving-moments"><i class="fa fa-check"></i><b>6.8</b> Techniques for Deriving Moments</a>
<ul>
<li class="chapter" data-level="6.8.1" data-path="moments.html"><a href="moments.html#bernoulli-direct-summation"><i class="fa fa-check"></i><b>6.8.1</b> Bernoulli: Direct Summation</a></li>
<li class="chapter" data-level="6.8.2" data-path="moments.html"><a href="moments.html#uniform-direct-integration"><i class="fa fa-check"></i><b>6.8.2</b> Uniform: Direct Integration</a></li>
<li class="chapter" data-level="6.8.3" data-path="moments.html"><a href="moments.html#geometric-series-convergence"><i class="fa fa-check"></i><b>6.8.3</b> Geometric: Series Convergence</a></li>
<li class="chapter" data-level="6.8.4" data-path="moments.html"><a href="moments.html#binomial-kernel-technique-series-version"><i class="fa fa-check"></i><b>6.8.4</b> Binomial: Kernel Technique, Series Version</a></li>
<li class="chapter" data-level="6.8.5" data-path="moments.html"><a href="moments.html#negative-binomial-and-hypergeometric-computing-exx-1"><i class="fa fa-check"></i><b>6.8.5</b> Negative Binomial and Hypergeometric: Computing <span class="math inline">\(E(X(X-1))\)</span></a></li>
<li class="chapter" data-level="6.8.6" data-path="moments.html"><a href="moments.html#poisson-exponential-taylor-series"><i class="fa fa-check"></i><b>6.8.6</b> Poisson: Exponential Taylor Series</a></li>
<li class="chapter" data-level="6.8.7" data-path="moments.html"><a href="moments.html#exponential-integration-by-parts"><i class="fa fa-check"></i><b>6.8.7</b> Exponential: Integration By Parts</a></li>
<li class="chapter" data-level="6.8.8" data-path="moments.html"><a href="moments.html#gamma-and-beta-kernel-technique-integration-version"><i class="fa fa-check"></i><b>6.8.8</b> Gamma and Beta: Kernel Technique, Integration Version</a></li>
<li class="chapter" data-level="6.8.9" data-path="moments.html"><a href="moments.html#normal-location-scale-trick-and-polar-integration"><i class="fa fa-check"></i><b>6.8.9</b> Normal: Location-Scale Trick and Polar Integration</a></li>
</ul></li>
<li class="chapter" data-level="6.9" data-path="moments.html"><a href="moments.html#other-moments-for-reference"><i class="fa fa-check"></i><b>6.9</b> Other Moments (for reference)</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="statistics.html"><a href="statistics.html"><i class="fa fa-check"></i><b>7</b> Statistics</a>
<ul>
<li class="chapter" data-level="7.1" data-path="statistics.html"><a href="statistics.html#sufficient-stats"><i class="fa fa-check"></i><b>7.1</b> Sufficient Statistics</a>
<ul>
<li class="chapter" data-level="7.1.1" data-path="statistics.html"><a href="statistics.html#techniques-for-finding-sufficient-statistics"><i class="fa fa-check"></i><b>7.1.1</b> Techniques for Finding Sufficient Statistics</a></li>
<li class="chapter" data-level="7.1.2" data-path="statistics.html"><a href="statistics.html#exp-fam-ss"><i class="fa fa-check"></i><b>7.1.2</b> Exponential Family Sufficient Statistics</a></li>
<li class="chapter" data-level="7.1.3" data-path="statistics.html"><a href="statistics.html#a-note-on-distributions-of-sufficient-statistics"><i class="fa fa-check"></i><b>7.1.3</b> A Note on Distributions of Sufficient Statistics</a></li>
<li class="chapter" data-level="7.1.4" data-path="statistics.html"><a href="statistics.html#moments-of-the-sufficient-statistic"><i class="fa fa-check"></i><b>7.1.4</b> Moments of the Sufficient Statistic</a></li>
<li class="chapter" data-level="7.1.5" data-path="statistics.html"><a href="statistics.html#table-ss"><i class="fa fa-check"></i><b>7.1.5</b> Table of Sufficient Statistics</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="statistics.html"><a href="statistics.html#minimal-sufficiency"><i class="fa fa-check"></i><b>7.2</b> Minimal Sufficiency</a></li>
<li class="chapter" data-level="7.3" data-path="statistics.html"><a href="statistics.html#ancillary-stats"><i class="fa fa-check"></i><b>7.3</b> Ancillary Statistics</a>
<ul>
<li class="chapter" data-level="7.3.1" data-path="statistics.html"><a href="statistics.html#why-are-we-interested-in-ancillary-statistics"><i class="fa fa-check"></i><b>7.3.1</b> Why are we interested in ancillary statistics?</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="statistics.html"><a href="statistics.html#complete-stats"><i class="fa fa-check"></i><b>7.4</b> Complete Statistics</a>
<ul>
<li class="chapter" data-level="7.4.1" data-path="statistics.html"><a href="statistics.html#techniques-for-finding-css"><i class="fa fa-check"></i><b>7.4.1</b> Techniques for Finding CSS</a></li>
<li class="chapter" data-level="7.4.2" data-path="statistics.html"><a href="statistics.html#basus-theorem"><i class="fa fa-check"></i><b>7.4.2</b> Basu’s Theorem</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="point-estimators-finite-samples.html"><a href="point-estimators-finite-samples.html"><i class="fa fa-check"></i><b>8</b> Point Estimators: Finite Samples</a>
<ul>
<li class="chapter" data-level="8.1" data-path="point-estimators-finite-samples.html"><a href="point-estimators-finite-samples.html#identifiability"><i class="fa fa-check"></i><b>8.1</b> Identifiability</a></li>
<li class="chapter" data-level="8.2" data-path="point-estimators-finite-samples.html"><a href="point-estimators-finite-samples.html#finding-estimators"><i class="fa fa-check"></i><b>8.2</b> Finding estimators</a>
<ul>
<li class="chapter" data-level="8.2.1" data-path="point-estimators-finite-samples.html"><a href="point-estimators-finite-samples.html#method-of-moments"><i class="fa fa-check"></i><b>8.2.1</b> Method of Moments</a></li>
<li class="chapter" data-level="8.2.2" data-path="point-estimators-finite-samples.html"><a href="point-estimators-finite-samples.html#maximum-likelihood-estimation"><i class="fa fa-check"></i><b>8.2.2</b> Maximum Likelihood Estimation</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="point-estimators-finite-samples.html"><a href="point-estimators-finite-samples.html#properties-of-estimators"><i class="fa fa-check"></i><b>8.3</b> Properties of Estimators</a></li>
<li class="chapter" data-level="8.4" data-path="point-estimators-finite-samples.html"><a href="point-estimators-finite-samples.html#uniform-minimum-variance-unbiased-estimators-umvues"><i class="fa fa-check"></i><b>8.4</b> Uniform Minimum Variance Unbiased Estimators (UMVUEs)</a>
<ul>
<li class="chapter" data-level="8.4.1" data-path="point-estimators-finite-samples.html"><a href="point-estimators-finite-samples.html#finding-umvues-i-cramér-rao-bound"><i class="fa fa-check"></i><b>8.4.1</b> Finding UMVUEs I: Cramér-Rao Bound</a></li>
<li class="chapter" data-level="8.4.2" data-path="point-estimators-finite-samples.html"><a href="point-estimators-finite-samples.html#rao-blackwell-and-lehmann-scheffé-theorem"><i class="fa fa-check"></i><b>8.4.2</b> Rao-Blackwell and Lehmann-Scheffé Theorem</a></li>
<li class="chapter" data-level="8.4.3" data-path="point-estimators-finite-samples.html"><a href="point-estimators-finite-samples.html#finding-umvues-ii-using-the-lehmann-scheffé-theorem"><i class="fa fa-check"></i><b>8.4.3</b> Finding UMVUEs II: Using the Lehmann-Scheffé Theorem</a></li>
<li class="chapter" data-level="8.4.4" data-path="point-estimators-finite-samples.html"><a href="point-estimators-finite-samples.html#finding-umvues-iii-lehmann-scheffé-corollary"><i class="fa fa-check"></i><b>8.4.4</b> Finding UMVUEs III: Lehmann-Scheffé Corollary</a></li>
<li class="chapter" data-level="8.4.5" data-path="point-estimators-finite-samples.html"><a href="point-estimators-finite-samples.html#proving-an-umvue-does-not-exist"><i class="fa fa-check"></i><b>8.4.5</b> Proving an UMVUE Does Not Exist</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="point-estimators-finite-samples.html"><a href="point-estimators-finite-samples.html#inferential-properties-of-exponential-families-distributions"><i class="fa fa-check"></i><b>8.5</b> Inferential Properties of Exponential Families Distributions</a>
<ul>
<li class="chapter" data-level="8.5.1" data-path="point-estimators-finite-samples.html"><a href="point-estimators-finite-samples.html#bernoulli-1"><i class="fa fa-check"></i><b>8.5.1</b> Bernoulli</a></li>
<li class="chapter" data-level="8.5.2" data-path="point-estimators-finite-samples.html"><a href="point-estimators-finite-samples.html#binomial-1"><i class="fa fa-check"></i><b>8.5.2</b> Binomial</a></li>
<li class="chapter" data-level="8.5.3" data-path="point-estimators-finite-samples.html"><a href="point-estimators-finite-samples.html#geometric"><i class="fa fa-check"></i><b>8.5.3</b> Geometric</a></li>
<li class="chapter" data-level="8.5.4" data-path="point-estimators-finite-samples.html"><a href="point-estimators-finite-samples.html#negative-binomial-1"><i class="fa fa-check"></i><b>8.5.4</b> Negative Binomial</a></li>
<li class="chapter" data-level="8.5.5" data-path="point-estimators-finite-samples.html"><a href="point-estimators-finite-samples.html#poisson-1"><i class="fa fa-check"></i><b>8.5.5</b> Poisson</a></li>
<li class="chapter" data-level="8.5.6" data-path="point-estimators-finite-samples.html"><a href="point-estimators-finite-samples.html#normal-1"><i class="fa fa-check"></i><b>8.5.6</b> Normal</a></li>
<li class="chapter" data-level="8.5.7" data-path="point-estimators-finite-samples.html"><a href="point-estimators-finite-samples.html#exponential-1"><i class="fa fa-check"></i><b>8.5.7</b> Exponential</a></li>
<li class="chapter" data-level="8.5.8" data-path="point-estimators-finite-samples.html"><a href="point-estimators-finite-samples.html#gamma-1"><i class="fa fa-check"></i><b>8.5.8</b> Gamma</a></li>
<li class="chapter" data-level="8.5.9" data-path="point-estimators-finite-samples.html"><a href="point-estimators-finite-samples.html#pareto-1"><i class="fa fa-check"></i><b>8.5.9</b> Pareto</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="point-estimators-asymptotics.html"><a href="point-estimators-asymptotics.html"><i class="fa fa-check"></i><b>9</b> Point Estimators: Asymptotics</a>
<ul>
<li class="chapter" data-level="9.1" data-path="point-estimators-asymptotics.html"><a href="point-estimators-asymptotics.html#consistency"><i class="fa fa-check"></i><b>9.1</b> Consistency</a>
<ul>
<li class="chapter" data-level="9.1.1" data-path="point-estimators-asymptotics.html"><a href="point-estimators-asymptotics.html#technique-weak-law-of-large-numbers"><i class="fa fa-check"></i><b>9.1.1</b> Technique: Weak Law of Large Numbers</a></li>
<li class="chapter" data-level="9.1.2" data-path="point-estimators-asymptotics.html"><a href="point-estimators-asymptotics.html#technique-direct-proof-via-convergence-in-probability"><i class="fa fa-check"></i><b>9.1.2</b> Technique: Direct Proof via Convergence in Probability</a></li>
<li class="chapter" data-level="9.1.3" data-path="point-estimators-asymptotics.html"><a href="point-estimators-asymptotics.html#technique-continuous-mapping-theorem."><i class="fa fa-check"></i><b>9.1.3</b> Technique: Continuous Mapping Theorem.</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="point-estimators-asymptotics.html"><a href="point-estimators-asymptotics.html#asymptotic-efficiency"><i class="fa fa-check"></i><b>9.2</b> Asymptotic Efficiency</a>
<ul>
<li class="chapter" data-level="9.2.1" data-path="point-estimators-asymptotics.html"><a href="point-estimators-asymptotics.html#central-limit-theorems"><i class="fa fa-check"></i><b>9.2.1</b> Central Limit Theorems</a></li>
<li class="chapter" data-level="9.2.2" data-path="point-estimators-asymptotics.html"><a href="point-estimators-asymptotics.html#the-delta-method"><i class="fa fa-check"></i><b>9.2.2</b> The Delta Method</a></li>
<li class="chapter" data-level="9.2.3" data-path="point-estimators-asymptotics.html"><a href="point-estimators-asymptotics.html#cramer-wold-device"><i class="fa fa-check"></i><b>9.2.3</b> Cramer-Wold Device</a></li>
<li class="chapter" data-level="9.2.4" data-path="point-estimators-asymptotics.html"><a href="point-estimators-asymptotics.html#asymptotic-distribution-in-practice"><i class="fa fa-check"></i><b>9.2.4</b> Asymptotic Distribution in Practice</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="point-estimators-asymptotics.html"><a href="point-estimators-asymptotics.html#asymptotic-properties-of-mles"><i class="fa fa-check"></i><b>9.3</b> Asymptotic Properties of MLEs</a></li>
<li class="chapter" data-level="9.4" data-path="point-estimators-asymptotics.html"><a href="point-estimators-asymptotics.html#variance-stabilizing-transformations"><i class="fa fa-check"></i><b>9.4</b> Variance Stabilizing Transformations</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="hypothesis-tests-finite-samples.html"><a href="hypothesis-tests-finite-samples.html"><i class="fa fa-check"></i><b>10</b> Hypothesis Tests: Finite Samples</a>
<ul>
<li class="chapter" data-level="10.1" data-path="hypothesis-tests-finite-samples.html"><a href="hypothesis-tests-finite-samples.html#constructing-a-test"><i class="fa fa-check"></i><b>10.1</b> Constructing a Test</a></li>
<li class="chapter" data-level="10.2" data-path="hypothesis-tests-finite-samples.html"><a href="hypothesis-tests-finite-samples.html#lrt"><i class="fa fa-check"></i><b>10.2</b> Likelihood Ratio Tests</a></li>
<li class="chapter" data-level="10.3" data-path="hypothesis-tests-finite-samples.html"><a href="hypothesis-tests-finite-samples.html#power"><i class="fa fa-check"></i><b>10.3</b> Power</a></li>
<li class="chapter" data-level="10.4" data-path="hypothesis-tests-finite-samples.html"><a href="hypothesis-tests-finite-samples.html#how-to-find-optimal-tests"><i class="fa fa-check"></i><b>10.4</b> How to Find Optimal Tests</a>
<ul>
<li class="chapter" data-level="10.4.1" data-path="hypothesis-tests-finite-samples.html"><a href="hypothesis-tests-finite-samples.html#properties-1"><i class="fa fa-check"></i><b>10.4.1</b> Properties</a></li>
<li class="chapter" data-level="10.4.2" data-path="hypothesis-tests-finite-samples.html"><a href="hypothesis-tests-finite-samples.html#optimality-for-simple-hypotheses-the-neyman-pearson-lemma"><i class="fa fa-check"></i><b>10.4.2</b> Optimality for Simple Hypotheses: The Neyman-Pearson Lemma</a></li>
<li class="chapter" data-level="10.4.3" data-path="hypothesis-tests-finite-samples.html"><a href="hypothesis-tests-finite-samples.html#optimality-for-one-sided-hypotheses-the-karlin-rubin-theorem"><i class="fa fa-check"></i><b>10.4.3</b> Optimality for One-Sided Hypotheses: The Karlin-Rubin Theorem</a></li>
<li class="chapter" data-level="10.4.4" data-path="hypothesis-tests-finite-samples.html"><a href="hypothesis-tests-finite-samples.html#optimality-for-two-sided-hypotheses."><i class="fa fa-check"></i><b>10.4.4</b> Optimality for Two-Sided Hypotheses.</a></li>
</ul></li>
<li class="chapter" data-level="10.5" data-path="hypothesis-tests-finite-samples.html"><a href="hypothesis-tests-finite-samples.html#nuisance-parameters"><i class="fa fa-check"></i><b>10.5</b> Nuisance Parameters</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="hypothesis-tests-asymptotics.html"><a href="hypothesis-tests-asymptotics.html"><i class="fa fa-check"></i><b>11</b> Hypothesis Tests: Asymptotics</a>
<ul>
<li class="chapter" data-level="11.1" data-path="hypothesis-tests-asymptotics.html"><a href="hypothesis-tests-asymptotics.html#wald-test"><i class="fa fa-check"></i><b>11.1</b> Wald Test</a></li>
<li class="chapter" data-level="11.2" data-path="hypothesis-tests-asymptotics.html"><a href="hypothesis-tests-asymptotics.html#score-test"><i class="fa fa-check"></i><b>11.2</b> Score Test</a></li>
<li class="chapter" data-level="11.3" data-path="hypothesis-tests-asymptotics.html"><a href="hypothesis-tests-asymptotics.html#likelihood-ratio-test"><i class="fa fa-check"></i><b>11.3</b> Likelihood Ratio Test</a></li>
<li class="chapter" data-level="11.4" data-path="hypothesis-tests-asymptotics.html"><a href="hypothesis-tests-asymptotics.html#composite-null-hypotheses"><i class="fa fa-check"></i><b>11.4</b> Composite Null Hypotheses</a>
<ul>
<li class="chapter" data-level="11.4.1" data-path="hypothesis-tests-asymptotics.html"><a href="hypothesis-tests-asymptotics.html#multiple-parameters"><i class="fa fa-check"></i><b>11.4.1</b> Multiple Parameters</a></li>
<li class="chapter" data-level="11.4.2" data-path="hypothesis-tests-asymptotics.html"><a href="hypothesis-tests-asymptotics.html#nuisance-parameters-1"><i class="fa fa-check"></i><b>11.4.2</b> Nuisance Parameters</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="12" data-path="confidence-intervals.html"><a href="confidence-intervals.html"><i class="fa fa-check"></i><b>12</b> Confidence Intervals</a>
<ul>
<li class="chapter" data-level="12.1" data-path="confidence-intervals.html"><a href="confidence-intervals.html#definition-1"><i class="fa fa-check"></i><b>12.1</b> Definition</a></li>
<li class="chapter" data-level="12.2" data-path="confidence-intervals.html"><a href="confidence-intervals.html#test-inversion"><i class="fa fa-check"></i><b>12.2</b> When You’ve Constructed a Hypothesis Test…</a></li>
<li class="chapter" data-level="12.3" data-path="confidence-intervals.html"><a href="confidence-intervals.html#when-youve-found-a-pivot-including-asymptotic-normality"><i class="fa fa-check"></i><b>12.3</b> When You’ve Found a Pivot (Including Asymptotic Normality)…</a></li>
<li class="chapter" data-level="12.4" data-path="confidence-intervals.html"><a href="confidence-intervals.html#when-all-you-have-is-a-distribution"><i class="fa fa-check"></i><b>12.4</b> When All You Have Is a Distribution…</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="random-processes.html"><a href="random-processes.html"><i class="fa fa-check"></i><b>13</b> Random Processes</a>
<ul>
<li class="chapter" data-level="13.1" data-path="random-processes.html"><a href="random-processes.html#branching-process"><i class="fa fa-check"></i><b>13.1</b> Branching Processes</a>
<ul>
<li class="chapter" data-level="13.1.1" data-path="random-processes.html"><a href="random-processes.html#random-variables"><i class="fa fa-check"></i><b>13.1.1</b> Random Variables</a></li>
<li class="chapter" data-level="13.1.2" data-path="random-processes.html"><a href="random-processes.html#probability-generating-function"><i class="fa fa-check"></i><b>13.1.2</b> Probability Generating Function</a></li>
<li class="chapter" data-level="13.1.3" data-path="random-processes.html"><a href="random-processes.html#finding-the-pgf-of-a-branching-process"><i class="fa fa-check"></i><b>13.1.3</b> Finding the PGF of a Branching Process</a></li>
<li class="chapter" data-level="13.1.4" data-path="random-processes.html"><a href="random-processes.html#finding-the-probability-of-extinction-criticality-theorem"><i class="fa fa-check"></i><b>13.1.4</b> Finding the Probability of Extinction: Criticality Theorem</a></li>
<li class="chapter" data-level="13.1.5" data-path="random-processes.html"><a href="random-processes.html#example"><i class="fa fa-check"></i><b>13.1.5</b> Example</a></li>
</ul></li>
<li class="chapter" data-level="13.2" data-path="random-processes.html"><a href="random-processes.html#poisson-processes"><i class="fa fa-check"></i><b>13.2</b> Poisson Processes</a>
<ul>
<li class="chapter" data-level="13.2.1" data-path="random-processes.html"><a href="random-processes.html#memorylessness-of-the-exponential"><i class="fa fa-check"></i><b>13.2.1</b> Memorylessness of the Exponential</a></li>
<li class="chapter" data-level="13.2.2" data-path="random-processes.html"><a href="random-processes.html#count-time-duality"><i class="fa fa-check"></i><b>13.2.2</b> Count-Time Duality</a></li>
<li class="chapter" data-level="13.2.3" data-path="random-processes.html"><a href="random-processes.html#poisson-distribution"><i class="fa fa-check"></i><b>13.2.3</b> Poisson Distribution</a></li>
<li class="chapter" data-level="13.2.4" data-path="random-processes.html"><a href="random-processes.html#exponential-distribution"><i class="fa fa-check"></i><b>13.2.4</b> Exponential Distribution</a></li>
<li class="chapter" data-level="13.2.5" data-path="random-processes.html"><a href="random-processes.html#example-1"><i class="fa fa-check"></i><b>13.2.5</b> Example</a></li>
<li class="chapter" data-level="13.2.6" data-path="random-processes.html"><a href="random-processes.html#merging-and-splitting"><i class="fa fa-check"></i><b>13.2.6</b> Merging and Splitting</a></li>
<li class="chapter" data-level="13.2.7" data-path="random-processes.html"><a href="random-processes.html#thinning"><i class="fa fa-check"></i><b>13.2.7</b> Thinning</a></li>
<li class="chapter" data-level="13.2.8" data-path="random-processes.html"><a href="random-processes.html#restarting"><i class="fa fa-check"></i><b>13.2.8</b> Restarting</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Solving Statistical Problems</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="statistics" class="section level1 hasAnchor" number="7">
<h1><span class="header-section-number">Chapter 7</span> Statistics<a href="statistics.html#statistics" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>One of the most important tasks we will be performing as statisticians is inference. Inference is the area of statistics which uses data from a random sample to estimate a population parameter.</p>
<div class="definition">
<p><span id="def:random-sample" class="definition"><strong>Definition 7.1  (Random Sample) </strong></span><span class="math inline">\(X = (X_1, X_2, ... X_n)\)</span> constitutes a <strong>random sample</strong> of size <span class="math inline">\(n\)</span> provided each <span class="math inline">\(X_i\)</span> is mutually independent.</p>
<p>The term <strong>iid</strong> stands for “independently and identically distributed.” A random sample is iid if all <span class="math inline">\(X_i\)</span> are independent <em>and</em> follows the same distribution. In this case,</p>
<p><span class="math display">\[f_X(x_1, ..., x_n) = f(x_1)\cdot f(x_2)\cdot,..., \cdot f(x_n) = \prod_{i=1}^nf(x_i)\]</span></p>
</div>
<p>To perform inference, it is usually necessary to <em>reduce</em> or <em>summarize</em> the recollected data into some measurement that we will refer to as <strong>statistic</strong>.</p>
<div class="definition">
<p><span id="def:unlabeled-div-59" class="definition"><strong>Definition 7.2  (Statistic) </strong></span>Let <span class="math inline">\(X \sim f(x|\theta)\)</span>, where both <span class="math inline">\(X, \theta\)</span> can be vectors. A statistic is a function <span class="math inline">\(T=T(\textbf{X})\)</span> of the sample <span class="math inline">\(\textbf{X}\)</span> from <span class="math inline">\(X\)</span>.</p>
</div>
<p>For example, the <span class="math inline">\(T= T(\textbf{X}) = \frac{1}{n} \sum_{i=1}^{n} X_i\)</span> reduces the sample <span class="math inline">\(\textbf{X}\)</span> to a single measurement.</p>
<p>In general, we would like to choose statistics that satisfy principles that will make inference about the population parameter <em>easier</em>. One of these principles is <strong>sufficiency</strong>. The idea behind <strong>sufficiency</strong> is to retain information about the population parameter, say <span class="math inline">\(\theta\)</span>, while reducing the data.</p>
<div id="sufficient-stats" class="section level2 hasAnchor" number="7.1">
<h2><span class="header-section-number">7.1</span> Sufficient Statistics<a href="statistics.html#sufficient-stats" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><strong>Sufficiency Principle</strong>. If <span class="math inline">\(T\)</span> is sufficient, then any information about the parameter <span class="math inline">\(\theta\)</span> should depend on <span class="math inline">\(X\)</span> only through <span class="math inline">\(T\)</span>.</p>
<div class="definition">
<p><span id="def:unlabeled-div-60" class="definition"><strong>Definition 7.3  (Sufficient Statistic) </strong></span>Let <span class="math inline">\(X \sim f(x|\theta)\)</span>. We say <span class="math inline">\(T=T(X)\)</span> is a sufficient statistic (SS) for <span class="math inline">\(\theta\)</span> if <span class="math inline">\(\mathcal{L}(\theta|X)\)</span> is independent of <span class="math inline">\(\theta\)</span>, i.e. <span class="math inline">\(f(x|T;\theta) = g(x)\)</span>.</p>
</div>
<div id="techniques-for-finding-sufficient-statistics" class="section level3 hasAnchor" number="7.1.1">
<h3><span class="header-section-number">7.1.1</span> Techniques for Finding Sufficient Statistics<a href="statistics.html#techniques-for-finding-sufficient-statistics" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>There are several ways to find a sufficient statistic - this section will outline several methods.</p>
<p>Firstly, if you are given a statistic and you wish to check whether it is sufficient, you can do so directly.</p>
<div class="example">
<p><span id="exm:unlabeled-div-61" class="example"><strong>Example 7.1  (Checking Whether a Sample Mean is Sufficient) </strong></span>Let <span class="math inline">\(X = (X_1, ..., X_n), X_i\)</span> iid N(<span class="math inline">\(\theta\)</span>, 1) and <span class="math inline">\(T = \bar{X}\)</span>. Recall that <span class="math inline">\(\bar{X} \sim N(\theta, 1/n)\)</span>.
<span class="math display">\[
\begin{aligned}
f(x|T, \theta) = n^{-1/2}(2\pi)^{-(\frac{n-1}{2})} exp\{-\sum_{i=1}^{n}(x_i - \bar{x})^2\}
\end{aligned}
\]</span>
Observe that the conditional density is independent of <span class="math inline">\(\theta\)</span>. By definition, <span class="math inline">\(T = \bar{X}\)</span> is a sufficient statistic for <span class="math inline">\(\theta\)</span>.</p>
</div>
<p>As you can see, given a particular statistic, you can check whether it is sufficient directly. However, this requires knowing its distribution. If you are given the joint distribution of random variables (or can compute it) involved in the statistic, the definition of sufficiency can be checked.</p>
<div class="example">
<p><span id="exm:unlabeled-div-62" class="example"><strong>Example 7.2  (Finding a Sufficient Statistic Directly) </strong></span>Let <span class="math inline">\(X_1, X_2\)</span> iid Poisson(<span class="math inline">\(\lambda\)</span>). The joint distribution of <span class="math inline">\(X_1, X_2\)</span> is <span class="math inline">\(P(X_1=x_1, X_2 = x_2) = \frac{\lambda^{x_1+x_2}exp(-2\lambda)}{x_1! x_2!}\)</span>. Let <span class="math inline">\(T(X_1, X_2) = X_1+X_2\)</span>. Observe that <span class="math inline">\(X_1+X_2 \sim\)</span> Poisson(2<span class="math inline">\(\lambda\)</span>). Thus,</p>
<p><span class="math display">\[
\begin{aligned}
P(X_1=x_1, X_2 = x_2|X_1+X_2=t) &amp;= \frac{P(X_1=x_1, X_2 = t - x_1)}{P(X_1 + X_2 = t)} \\
&amp;=\frac{exp(-\lambda)\lambda^{x_1} exp(-\lambda)\lambda^{t -x_1}t!}{x_1!(t-x_1)! exp(-2\lambda)(2\lambda)^t}\\
&amp;= { t \choose x_1} \bigg(\frac{1}{2}\bigg)^t
\end{aligned}
\]</span>
which is independent of <span class="math inline">\(\lambda\)</span> and <span class="math inline">\(X_1+X_2\)</span> is sufficient for <span class="math inline">\(\lambda\)</span>.</p>
</div>
<p>As can be seen from the example, we need a candidate statistic to prove sufficiency using the definition. Furthermore, checking sufficiency of a statistics is difficult because we need to compute the conditional distribution.</p>
<p>Even if you are not given a statistic, a sufficient statistic can still be identified using the <em>factorization theorem</em>:</p>
<div class="theorem">
<p><span id="thm:factorization-theorem" class="theorem"><strong>Theorem 7.1  (Factorization Theorem) </strong></span><span class="math inline">\(T(X)\)</span> is sufficient for <span class="math inline">\(\theta \Longleftrightarrow \exists g(t|\theta)\)</span> and <span class="math inline">\(h(x)\)</span>, such that</p>
<p><span class="math display">\[f(x|\theta)=g(t|\theta)h(x)\]</span>
<span class="math inline">\(\forall x, \theta\)</span>.</p>
</div>
<p>Note that the factorization theorem tells us that if we can manipulate <span class="math inline">\(f(x|\theta)\)</span> as above, we have a sufficient statistic. Here is an example.</p>
<p>::: {.example name=“Using the Factorization Theorem}
Let <span class="math inline">\(\textbf{X}= (X_1, ..., X_n), X_i\)</span> iid <span class="math inline">\(Poisson(\lambda)\)</span>.
<span class="math display">\[
\begin{aligned}
P(X_1=x_1,..., X_n = x_n) &amp;= \frac{exp(-n\lambda)\lambda^{\sum x_i}}{\prod x_i!}\\
&amp;= h(x)g\Big(\sum x_i|\lambda\Big)
\end{aligned}
\]</span>
<span class="math inline">\(\Rightarrow T(X) = \sum X_i\)</span> sufficient for <span class="math inline">\(\lambda\)</span>.
:::</p>
<p>Some distributions involve indicator functions. If an indicator function involves both <span class="math inline">\(X_i\)</span> and <span class="math inline">\(\theta\)</span>, such as <span class="math inline">\(I(X_i &lt; \theta)\)</span>, chances are that an order statistic like <span class="math inline">\(X_{(1)}\)</span> (the minimum) or <span class="math inline">\(X_{(n)}\)</span> (the maximum) will be involved in the sufficient statistic.</p>
<p>Why is this? Take <span class="math inline">\(I(X_i &lt; \theta)\)</span> as an example. For iid <span class="math inline">\(X_i\)</span>, <span class="math inline">\(\prod_{i=1}^nI(X_i &lt; \theta) = I(X_{(n)} &lt; \theta)\)</span>. For this product to be nonzero, every <span class="math inline">\(X_i\)</span> must be less than <span class="math inline">\(\theta\)</span>, which is equivalent to saying that the largest value (<span class="math inline">\(X_{(n)}\)</span>) is less than <span class="math inline">\(\theta\)</span>.</p>
<p>This trick is useful for finding sufficient statistics. Here’s an example involving the Uniform distribution.</p>
<p>::: {.example name=“Indicator Functions in the Factorization Theorem}
Let <span class="math inline">\(\textbf{X}= (X_1, ..., X_n), X_i\)</span> iid <span class="math inline">\(Uniform(0,\theta)\)</span>. Then,</p>
<p><span class="math display">\[
\begin{aligned}
P_{\theta}(x_1, ..., x_n) &amp;= \frac{1}{\theta^n} \prod_{i=1}^{n}I(0&lt;x_i&lt;\theta)\\
&amp; = \frac{1}{\theta^n} I(x_{(1)}&gt;0) I(x_{(n)}&lt;\theta)
\end{aligned}
\]</span>
<span class="math inline">\(\Rightarrow T(X) = X_{(n)}\)</span> sufficient for <span class="math inline">\(\theta\)</span>.
:::</p>
<div id="important-facts-about-ss" class="section level4 hasAnchor" number="7.1.1.1">
<h4><span class="header-section-number">7.1.1.1</span> Important facts about SS<a href="statistics.html#important-facts-about-ss" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li>Sufficient statistics may or may not reduce the data.
<ul>
<li>Original data are always sufficient.</li>
<li>In iid sample, the order statistics are sufficient.</li>
</ul></li>
<li>Sufficient statistics are never unique.
<ul>
<li><p>Suppose <span class="math inline">\(X \sim N(0,\sigma^2)\)</span>. Then by the factorization theorem <span class="math inline">\(T(X)=X^2, |X|, X^4, exp(X^2)\)</span> are all sufficient.</p></li>
<li><p>Any 1-1 function <span class="math inline">\(g\)</span> of a sufficient statistic is also sufficient.</p>
<p><strong>Proof</strong>. Let <span class="math inline">\(T^*(X)= g(T(X))\)</span>. By assumption <span class="math inline">\(g^{-1}\)</span> exists since g is 1-1.</p></li>
</ul>
<span class="math display">\[
\begin{aligned}
f(x|\theta) &amp;= g(T(X)|\theta)\\
    &amp;=g(r^{-1}(T^*(x))|\theta)h(x)\\
\end{aligned}
\]</span>
By the factorization theorem, <span class="math inline">\(T^*(X)\)</span> is sufficient for <span class="math inline">\(\theta\)</span>.</li>
</ul>
</div>
</div>
<div id="exp-fam-ss" class="section level3 hasAnchor" number="7.1.2">
<h3><span class="header-section-number">7.1.2</span> Exponential Family Sufficient Statistics<a href="statistics.html#exp-fam-ss" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>If <span class="math inline">\(X_i\)</span> is an exponential family, we automatically know its sufficient statistic. Consider a sample <span class="math inline">\(X=(X_1,...,X_n)\)</span> from <span class="math inline">\(f_X(x|\theta)= h(x)c(\theta)\exp\Big(\sum_{i=1}^k w_i(\theta)t_i(x)\Big)\)</span>. Then <span class="math inline">\(T(X) = \Big(\sum_{i=1}^n t_1(x), ..., \sum_{i=1}^n t_k(x)\Big)\)</span> is a <a href="statistics.html#statistics">sufficient statistic</a>. Note that this result follows directly from the <a href="#id_%20sufficient-stats">factorization theorem</a>.</p>
<p><strong>Problem-Solving Tip:</strong> Whenever possible, use exponential families to quickly find sufficient statistics! It is often easier to prove that a distribution is an exponential family than to otherwise find a sufficient statistic. In addition, showing a distribution is an exponential provides <a href="known-distributions.html#exponential-family">many other useful results</a>.</p>
</div>
<div id="a-note-on-distributions-of-sufficient-statistics" class="section level3 hasAnchor" number="7.1.3">
<h3><span class="header-section-number">7.1.3</span> A Note on Distributions of Sufficient Statistics<a href="statistics.html#a-note-on-distributions-of-sufficient-statistics" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Recall a convenient property of <a href="known-distributions.html#exponential-family">exponential families</a>: that their maximum likelihood estimate is a function of their sufficient statistic. Because of this, in order to prove <a href="point-estimators-finite-samples.html#point-estimators-finite-samples">Finite Sample Properties</a> of an estimator or construct <a href="hypothesis-tests-finite-samples.html#hypothesis-tests-finite-samples">Hypothesis Tests</a>, it is often useful to understand their distributions. This is why the distributions of each of the sufficient statistics are included in the fourth column of the table below. These distributions are mostly derived from additive, location-scale, and other properties in <a href="known-distributions.html#known-distributions">Chapter 4 - Known Distributions</a></p>
</div>
<div id="moments-of-the-sufficient-statistic" class="section level3 hasAnchor" number="7.1.4">
<h3><span class="header-section-number">7.1.4</span> Moments of the Sufficient Statistic<a href="statistics.html#moments-of-the-sufficient-statistic" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>As stated by <span class="citation">Casella and Berger (<a href="#ref-Casella1990">1990</a>)</span>, if <span class="math inline">\(X\)</span> is an exponential family, the moments of its exponential family can be easily computed using certain properties.</p>
<div class="theorem">
<p><span id="thm:unlabeled-div-63" class="theorem"><strong>Theorem 7.2  (I Don't Know What To Call This) </strong></span>If <span class="math inline">\(X\)</span> is an <a href="known-distributions.html#exponential-family">exponential family</a>, then</p>
<p><span class="math display">\[E\Big(\sum_{i=1}^k \frac{\partial w_i(\theta)}{\partial \theta_j}t_i(X)\Big) = -\frac{\partial}{\partial\theta_j} \log c(\theta)\]</span>
and</p>
<p><span class="math display">\[Var\Big(\sum_{i=1}^k \frac{\partial w_i(\theta)}{\partial \theta_j}t_i(X)\Big) = -\frac{\partial}{\partial\theta_j} \log c(\theta) - E\Big(\sum_{i=1}^k \frac{\partial^2w_i(\theta)}{\partial \theta_j^2}t_i(X)\Big)\]</span></p>
</div>
<p>If <span class="math inline">\(X\)</span> is a <a href="known-distributions.html#natural-exponential-family">natural exponential family</a>, then these identities simplify even further.</p>
<div class="theorem">
<p><span id="thm:unlabeled-div-64" class="theorem"><strong>Theorem 7.3  (I Don't Know What To Call This 2) </strong></span>If <span class="math inline">\(X\)</span> is a natural exponential family, then$</p>
<p><span class="math display">\[E(t_j(X)) = -\frac{\partial}{\partial\eta_j}\log(c^*(\eta))\]</span>
and</p>
<p><span class="math display">\[Var(t_j(X)) = -\frac{\partial^2}{\partial\eta_j^2}\log(c^*(\eta))\]</span></p>
</div>
<p>Using these theorems, we can calculate the moments of the sufficient statistics of exponential families directly. This is because if <span class="math inline">\(T(X)\)</span> is a sufficient statistic, then <span class="math inline">\(T(X) = \Big(\sum_{i=1}^nt_1(X), ..., \sum_{i=1}^nt_k(X)\Big)\)</span>. Suppose, for simplicity, that <span class="math inline">\(T(X)\)</span> is one-dimensional and <span class="math inline">\(X_i\)</span> are iid. Then, if <span class="math inline">\(X_i\)</span> is a natural exponential family,</p>
<p><span class="math display">\[E(T(X)) = E\Big(\sum_{i=1}^nt(X_i)\Big) = nE(t(X_i)) = n\cdot-\frac{\partial}{\partial \eta}\log(c^*(\eta))\]</span>
In fact, letting <span class="math inline">\(A(\eta) = -\log(c^*(\eta))\)</span>, we can obtain all of the moments of <span class="math inline">\(T(X)\)</span> by simply differentiating <span class="math inline">\(A(\eta)\)</span></p>
</div>
<div id="table-ss" class="section level3 hasAnchor" number="7.1.5">
<h3><span class="header-section-number">7.1.5</span> Table of Sufficient Statistics<a href="statistics.html#table-ss" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Translated from <a href="https://en.wikipedia.org/wiki/Exponential_family" class="uri">https://en.wikipedia.org/wiki/Exponential_family</a></p>
<table>
<colgroup>
<col width="25%" />
<col width="25%" />
<col width="25%" />
<col width="25%" />
</colgroup>
<thead>
<tr class="header">
<th align="center">Distribution</th>
<th align="center">Parameter</th>
<th align="center">Sufficient Statistic</th>
<th align="center">S.S. Distribution</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">Bernoulli</td>
<td align="center">p</td>
<td align="center"><span class="math inline">\(\sum_{i=1}^n x_i\)</span></td>
<td align="center"><span class="math inline">\(Binomial(n, p)\)</span></td>
</tr>
<tr class="even">
<td align="center">Binomial</td>
<td align="center">p</td>
<td align="center"><span class="math inline">\(\sum_{i=1}^n x_i\)</span></td>
<td align="center"><span class="math inline">\(Binomial(nm, p)\)</span></td>
</tr>
<tr class="odd">
<td align="center">Poisson</td>
<td align="center"><span class="math inline">\(\lambda\)</span></td>
<td align="center"><span class="math inline">\(\sum_{i=1}^n x_i\)</span></td>
<td align="center"><span class="math inline">\(Poisson(n\lambda)\)</span></td>
</tr>
<tr class="even">
<td align="center">Negative Binomial</td>
<td align="center">p</td>
<td align="center"><span class="math inline">\(\sum_{i=1}^n x_i\)</span></td>
<td align="center"></td>
</tr>
<tr class="odd">
<td align="center">Exponential</td>
<td align="center"><span class="math inline">\(\lambda\)</span></td>
<td align="center"><span class="math inline">\(\sum_{i=1}^n x_i\)</span></td>
<td align="center"><span class="math inline">\(Gamma(n, \lambda)\)</span></td>
</tr>
<tr class="even">
<td align="center">Normal (known <span class="math inline">\(\sigma^2\)</span>)</td>
<td align="center"><span class="math inline">\(\mu\)</span></td>
<td align="center"><span class="math inline">\(\frac{1}{n}\sum_{i=1}^n x_i\)</span></td>
<td align="center"><span class="math inline">\(Normal(\mu, \sigma^2/n)\)</span></td>
</tr>
<tr class="odd">
<td align="center">Normal</td>
<td align="center"><span class="math inline">\(\mu\)</span>, <span class="math inline">\(\sigma^2\)</span></td>
<td align="center"><span class="math inline">\((\sum_{i=1}^n x_i, \sum_{i=1}^n x_i^2)\)</span></td>
<td align="center"></td>
</tr>
<tr class="even">
<td align="center">Chi-Squared</td>
<td align="center"><span class="math inline">\(\nu\)</span></td>
<td align="center"><span class="math inline">\(\sum_{i=1}^n \log(x_i)\)</span></td>
<td align="center"></td>
</tr>
<tr class="odd">
<td align="center">Pareto (known min <span class="math inline">\(x_m\)</span>)</td>
<td align="center"><span class="math inline">\(\alpha\)</span></td>
<td align="center"><span class="math inline">\(\sum_{i=1}^n \log(x_i)\)</span></td>
<td align="center"></td>
</tr>
<tr class="even">
<td align="center">Gamma</td>
<td align="center"><span class="math inline">\(\alpha, \beta\)</span></td>
<td align="center"><span class="math inline">\((\sum_{i=1}^n\log(x_i), \sum_{i=1}^n x_i)\)</span></td>
<td align="center"></td>
</tr>
<tr class="odd">
<td align="center">Beta</td>
<td align="center"><span class="math inline">\(\alpha, \beta\)</span></td>
<td align="center"><span class="math inline">\((\sum_{i=1}^n\log(x_i), \sum_{i=1}^n\log(1 - x_i))\)</span></td>
<td align="center"></td>
</tr>
<tr class="even">
<td align="center">Weibull (known shape <span class="math inline">\(k\)</span>)</td>
<td align="center"><span class="math inline">\(\lambda\)</span></td>
<td align="center"><span class="math inline">\(\sum_{i=1}^n x^k\)</span></td>
<td align="center"></td>
</tr>
</tbody>
</table>
</div>
</div>
<div id="minimal-sufficiency" class="section level2 hasAnchor" number="7.2">
<h2><span class="header-section-number">7.2</span> Minimal Sufficiency<a href="statistics.html#minimal-sufficiency" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In any setting there are many sufficient statistics. However, we should aim at dealing with the statistic that summarizes the data as concisely as possible. Let <span class="math inline">\(S\)</span> be any a sufficient statistic for <span class="math inline">\(\theta\)</span>. In principle, <span class="math inline">\(W=(S,T)\)</span> is also a sufficient statistic, but we rather deal with <span class="math inline">\(S\)</span> reduces the data to one-dimension. When no further reduction from a sufficient statistic is possible, then that statistic is <strong>minimal sufficient</strong>.</p>
<div class="definition">
<p><span id="def:unlabeled-div-65" class="definition"><strong>Definition 7.4  (Minimal Sufficient Statistic) </strong></span>If <span class="math inline">\(T\)</span> is sufficient for <span class="math inline">\(\theta\)</span>, then it is a <strong>minimal sufficient statistic</strong> (MSS) if for any other sufficient statistic <span class="math inline">\(T^*, T\)</span> is a function of <span class="math inline">\(T^*\)</span>.</p>
</div>
<p><strong>Remark:</strong> Of all sufficient statistics, a minimal sufficient statistic offers the maximal reduction of the data.</p>
<p>Some “intuition” behind this definition goes as follows. A minimal sufficient statistic <span class="math inline">\(T(X)\)</span> creates a partition of the sample space, <span class="math inline">\(\Omega\)</span> into sets <span class="math inline">\(A_t\)</span>, where <span class="math inline">\(t \in \mathcal(T) = \{t : t = T(x) \text{ for some } x\in \Omega\}\)</span>. Now consider another sufficient statistic <span class="math inline">\(T^*(X)\)</span> that creates another partition of the sample space such that <span class="math inline">\(A^*_s = \{x: T^*(x)=s\}\)</span>. Then for ever <span class="math inline">\(s\)</span> there is a <span class="math inline">\(t\)</span> such that <span class="math inline">\(A^*_s \subset A_t\)</span>. Thus the partition associated with the minimal sufficient statistic is the coarsest possible partition for a sufficient statistic.</p>
<p>As before, the definition for MSS is conceptually useful but it does not help us find a MSS, or how to prove a statistic is a MSS. For this task, we invoke Casella-Berger’s Theorem 6.2.13.</p>
<div class="theorem">
<p><span id="thm:unlabeled-div-66" class="theorem"><strong>Theorem 7.4  (Casella-Berger's Theorem 6.2.13) </strong></span>Let <span class="math inline">\(X \sim f(x|\theta)\)</span>. Suppose the exists a statistic <span class="math inline">\(T=T(X)\)</span> such that for every <span class="math inline">\(x,y\)</span> in the support of <span class="math inline">\(X\)</span></p>
<p><span class="math display">\[(*) \frac{f(x|\theta)}{f(y|\theta)} = g(x,y) \Longleftrightarrow T(x)=T(y)\]</span>
then <span class="math inline">\(T\)</span> is a MSS.</p>
</div>
<p>In practice this theorem is used both to find a MSS and to prove that a given statistic is a MSS. Let us see some examples of applications of this theorem.</p>
<div class="example">
<p><span id="exm:unlabeled-div-67" class="example"><strong>Example 7.3  </strong></span>Let <span class="math inline">\(\textbf{X} = (X_1,..., X_n), X_i\)</span> iid <span class="math inline">\(N(\mu, \sigma^2), \theta = (\mu, \sigma^2)\)</span>. Consider <span class="math inline">\(T=(\bar{X}, S^2)\)</span> where <span class="math inline">\(S^2\)</span> is the sample variance. Let <span class="math inline">\(\textbf{x} = (x_1, ..., x_n), \textbf{y} = (y_1, ..., y_n)\)</span>. Then,</p>
<p><span class="math display">\[\frac{f(\textbf{x}|\theta)}{f(\textbf{y}|\theta)} = ... = exp\{-\frac{1}{2\sigma^2}[n(\bar{x}-\bar{y})^2 + 2 n\mu(\bar{x}-\bar{y}) -(n-1)(s^2_x - s^2_y)]\}.\]</span>
<span class="math inline">\(\Rightarrow\)</span> Suppose the ratio is independent of <span class="math inline">\(\theta = (\mu, \sigma^2)\)</span>. Then we must have that <span class="math inline">\(\bar{x} = \bar{y}, s^2_x = s^2_y\)</span>.</p>
<p><span class="math inline">\(\Leftarrow\)</span> Suppose that <span class="math inline">\(T(x)=T(y)\)</span>. Then <span class="math inline">\(\bar{x} = \bar{y}, s^2_x = s^2_y\)</span> and the ratio is 1.</p>
<p>By the previous theorem we have that <span class="math inline">\(T\)</span> is a MSS.</p>
</div>
<div class="example">
<p><span id="exm:unlabeled-div-68" class="example"><strong>Example 7.4  </strong></span>Let <span class="math inline">\(\textbf{X} = (X_1,..., X_n), X_i\)</span> iid <span class="math inline">\(U(\theta, \theta + 1)\)</span>.
<span class="math display">\[f(\textbf{x}|\theta) = \frac{1}{2^n} I(X_{(1)} &gt; \theta) I(X_{(n)} &lt; \theta + 1)\]</span>
Then,</p>
<p><span class="math display">\[\frac{f(\textbf{x}|\theta)}{f(\textbf{y}|\theta)} = \frac{I(x_{(1)} &gt; \theta ) I(x_{(n)} &lt; \theta + 1)}{I(y_{(1)} &gt; \theta) I(y_{(n)} &lt; \theta + 1)}\]</span>
and the ratio is independent of <span class="math inline">\(\theta\)</span> if and only if <span class="math inline">\((x_{(1)}, x_{(n)}) = (y_{(1)}, y_{(n)})\)</span>. Thus <span class="math inline">\(T = (X_{(1)}, X_{(n)})\)</span> is MSS.</p>
</div>
<p>Note that in the previous example, <span class="math inline">\(dim(T(X))&gt;dim(\theta)\)</span>. This means that there is no estimator of <span class="math inline">\(\theta\)</span> that is sufficient.</p>
<div class="theorem">
<p><span id="thm:unlabeled-div-69" class="theorem"><strong>Theorem 7.5  </strong></span>If <span class="math inline">\(X_1,..., X_n (n\ge 1)\)</span> are iid with <span class="math inline">\(X_i \sim k\)</span>-parameter exponential family, then <span class="math inline">\(T(X) = \Big(\sum_{i=1}^n t_1(x), ..., \sum_{i=1}^n t_k(x)\Big)\)</span> is a MSS.</p>
</div>
<p><strong>Fact</strong>: Like sufficient statistics, MSSs are not unique. Any 1-1 function of a MSS is also a MSS.</p>
</div>
<div id="ancillary-stats" class="section level2 hasAnchor" number="7.3">
<h2><span class="header-section-number">7.3</span> Ancillary Statistics<a href="statistics.html#ancillary-stats" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Sufficiency describes where all the information in the data is contained. Ancillarity is the dual of sufficiency, describing where there is no information.</p>
<div class="definition">
<p><span id="def:unlabeled-div-70" class="definition"><strong>Definition 7.5  (Ancillary Statistic) </strong></span>Let <span class="math inline">\(X \sim f(x|\theta)\)</span>. The statistic <span class="math inline">\(S(X)\)</span> is <strong>ancillary</strong> for <span class="math inline">\(\theta\)</span> if <span class="math inline">\(\mathcal{L}(S), g(s|\theta)\)</span> are independent of <span class="math inline">\(\theta\)</span>, i.e. for any <span class="math inline">\(\theta_1, \theta_2 \in \Theta\)</span>,
<span class="math display">\[g(s|\theta_1)=g(s|\theta_2) \text{ for all } s.\]</span></p>
</div>
<p>From this definition, it is generally possible to prove ancillary by definition - i.e., by showing that the distribution of <span class="math inline">\(S\)</span> does not depend on <span class="math inline">\(\theta\)</span>. However, several theorems also establish results about ancillary.</p>
<div class="example">
<p><span id="exm:unlabeled-div-71" class="example"><strong>Example 7.5  </strong></span>Let <span class="math inline">\(\textbf{X} = (X_1,..., X_n), X_i\)</span> iid <span class="math inline">\(U(\theta, \theta + 1)\)</span>. Consider the order statistics of the sample <span class="math inline">\(\textbf{X}\)</span>. The range statistic <span class="math inline">\(R = X_{(n)} - X_{(1)}\)</span> is ancillary for <span class="math inline">\(\theta\)</span> by showing that the pdf of <span class="math inline">\(R\)</span> is independent of <span class="math inline">\(\theta\)</span>. Intuitive, the range does not tell anything about the location of <span class="math inline">\(\theta\)</span> in the real line. In this case the ancillarity of <span class="math inline">\(R\)</span> does not depend on the uniformity of the observations, but on the parameter of the distribution being a location parameter.</p>
</div>
<div class="theorem">
<p><span id="thm:unlabeled-div-72" class="theorem"><strong>Theorem 7.6  (Location family ancillary statistic) </strong></span>Let <span class="math inline">\(\textbf{X} = (X_1,..., X_n), X_i\)</span> iid <span class="math inline">\(f_{\mathcal{L}}(x|\theta)\)</span>, a <a href="known-distributions.html#location-scale">location family</a>, then <span class="math inline">\(R = X_{(n)} - X_{(1)}\)</span> is ancillary for <span class="math inline">\(\theta\)</span>.</p>
</div>
<div class="theorem">
<p><span id="thm:unlabeled-div-73" class="theorem"><strong>Theorem 7.7  (Scale family ancillary statistic) </strong></span>Let <span class="math inline">\(\textbf{X} = (X_1,..., X_n), X_i\)</span> iid <span class="math inline">\(f_{\mathcal{S}}(x|\sigma)\)</span>, a <a href="known-distributions.html#location-scale">scale family</a>, and let <span class="math inline">\(S_j = X_j/X_n\)</span>. Then <span class="math inline">\(S=(S_1, S_2,..., S_n)\)</span> is ancillary for <span class="math inline">\(\sigma\)</span>.</p>
</div>
<p><strong>Note:</strong> Since <span class="math inline">\(S\)</span> is ancillary, any function of <span class="math inline">\(S=(S_1, S_2,..., S_n)\)</span> is also ancillary. For example, <span class="math inline">\(S_1 + ... + S_n\)</span> is also ancillary.</p>
<div class="theorem">
<p><span id="thm:unlabeled-div-74" class="theorem"><strong>Theorem 7.8  (Location-Scale family ancillary statistic) </strong></span>Let <span class="math inline">\(\textbf{X} = (X_1,..., X_n), X_i\)</span> iid <span class="math inline">\(f_{\mathcal{L,S}}(x|\mu,\sigma)\)</span>, a <a href="known-distributions.html#location-scale">location-scale family</a> where <span class="math inline">\(\theta = (\mu, \sigma)\)</span>. Let <span class="math inline">\(T_1= T_1(\textbf{X})\)</span> and <span class="math inline">\(T_2= T_2(\textbf{X})\)</span> be any two statistics such that</p>
<p><span class="math display">\[ (*) \quad T_j(aX_1 + b, ..., aX_n+b) = aT_j(X_1,..., X_n) \quad j = 1,2.\]</span></p>
<p>Then, <span class="math inline">\(T_1/T_2\)</span> is ancillary for <span class="math inline">\(\theta\)</span></p>
</div>
<div class="example">
<p><span id="exm:unlabeled-div-75" class="example"><strong>Example 7.6  </strong></span>Consider the range <span class="math inline">\(R = X_{(n)} - X_{(1)}\)</span> and <span class="math inline">\(S = \sqrt{\frac{1}{n-1}\sum(X_1 -\bar{X})^2}\)</span> satisfy (*) so <span class="math inline">\(R/S\)</span> is ancillary for <span class="math inline">\(\theta = (\mu, \sigma)\)</span>.</p>
</div>
<div id="why-are-we-interested-in-ancillary-statistics" class="section level3 hasAnchor" number="7.3.1">
<h3><span class="header-section-number">7.3.1</span> Why are we interested in ancillary statistics?<a href="statistics.html#why-are-we-interested-in-ancillary-statistics" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>There is some relationship between ancillarity and minimal sufficiency. Suppose there is a statistic <span class="math inline">\(c=C(\textbf{X})\)</span> and an ancillary statistic <span class="math inline">\(S=S(\textbf{X})\)</span> such that <span class="math inline">\(T=(S,C)\)</span> is minimal sufficient. The <strong>ancillarity principle</strong> states that inference on the parameter should be based on the conditional distribution of <span class="math inline">\(C\)</span> given the ancillary statistic <span class="math inline">\(S\)</span>. Furthermore, we will see later that ancillary statistics are independent of complete and sufficient statistics.</p>
<p><strong>Remark:</strong> Recall the <span class="math inline">\(U(\theta, \theta + 1)\)</span> example in which we found that the MSS, T, is two dimensional, and therefore no sufficient estimator for <span class="math inline">\(\theta\)</span> exists. We can write <span class="math inline">\(T = (S,C)\)</span> where <span class="math inline">\(C\)</span> has a marginal distribution that is ancillary (independent of the parameter), and then <span class="math inline">\(S\)</span> is conditionally sufficient, i.e. sufficient conditional on <span class="math inline">\(C\)</span>.</p>
<div class="example">
<p><span id="exm:unlabeled-div-76" class="example"><strong>Example 7.7  </strong></span>Let <span class="math inline">\(N\)</span> be a random variable with known distribution, <span class="math inline">\(P(N=n) = p_n\)</span>, and let <span class="math inline">\(X_1, ..., X_N\)</span> be iid with exponential family density. Then the likelihood of the data, <span class="math inline">\((N, X_1,..., X_N)\)</span> is</p>
<p><span class="math display">\[p_n (\prod_{i=1}^{n}h(x_i))c(\theta)^{n} exp\bigg\{\sum_{j=1}^{k} w_k(\theta) \sum_{i=1}^{n}t_j(x_i)\bigg\} \]</span>
and thus <span class="math inline">\(\bigg\{N, \{\sum_{i=1}^{N}t_j(x_i)\}_{j=1}^{k}\bigg\}\)</span> is sufficient for <span class="math inline">\(\theta\)</span>. Observe that <span class="math inline">\(N\)</span> is ancillary (independent of <span class="math inline">\(\theta\)</span>) and <span class="math inline">\(\{\sum_{i=1}^{N}t_j(x_i)\}_{j=1}^{k}\)</span> is sufficient for <span class="math inline">\(\theta\)</span> conditional on <span class="math inline">\(N\)</span>.</p>
</div>
<p><strong>Remark:</strong> Ancillary statistics may not be unique, and there is no general method for constructing them.</p>
<div class="example">
<p><span id="exm:unlabeled-div-77" class="example"><strong>Example 7.8  (Bivariate normal with unknown correlation) </strong></span>Suppose that <span class="math inline">\((X_1, Y_1), ..., (X_n, Y_n)\)</span> are iid bivariate normal with zero means and unit variances, and unknown correlations <span class="math inline">\(\rho\)</span>. Thus their joint pdf is</p>
<p><span class="math display">\[\frac{1}{(2\pi)^{n}(1-\rho^2)^{n/2}} exp \bigg\{ -\frac{\sum(x_i^2+y_i^2)}{2(1-\rho^2)} + \frac{\rho \sum x_iy_i}{(1-\rho^2)}\bigg\}.\]</span>
Since this is an exponential family distribution,it is easy to see that the MSS is <span class="math inline">\(T(X,Y) = (\sum(x_i^2+y_i^2), \sum x_iy_i )\)</span>. Observe that there is no ancillary statistic that is a function of <span class="math inline">\(T\)</span>. However, if we allow ancillary statistics that are not functions of <span class="math inline">\(T\)</span>, then both <span class="math inline">\(\sum x_i^2\)</span> and <span class="math inline">\(\sum y_i^2\)</span> are ancillary for <span class="math inline">\(\rho\)</span>. But which one do we choose? Any one of them!</p>
</div>
</div>
</div>
<div id="complete-stats" class="section level2 hasAnchor" number="7.4">
<h2><span class="header-section-number">7.4</span> Complete Statistics<a href="statistics.html#complete-stats" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>An important technical property for a statistic to have is completeness. .</p>
<div class="example">
<p><span id="exm:unlabeled-div-78" class="example"><strong>Example 7.9  (Complete Statistics) </strong></span>Let <span class="math inline">\(X\sim f(x|\theta), \theta \in \Theta\)</span>. A statistic <span class="math inline">\(T=T(X)\)</span> (not necessarily sufficient) is <strong>complete</strong> if for any function <span class="math inline">\(g(\cdot)\)</span>,</p>
<p><span class="math display">\[E_{\theta}(g(T)) = 0 \quad \forall \theta \Rightarrow P_{\theta}(g(T) = 0) = 1 \quad \forall \theta.\]</span></p>
</div>
<p><strong>Note:</strong> The family of pdfs of <span class="math inline">\(T\)</span> is called complete.</p>
<p><strong>Remark:</strong> Why do we care about completeness? The definition says that if <span class="math inline">\(T\)</span> is complete, then only function of <span class="math inline">\(T\)</span> that can have zero expectation on <span class="math inline">\(\theta\)</span> is the zero function. In other words, there cannot be any non-zero unbiased estimates of 0 based on <span class="math inline">\(T\)</span>.</p>
<p>If <span class="math inline">\(T\)</span> is not complete, then it is possible for the statistic to have mean 0, but also involve a distribution around 0 containing nonzero values of <span class="math inline">\(T(X)\)</span>. This means that there could be another <span class="math inline">\(\theta\)</span> that yields a different expectation than 0.</p>
<p>Conversely, if <span class="math inline">\(T\)</span> <em>is</em> complete, then a <span class="math inline">\(T\)</span> that estimates 0 must have a “trivial” distribution (a point mass at 0). This “trivial” distribution is important because it means that we can actually use it for inference. Completeness is used in the <a href="#lehmann-scheffe">Lehmann-Scheffe Theorem</a> to prove optimality results for estimators.</p>
<div id="techniques-for-finding-css" class="section level3 hasAnchor" number="7.4.1">
<h3><span class="header-section-number">7.4.1</span> Techniques for Finding CSS<a href="statistics.html#techniques-for-finding-css" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div class="example">
<p><span id="exm:unlabeled-div-79" class="example"><strong>Example 7.10  (Polynomial technique) </strong></span>Let <span class="math inline">\(\textbf{X}=(X_1,..., X_n), X_i\)</span> iid <span class="math inline">\(Bern(p)\)</span>. Consider <span class="math inline">\(T = \sum_{i=1}^{n}X_i\)</span>. Note that <span class="math inline">\(T\sim Bin(n,p)\)</span> as it is the sum of idd Bernoulli random variables. Suppose we have a <span class="math inline">\(g(\cdot)\)</span> function that satisfies the definition of complete statistic. Then <span class="math inline">\(T\)</span> would be complete.</p>
<p>Now what does <span class="math inline">\(E_{\theta}(g(T)) = 0 \quad \forall p\)</span> mean?</p>
<p><span class="math display">\[ 0 = \sum_{t=0}^{n} g(t) {n \choose t} p^t (1-p)^{n-t} = (1-p)^n \sum_{t=0}^{n} g(t) {n \choose t} \bigg(\frac{p}{1-p}\bigg)^t\]</span>
Observe that for any <span class="math inline">\(t, {n \choose t} \bigg(\frac{p}{1-p}\bigg)^t &gt; 0\)</span>. Furthermore <span class="math inline">\((1-p)^n&gt;0\)</span> for all <span class="math inline">\(n, p, t\)</span>. Observe that <span class="math inline">\(g(t)\)</span> is a polynomial of degree <span class="math inline">\(n\)</span> and the only option to satisfy the definition for T to be complete is for <span class="math inline">\(g(t) = 0 \forall t\)</span>. Now <span class="math inline">\(P(g(T) = 0)=1 \Rightarrow T\)</span> is complete.</p>
</div>
<div class="example">
<p><span id="exm:unlabeled-div-80" class="example"><strong>Example 7.11  </strong></span>Let <span class="math inline">\(\textbf{X}=(X_1,..., X_n), X_i\)</span> iid <span class="math inline">\(N(\theta, 1)\)</span>. Let <span class="math inline">\(T(X)=(X_1,X_2)\)</span> be a statistic. Observe that if <span class="math inline">\(g(T) = X_1 - X_2\)</span> then <span class="math inline">\(E(g(T)) = E(X_1 - X_2) = 0\)</span> but <span class="math inline">\(P(X_1 - X_2=0)\ne 1\)</span> for all <span class="math inline">\(\theta\)</span>. Thus <span class="math inline">\(T\)</span> is not a complete statistic.</p>
</div>
<div class="example">
<p><span id="exm:unlabeled-div-81" class="example"><strong>Example 7.12  </strong></span>Let <span class="math inline">\(\textbf{X}=(X_1,..., X_n), X_i\)</span> iid <span class="math inline">\(U(\theta, \theta + 1)\)</span>. Recall that <span class="math inline">\(T=T(X) = (X_{(1)}, X_{(n)})\)</span> is MSS for <span class="math inline">\(\theta\)</span>. Consider the ancillary statistic $ X_{(n)} - X_{(1)}$ and <span class="math inline">\(E(X_{(n)} - X_{(1)}) = c\)</span> for some <span class="math inline">\(c\)</span> independent of <span class="math inline">\(\theta\)</span>. Thus, <span class="math inline">\(E(X_{(n)} - X_{(1)}-c) = 0\)</span> but <span class="math inline">\(P(X_{(n)} - X_{(1)}-c=0) \ne 1\)</span> for all <span class="math inline">\(\theta\)</span>. Therefore <span class="math inline">\(T\)</span> is not a complete statistic.</p>
</div>
<p><strong>Remark:</strong> If <span class="math inline">\(T\)</span> is sufficient, then it contains all the information regarding <span class="math inline">\(\theta\)</span>. If <span class="math inline">\(T\)</span> is also <a href="statistics.html#complete-stats">complete</a>, then <span class="math inline">\(T\)</span> contains no <em>irrelevant</em> information about <span class="math inline">\(\theta\)</span>. Intuitively, suppose that <span class="math inline">\(T\)</span> is sufficient and let <span class="math inline">\(T = (T_1, S_1)\)</span> where <span class="math inline">\(S_1\)</span> is ancillary. Then <span class="math inline">\(T\)</span> cannot be complete because it contains irrelevant information about the parameter (recall that ancillary statistics describe where there is no information about <span class="math inline">\(\theta\)</span>).</p>
<div class="theorem">
<p><span id="thm:unlabeled-div-82" class="theorem"><strong>Theorem 7.9  </strong></span>If <span class="math inline">\(T\)</span> is complete and sufficient and a MSS exists, then <span class="math inline">\(T\)</span> is MSS.</p>
</div>
<div class="example">
<p><span id="exm:unlabeled-div-83" class="example"><strong>Example 7.13  </strong></span>Let <span class="math inline">\(\textbf{X}=(X_1,..., X_n), X_i\)</span> iid <span class="math inline">\(U(0, \theta ), \theta &gt;0\)</span>. Consider <span class="math inline">\(T = T(X)= X_{(n)}\)</span>. Is <span class="math inline">\(T\)</span> complete? Yes. To prove this we will employ the <em>polynomial technique</em>. By definition of expectation,
<span class="math display">\[ E(g(X_{(n)})) = \int_{0}^{\theta}g(x)f_{X_{(n)}}(x)dx.\]</span>
Recall <a href="#order-stats">order statistics</a> results, specifically the pdf formula for a given order statistic.</p>
<p><span class="math display">\[f_{X_{(n)}}(x) = n x^{n-1}\theta^{-n} I(0\le x \le \theta)\]</span>
Now, we have that <span class="math inline">\(E(g(X_{(n)})) = \int_{0}^{\theta}g(x)n x^{n-1}\theta^{-n}dx =n\theta^{-n} \int_{0}^{\theta}g(x)x^{n-1} = 0 \Rightarrow \int_{0}^{\theta}g(x)x^{n-1} = 0\)</span>. By applying <a href="math-tricks.html#math-tricks">Leibnitz’ Rule</a> wrt <span class="math inline">\(\theta\)</span> we obtain that <span class="math inline">\(g(\theta)\theta^{n-1}=0\)</span>, which implies that <span class="math inline">\(g(\cdot)\)</span> must be identically zero because we assumed that <span class="math inline">\(\theta &gt;0\)</span>. Therefore <span class="math inline">\(T\)</span> is a complete statistic.</p>
</div>
<p><strong>Remark:</strong> If the density of <span class="math inline">\(T\)</span> satisfies
<span class="math display">\[f(t|\theta)=h(t)c(\theta)I(0 \le t \le \theta)\]</span>
then <span class="math inline">\(T\)</span> is complete.</p>
<p>Certainly, the previous remark provides a useful and fast strategy to prove that a statistic is complete. However, as we have mentioned before, exponential family distributions provide what some people would call “easy and fast” strategies.</p>
<div class="theorem">
<p><span id="thm:unlabeled-div-84" class="theorem"><strong>Theorem 7.10  (Complete statistics in exponential family distributions) </strong></span>If <span class="math inline">\(X_1,..., X_n, iid, X_i\sim\)</span> exponential family with pdf/pmf of the form</p>
<p><span class="math display">\[h(x)c(\theta)\exp\Big(\sum_{i=1}^k w_i(\theta)t_i(x)\Big)\]</span></p>
<p>and <span class="math inline">\(\mathcal{H}=\{(w_1(\theta), ..., w_n(\theta))|\theta \in \Theta\}\)</span> contains an open set (or open rectangle) in <span class="math inline">\(\mathbb{R}^{k}\)</span>, then <span class="math inline">\(T=(T_1,..., T_k)\)</span> is a complete and sufficient statistic, where <span class="math inline">\(T_j = \sum_{i=1}^{n}t_j(x_i)\)</span>.</p>
</div>
<p>At a glance, this exponential family theorem on complete and sufficient statistics is great. However, there’s a catch: for most of us it is hard to grasp the concept of open set. In this part we will try to provide intuition behind the open set concept.</p>
<ul>
<li>An open <span class="math inline">\(k-\)</span> dimensional ball of radius <span class="math inline">\(r&gt;0\)</span> (also called epsilon-balls) centered at a point <span class="math inline">\(x \in \mathbb{R}^k\)</span> is defined as the set <span class="math inline">\(B(x,r) = \{y| d(x,y) &lt; r\}\)</span>, where <span class="math inline">\(d(x,y)\)</span> is a distance function.
<ul>
<li>Observe that what makes the ball <em>open</em> is the fact that those points <span class="math inline">\(y\)</span> whose distance to <span class="math inline">\(x\)</span> is exactly <span class="math inline">\(r\)</span> are not in the set.</li>
</ul></li>
</ul>
<p>However, we are interested in a set that contains an open set, like an open ball. Let <span class="math inline">\(A \subset \mathbb{R}^{k}\)</span>. We say that the set <span class="math inline">\(A\)</span> contains an open set in <span class="math inline">\(\mathbb{R}^{k}\)</span> if and only is <span class="math inline">\(A\)</span> contains a <span class="math inline">\(k-\)</span>dimensional ball B(x,r), i.e. there exists <span class="math inline">\(x \in \mathbb{R}^{k}\)</span> and <span class="math inline">\(r&gt;0\)</span> such that <span class="math inline">\(B(x,r) \subset A\)</span>.</p>
<p>Observe that we do not need our set <span class="math inline">\(A\)</span> to be open, we just need <span class="math inline">\(A\)</span> to <em>contain</em> at least one open set in the relevant space.</p>
<p>For example, in the real line any open interval contains an open set, i.e. <span class="math inline">\((a,b)\)</span> contains an open subset of <span class="math inline">\(\mathbb{R}\)</span> because for at least point between <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> we can fit an infinitesimally small open ball centered at the point.</p>
<p>Let us see an example.</p>
<div class="example">
<p><span id="exm:unlabeled-div-85" class="example"><strong>Example 7.14  </strong></span>Recall that the Bernoulli pmf is an exponential family distribution:</p>
<p><span class="math display">\[f(x|\theta) = \theta^x (1-\theta)^{1-x} = (1-\theta)exp\bigg\{ x\text{ }  log\bigg(\frac{p}{1-p}\bigg)\bigg\}, p \in (0,1), x \in \{0,1\}\]</span>
For an iid <span class="math inline">\(Bern(p)\)</span> sample, previous theorems say that <span class="math inline">\(T = \sum_{i=1}^{n} x_i\)</span> is a MSS for <span class="math inline">\(p\)</span>. Furthermore, <span class="math inline">\(\mathcal{H} = \{log(p/(1-p))|p \in (0,1)\}\)</span>. Does <span class="math inline">\(\mathcal{H}\)</span> contain an open set? Yes. Think about the range of <span class="math inline">\(log(p/(1-p))\)</span> for <span class="math inline">\(p \in (0,1)\)</span>. First observe that <span class="math inline">\(p/(1-p) \in (0, \infty)\)</span> which implies that <span class="math inline">\(log(p/(1-p)) \in (-\infty, \infty) = \mathbb{R}\)</span>, which certainly contains an open set, <span class="math inline">\((-1,1)\)</span> for example. Therefore <span class="math inline">\(\mathcal{H}\)</span> contains an open set in <span class="math inline">\(\mathbb{R}\)</span> and we conclude that <span class="math inline">\(T = \sum_{i=1}^{n} x_i\)</span> is a complete statistic.</p>
</div>
<p>Most of the time when the statistic T is in fact complete, we can think about the range of the <span class="math inline">\(w_i(\theta)\)</span> functions. If we can find any open set within the range, then we are good to go. However, the task becomes more difficult when the set <span class="math inline">\(\mathcal{H}\)</span> does not contain an open set, because this does <strong>not</strong> necesarily mean that the statistic is not complete.</p>
<div class="example">
<p><span id="exm:unlabeled-div-86" class="example"><strong>Example 7.15  </strong></span>Consider the following pdf</p>
<p><span class="math display">\[f(x|\theta) = \frac{2 exp \{-(x-\theta)^4\}}{\Gamma(1/4)}, -\infty &lt; x &lt; \infty, -\infty &lt; \theta&lt; \infty.\]</span>
Observe that this distribution belongs to the exponential family as we can express the pdf in exponential family form,</p>
<p><span class="math display">\[f(x|\theta) = \frac{2 exp \{-x^4\}}{\Gamma(1/4)} exp(\theta^4) exp\{(4\theta)x^3 - (6\theta^2) x^2 + (4\theta^3)x\}\]</span>
where <span class="math inline">\(t_1(x) = x^3, t_2(x) = x^2, t_3(x) = x\)</span> and <span class="math inline">\(w_1(\theta) = 4\theta, w_2(\theta)= 6\theta^2, w_3(\theta)=4\theta^3\)</span>. Therefore, this is a 3-parameter exponential family, with <span class="math inline">\(dim(\theta)=1\)</span>. Now
<span class="math display">\[
\begin{aligned}
\mathcal{H}&amp;=\{(w_1(\theta), w_2(\theta), w_3(\theta)) | w_1(\theta)  = 4\theta, w_2(\theta)= 6\theta^2, w_3(\theta)=4\theta^3, -\infty &lt; \theta&lt; \infty \}\\
&amp; = \{(w_1(\theta), w_2(\theta), w_3(\theta)) | w_1(\theta) \in \mathbb{R}, w_2(\theta)= -(6/16)w_1(\theta)^2, w_3(\theta)=w_1(\theta)^3/16 \}
\end{aligned}
\]</span>
where in the last equality we have that <span class="math inline">\(w_2(\theta), w_3(\theta)\)</span> can be expressed in terms of <span class="math inline">\(w_1(\theta)\)</span>. This implies that the set <span class="math inline">\(\mathcal{H}\)</span> is a curve in <span class="math inline">\(\mathbb{R}^3\)</span> and a sphere (open set in <span class="math inline">\(\mathbb{R}^3\)</span>) cannot fit in a curve and <span class="math inline">\(\mathcal{H}\)</span> does not contain an open set in <span class="math inline">\(\mathbb{R}^3\)</span>. This means that we cannot use the theorem to prove that <span class="math inline">\(T\)</span> is complete (or not complete). In this case we would have to use the definition of <a href="#complete-stat">complete statistic</a> to verify completeness.</p>
</div>
<div class="example">
<p><span id="exm:unlabeled-div-87" class="example"><strong>Example 7.16  </strong></span>Consider the <span class="math inline">\(N(\mu, \sigma^2)\)</span> family with <span class="math inline">\(\theta = (\mu, \sigma^2) \in \Theta\)</span>. This family has pdf that can be expressed as an exponential family with <span class="math inline">\(w_1(\theta) = \mu/\sigma^2, w_2(\theta) = -1/2\sigma^2, t_1(x)= x, t_2(x)=x^2\)</span>. Let <span class="math inline">\(\textbf{X} = (X_1,...,X_n)\)</span> be iid <span class="math inline">\(N(\mu, \sigma^2)\)</span>. By theorem, <span class="math inline">\(T(\textbf{X}) = \sum X_i, \sum X_i^2)\)</span> is a sufficient statistic regardless of the parameter set <span class="math inline">\(\Theta\)</span>. Furthermore, observe that <span class="math inline">\(U(\textbf{X}) = (\bar{x}, s^2)\)</span> is a 1-1 function of <span class="math inline">\(T(\textbf{X})\)</span>. Let us explore if <span class="math inline">\(T\)</span> (or <span class="math inline">\(U\)</span>) is complete. We will see that the application of the CSS for exponential family theorem will depend on the parameter set <span class="math inline">\(\Theta\)</span> and on <span class="math inline">\(\mathcal{H}\)</span>.</p>
<ol style="list-style-type: decimal">
<li><p><span class="math inline">\(\Theta_1=\{(\mu, \sigma^2)|\sigma^2&gt;0\} \Rightarrow \mathcal{H}= \{(\mu/\sigma^2, -1/2\sigma^2) | \sigma^2&gt;0 \}\)</span>. Observe that in this case there are no constraints on <span class="math inline">\(\mu\)</span>. Thus <span class="math inline">\(\mathcal{H} = \mathbb{R}^2\)</span> which trivially contains an open set of <span class="math inline">\(\mathbb{R}^2\)</span>. In this case we can use the theorem and conclude that <span class="math inline">\(T\)</span> is complete.</p></li>
<li><p><span class="math inline">\(\Theta_2=\{(\mu, \sigma^2)|\sigma^2=\sigma_0^2\} \Rightarrow \mathcal{H}= \{(\mu/\sigma_0^2, -1/2\sigma_0^2)\} = \{(x,-1/2\sigma_0^2)|x\in \mathbb{R}\}\)</span>. Now <span class="math inline">\(\sigma^2\)</span> can only take one value, which implies that <span class="math inline">\(\mathcal{H}\)</span> is a line in <span class="math inline">\(\mathbb{R}^2\)</span>. Can a disk fit in a line? No, thus we can’t use the theorem to assess completeness of <span class="math inline">\(T\)</span>. Using the definition of complete statistic one can prove that, in fact, <span class="math inline">\(T\)</span> is not complete in this case.</p></li>
<li><p><span class="math inline">\(\Theta_3 = [1,3] \times [4,6]\)</span> implies that <span class="math inline">\(\mu, \sigma^2\)</span> can take values within the closed square that is <span class="math inline">\(\Theta_3\)</span>. Does a disk fit into a square? Yes. Thus we can conclude that <span class="math inline">\(T\)</span> is complete.</p></li>
</ol>
</div>
<p>Hopefully that discussion clarifies when and how to use the theorem on complete statistics in exponential family distributions. Now, let us step back a bit and summarize what we have discussed until now as we present Basu’s Theorem, one of the most important theorems in statistical inference.</p>
</div>
<div id="basus-theorem" class="section level3 hasAnchor" number="7.4.2">
<h3><span class="header-section-number">7.4.2</span> Basu’s Theorem<a href="statistics.html#basus-theorem" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>How do we know if two statistics <span class="math inline">\(T\)</span> and <span class="math inline">\(S\)</span> are <strong>independent</strong>? Determining whether two statistics are independent directly can be difficult, since it requires finding their joint distribution, which may be infeasible. Basu’s theorem provides a more efficient technique for evaluating the independence of two statistics.</p>
<div class="theorem">
<p><span id="thm:unlabeled-div-88" class="theorem"><strong>Theorem 7.11  (Basu's Theorem) </strong></span>Let <span class="math inline">\(X \sim f(x|\theta), \theta \in \Theta\)</span>. If <span class="math inline">\(T(X)\)</span> is a complete and sufficient statistic (for <span class="math inline">\(\theta \in \Theta\)</span>), and <span class="math inline">\(S(X)\)</span> is ancillary, then <span class="math inline">\(T(X)\)</span> and <span class="math inline">\(S(X)\)</span> are independent for all <span class="math inline">\(\theta \in \Theta\)</span>.</p>
</div>
<p><strong>Remark:</strong> This result plainly says that CSSs are independent of ancillary statistics. Note, however, that the converse is not true; just because <span class="math inline">\(T\)</span> is independent of all ancillary statistics does not imply that <span class="math inline">\(T\)</span> is complete.</p>
<div class="example">
<p><span id="exm:unlabeled-div-89" class="example"><strong>Example 7.17  </strong></span>Let <span class="math inline">\(X_1, ..., X_n\)</span> iid <span class="math inline">\(N(\mu, \sigma^2)\)</span>. Suppose that <span class="math inline">\(\sigma^2\)</span> is known. We want to show that <span class="math inline">\(\bar{X}\)</span> and <span class="math inline">\(S^2\)</span> are independent using Basu’s Lemma.
- First, let us show that <span class="math inline">\(\bar{X}\)</span> is a CSS for <span class="math inline">\(\mu\)</span>. Observe that the pdf for a <span class="math inline">\(N(\mu, \sigma^2)\)</span> random variable where <span class="math inline">\(\sigma^2\)</span> is known is an exponential family distribution where <span class="math inline">\(t_1(x) = x, w_1(\theta)= \mu/\sigma^2\)</span>. Here <span class="math inline">\(\mathcal{H} = \{\mu/\sigma^2 | \mu \in \mathbb{R}\} = \mathbb{R}\)</span> which trially contains an open set. Thus <span class="math inline">\(T(X) = \{\sum X_i\}\)</span> is a CSS. Furthermore consider <span class="math inline">\(S(X) = T(X)/n = \bar{X}\)</span> is a 1-1 function of a CSS and thus is also a CSS.
- Second, we must prove that <span class="math inline">\(S^2\)</span> is ancillary for <span class="math inline">\(\mu\)</span>. By definition,
<span class="math display">\[S^2 = \frac{1}{n-1}\sum_{i=1}^{n}(X_i - \bar{X})^2\]</span>.
Let <span class="math inline">\(X_i = Z_i + \mu\)</span> where <span class="math inline">\(Z \sim N(0,1)\)</span>. Now,</p>
<p><span class="math display">\[
\begin{aligned}
S^2 &amp;= \frac{1}{n-1}\sum_{i=1}^{n}(X_i - \bar{X})^2\\
&amp; = \frac{1}{n-1}\sum_{i=1}^{n}(Z_i + \mu - \frac{\sum(Z_i + \mu)}{n})^2\\
&amp;= \frac{1}{n-1}\sum_{i=1}^{n}(Z_i + \mu - \frac{\sum Z_i}{n} - \mu)^2\\
&amp;= \frac{1}{n-1}\sum_{i=1}^{n}(Z_i - \bar{Z} )^2
\end{aligned}
\]</span>
which is independent of <span class="math inline">\(\mu\)</span>. Therefore, <span class="math inline">\(S^2\)</span> is ancillary for <span class="math inline">\(\mu\)</span>.</p>
<p>By Basu’s theorem we conclude that <span class="math inline">\(\bar{X}\)</span> is independent of <span class="math inline">\(S^2\)</span>. Now what happens when <span class="math inline">\(\sigma^2\)</span> is also unknown? Observe that the previous work assumed that <span class="math inline">\(\sigma ^2\)</span> as known, fixed, and <em>arbitrary</em>. Thus <span class="math inline">\(\bar{X}\)</span> is independent of <span class="math inline">\(S^2\)</span> even in the case where both <span class="math inline">\(\mu, \sigma^2\)</span> are unknown.</p>
</div>
<div class="example">
<p><span id="exm:unlabeled-div-90" class="example"><strong>Example 7.18  </strong></span>In this <a href="https://ani.stat.fsu.edu/~debdeep/complete.pdf">example</a> we will use Basu’s theorem to prove that a statistic is <em>not</em> complete.
Let <span class="math inline">\(\textbf{X} = (X_1,..., X_n), X_i\)</span> iid <span class="math inline">\(U(\theta, \theta + 1)\)</span>. In a previous example we saw that <span class="math inline">\(T(X) = (X_{(1)}, X_{(n)})\)</span> is a MSS for <span class="math inline">\(\theta\)</span>. Furthermore, we also saw that <span class="math inline">\(R=X_{(n)} - X_{(1)}\)</span> is an ancillary statistic for <span class="math inline">\(\theta\)</span>. Observe that <span class="math inline">\(R\)</span> is a function of <span class="math inline">\(T\)</span> and cannot be independent. Thus, by the contrapositive of Basu’s Lemma, <span class="math inline">\(T\)</span> cannot be complete.</p>
</div>

</div>
</div>
</div>
<h3>References<a href="references.html#references" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-Casella1990" class="csl-entry">
Casella, G C, and Roger L Berger. 1990. <em>Statistical Inference</em>. 2nd ed. The Wadsworth &amp; Brooks/Cole Statistics/Probability Series. Florence, KY: Brooks/Cole.
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="moments.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="point-estimators-finite-samples.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown-demo/edit/master/07-statistics.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["Solving-Statistical-Problems.pdf", "Solving-Statistical-Problems.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
