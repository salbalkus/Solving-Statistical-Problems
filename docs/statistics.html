<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 7 Statistics | Solving Statistical Problems</title>
  <meta name="description" content="Chapter 7 Statistics | Solving Statistical Problems" />
  <meta name="generator" content="bookdown 0.34 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 7 Statistics | Solving Statistical Problems" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="Chapter 7 Statistics | Solving Statistical Problems" />
  <meta name="github-repo" content="salbalkus/Solving-Statistical-Problems" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 7 Statistics | Solving Statistical Problems" />
  
  <meta name="twitter:description" content="Chapter 7 Statistics | Solving Statistical Problems" />
  

<meta name="author" content="Salvador Balkus, Kimberly Greco, and Mónica Robles Fontán" />


<meta name="date" content="2023-06-28" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="moments.html"/>
<link rel="next" href="point-estimators-finite-samples.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>
<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Solving Statistical Problems</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Introduction</a></li>
<li class="chapter" data-level="2" data-path="math-tricks.html"><a href="math-tricks.html"><i class="fa fa-check"></i><b>2</b> Math Tricks</a>
<ul>
<li class="chapter" data-level="2.1" data-path="math-tricks.html"><a href="math-tricks.html#combinatorics"><i class="fa fa-check"></i><b>2.1</b> Combinatorics</a></li>
<li class="chapter" data-level="2.2" data-path="math-tricks.html"><a href="math-tricks.html#geometric-series"><i class="fa fa-check"></i><b>2.2</b> Geometric Series</a></li>
<li class="chapter" data-level="2.3" data-path="math-tricks.html"><a href="math-tricks.html#exponential-taylor"><i class="fa fa-check"></i><b>2.3</b> Exponential Taylor Series</a></li>
<li class="chapter" data-level="2.4" data-path="math-tricks.html"><a href="math-tricks.html#exponential-limit"><i class="fa fa-check"></i><b>2.4</b> Exponential Limit</a></li>
<li class="chapter" data-level="2.5" data-path="math-tricks.html"><a href="math-tricks.html#ibp"><i class="fa fa-check"></i><b>2.5</b> Integration by Parts</a></li>
<li class="chapter" data-level="2.6" data-path="math-tricks.html"><a href="math-tricks.html#leibniz-rule"><i class="fa fa-check"></i><b>2.6</b> Leibniz’s Rule</a></li>
<li class="chapter" data-level="2.7" data-path="math-tricks.html"><a href="math-tricks.html#gamma-function"><i class="fa fa-check"></i><b>2.7</b> Gamma Function</a></li>
<li class="chapter" data-level="2.8" data-path="math-tricks.html"><a href="math-tricks.html#triangle-inequality"><i class="fa fa-check"></i><b>2.8</b> Triangle Inequality</a></li>
<li class="chapter" data-level="2.9" data-path="math-tricks.html"><a href="math-tricks.html#fubinis-theorem"><i class="fa fa-check"></i><b>2.9</b> Fubini’s Theorem</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="probability.html"><a href="probability.html"><i class="fa fa-check"></i><b>3</b> Probability</a>
<ul>
<li class="chapter" data-level="3.1" data-path="probability.html"><a href="probability.html#basic-axioms"><i class="fa fa-check"></i><b>3.1</b> Basic Axioms</a></li>
<li class="chapter" data-level="3.2" data-path="probability.html"><a href="probability.html#basic-probability-solving-techniques"><i class="fa fa-check"></i><b>3.2</b> Basic Probability Solving Techniques</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="probability.html"><a href="probability.html#disjointify"><i class="fa fa-check"></i><b>3.2.1</b> Disjointification</a></li>
<li class="chapter" data-level="3.2.2" data-path="probability.html"><a href="probability.html#demorgan"><i class="fa fa-check"></i><b>3.2.2</b> DeMorgan’s Laws</a></li>
<li class="chapter" data-level="3.2.3" data-path="probability.html"><a href="probability.html#proving-inequalities-subsetting"><i class="fa fa-check"></i><b>3.2.3</b> Proving Inequalities: Subsetting</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="probability.html"><a href="probability.html#conditional-probability"><i class="fa fa-check"></i><b>3.3</b> Conditional Probability</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="probability.html"><a href="probability.html#conditional-probability-in-practice"><i class="fa fa-check"></i><b>3.3.1</b> Conditional Probability in Practice</a></li>
<li class="chapter" data-level="3.6.3" data-path="probability.html"><a href="probability.html#important-theorems"><i class="fa fa-check"></i><b>3.6.3</b> Important Theorems</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="known-distributions.html"><a href="known-distributions.html"><i class="fa fa-check"></i><b>4</b> Known Distributions</a>
<ul>
<li class="chapter" data-level="4.1" data-path="known-distributions.html"><a href="known-distributions.html#families-of-distributions"><i class="fa fa-check"></i><b>4.1</b> Families of Distributions</a></li>
<li class="chapter" data-level="4.2" data-path="known-distributions.html"><a href="known-distributions.html#location-and-scale-families-location-scale"><i class="fa fa-check"></i><b>4.2</b> Location and Scale Families (#location-scale)</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="known-distributions.html"><a href="known-distributions.html#location-families"><i class="fa fa-check"></i><b>4.2.1</b> Location Families</a></li>
<li class="chapter" data-level="4.2.2" data-path="known-distributions.html"><a href="known-distributions.html#scale-families"><i class="fa fa-check"></i><b>4.2.2</b> Scale Families</a></li>
<li class="chapter" data-level="4.2.3" data-path="known-distributions.html"><a href="known-distributions.html#properties-of-location-scale-families"><i class="fa fa-check"></i><b>4.2.3</b> Properties of Location-Scale Families</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="known-distributions.html"><a href="known-distributions.html#exponential-family"><i class="fa fa-check"></i><b>4.3</b> Exponential Families</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="known-distributions.html"><a href="known-distributions.html#properties"><i class="fa fa-check"></i><b>4.3.1</b> Properties</a></li>
<li class="chapter" data-level="4.3.2" data-path="known-distributions.html"><a href="known-distributions.html#natural-exponential-family"><i class="fa fa-check"></i><b>4.3.2</b> Natural Exponential Families</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="known-distributions.html"><a href="known-distributions.html#known-univariate-exponential-families"><i class="fa fa-check"></i><b>4.4</b> Known Univariate Exponential Families</a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="known-distributions.html"><a href="known-distributions.html#bernoulli"><i class="fa fa-check"></i><b>4.4.1</b> Bernoulli</a></li>
<li class="chapter" data-level="4.4.2" data-path="known-distributions.html"><a href="known-distributions.html#binomial"><i class="fa fa-check"></i><b>4.4.2</b> Binomial</a></li>
<li class="chapter" data-level="4.4.3" data-path="known-distributions.html"><a href="known-distributions.html#geometric-distribution"><i class="fa fa-check"></i><b>4.4.3</b> Geometric</a></li>
<li class="chapter" data-level="4.4.4" data-path="known-distributions.html"><a href="known-distributions.html#negative-binomial"><i class="fa fa-check"></i><b>4.4.4</b> Negative Binomial</a></li>
<li class="chapter" data-level="4.4.5" data-path="known-distributions.html"><a href="known-distributions.html#poisson"><i class="fa fa-check"></i><b>4.4.5</b> Poisson</a></li>
<li class="chapter" data-level="4.4.6" data-path="known-distributions.html"><a href="known-distributions.html#normal"><i class="fa fa-check"></i><b>4.4.6</b> Normal</a></li>
<li class="chapter" data-level="4.4.7" data-path="known-distributions.html"><a href="known-distributions.html#exponential"><i class="fa fa-check"></i><b>4.4.7</b> Exponential</a></li>
<li class="chapter" data-level="4.4.8" data-path="known-distributions.html"><a href="known-distributions.html#gamma"><i class="fa fa-check"></i><b>4.4.8</b> Gamma</a></li>
<li class="chapter" data-level="4.4.9" data-path="known-distributions.html"><a href="known-distributions.html#beta"><i class="fa fa-check"></i><b>4.4.9</b> Beta</a></li>
<li class="chapter" data-level="4.4.10" data-path="known-distributions.html"><a href="known-distributions.html#chi-squared"><i class="fa fa-check"></i><b>4.4.10</b> Chi-squared</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="known-distributions.html"><a href="known-distributions.html#exponential-families-with-certain-parameters-fixed"><i class="fa fa-check"></i><b>4.5</b> Exponential families with certain parameters fixed</a>
<ul>
<li class="chapter" data-level="4.5.1" data-path="known-distributions.html"><a href="known-distributions.html#weibull"><i class="fa fa-check"></i><b>4.5.1</b> Weibull</a></li>
<li class="chapter" data-level="4.5.2" data-path="known-distributions.html"><a href="known-distributions.html#pareto"><i class="fa fa-check"></i><b>4.5.2</b> Pareto</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="known-distributions.html"><a href="known-distributions.html#non-exponential-families"><i class="fa fa-check"></i><b>4.6</b> Non-exponential families</a>
<ul>
<li class="chapter" data-level="4.6.1" data-path="known-distributions.html"><a href="known-distributions.html#uniform"><i class="fa fa-check"></i><b>4.6.1</b> Uniform</a></li>
<li class="chapter" data-level="4.6.2" data-path="known-distributions.html"><a href="known-distributions.html#cauchy"><i class="fa fa-check"></i><b>4.6.2</b> Cauchy</a></li>
<li class="chapter" data-level="4.6.3" data-path="known-distributions.html"><a href="known-distributions.html#t-distribution"><i class="fa fa-check"></i><b>4.6.3</b> t-distribution</a></li>
<li class="chapter" data-level="4.6.4" data-path="known-distributions.html"><a href="known-distributions.html#f-distribution"><i class="fa fa-check"></i><b>4.6.4</b> F-distribution</a></li>
<li class="chapter" data-level="4.6.5" data-path="known-distributions.html"><a href="known-distributions.html#hypergeometric"><i class="fa fa-check"></i><b>4.6.5</b> Hypergeometric</a></li>
</ul></li>
<li class="chapter" data-level="4.7" data-path="known-distributions.html"><a href="known-distributions.html#multivariate-distributions"><i class="fa fa-check"></i><b>4.7</b> Multivariate Distributions</a>
<ul>
<li class="chapter" data-level="4.7.1" data-path="known-distributions.html"><a href="known-distributions.html#bivariate-normal"><i class="fa fa-check"></i><b>4.7.1</b> Bivariate Normal</a></li>
<li class="chapter" data-level="4.7.2" data-path="known-distributions.html"><a href="known-distributions.html#multinomial"><i class="fa fa-check"></i><b>4.7.2</b> Multinomial</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="new-distributions.html"><a href="new-distributions.html"><i class="fa fa-check"></i><b>5</b> New Distributions</a>
<ul>
<li class="chapter" data-level="5.1" data-path="new-distributions.html"><a href="new-distributions.html#transformations"><i class="fa fa-check"></i><b>5.1</b> Transformations</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="new-distributions.html"><a href="new-distributions.html#theorems"><i class="fa fa-check"></i><b>5.1.1</b> Theorems</a></li>
<li class="chapter" data-level="5.1.2" data-path="new-distributions.html"><a href="new-distributions.html#practical-strategy"><i class="fa fa-check"></i><b>5.1.2</b> Practical Strategy</a></li>
<li class="chapter" data-level="5.1.3" data-path="new-distributions.html"><a href="new-distributions.html#proving-independence-from-a-joint-transformation"><i class="fa fa-check"></i><b>5.1.3</b> Proving Independence From a Joint Transformation</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="new-distributions.html"><a href="new-distributions.html#computing-joint-probabilities"><i class="fa fa-check"></i><b>5.2</b> Computing Joint Probabilities</a></li>
<li class="chapter" data-level="5.3" data-path="new-distributions.html"><a href="new-distributions.html#probability-integral-transform"><i class="fa fa-check"></i><b>5.3</b> Probability Integral Transform</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="new-distributions.html"><a href="new-distributions.html#hiearchical-models-iterated-moments"><i class="fa fa-check"></i><b>5.3.1</b> Hiearchical Models (Iterated Moments)</a></li>
<li class="chapter" data-level="5.3.2" data-path="new-distributions.html"><a href="new-distributions.html#convolutions"><i class="fa fa-check"></i><b>5.3.2</b> Convolutions</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="moments.html"><a href="moments.html"><i class="fa fa-check"></i><b>6</b> Moments</a>
<ul>
<li class="chapter" data-level="6.1" data-path="moments.html"><a href="moments.html#basic-definitions"><i class="fa fa-check"></i><b>6.1</b> Basic Definitions</a></li>
<li class="chapter" data-level="6.2" data-path="moments.html"><a href="moments.html#expected-value"><i class="fa fa-check"></i><b>6.2</b> <span class="math inline">\(E(X)\)</span> Properties</a></li>
<li class="chapter" data-level="6.3" data-path="moments.html"><a href="moments.html#varx-properties"><i class="fa fa-check"></i><b>6.3</b> <span class="math inline">\(Var(X)\)</span> Properties</a></li>
<li class="chapter" data-level="6.4" data-path="moments.html"><a href="moments.html#covariance-and-correlation"><i class="fa fa-check"></i><b>6.4</b> Covariance and Correlation</a></li>
<li class="chapter" data-level="6.5" data-path="moments.html"><a href="moments.html#conditional-expectation"><i class="fa fa-check"></i><b>6.5</b> Conditional Expectation</a></li>
<li class="chapter" data-level="6.6" data-path="moments.html"><a href="moments.html#mgf"><i class="fa fa-check"></i><b>6.6</b> Moment Generating Functions</a></li>
<li class="chapter" data-level="6.7" data-path="moments.html"><a href="moments.html#moment-bounds"><i class="fa fa-check"></i><b>6.7</b> Moment Inequalities</a></li>
<li class="chapter" data-level="6.8" data-path="moments.html"><a href="moments.html#techniques-for-deriving-moments"><i class="fa fa-check"></i><b>6.8</b> Techniques for Deriving Moments</a>
<ul>
<li class="chapter" data-level="6.8.1" data-path="moments.html"><a href="moments.html#bernoulli-direct-summation"><i class="fa fa-check"></i><b>6.8.1</b> Bernoulli: Direct Summation</a></li>
<li class="chapter" data-level="6.8.2" data-path="moments.html"><a href="moments.html#uniform-direct-integration"><i class="fa fa-check"></i><b>6.8.2</b> Uniform: Direct Integration</a></li>
<li class="chapter" data-level="6.8.3" data-path="moments.html"><a href="moments.html#geometric-series-convergence"><i class="fa fa-check"></i><b>6.8.3</b> Geometric: Series Convergence</a></li>
<li class="chapter" data-level="6.8.4" data-path="moments.html"><a href="moments.html#binomial-kernel-technique-series-version"><i class="fa fa-check"></i><b>6.8.4</b> Binomial: Kernel Technique, Series Version</a></li>
<li class="chapter" data-level="6.8.5" data-path="moments.html"><a href="moments.html#negative-binomial-and-hypergeometric-computing-exx-1"><i class="fa fa-check"></i><b>6.8.5</b> Negative Binomial and Hypergeometric: Computing <span class="math inline">\(E(X(X-1))\)</span></a></li>
<li class="chapter" data-level="6.8.6" data-path="moments.html"><a href="moments.html#poisson-exponential-taylor-series"><i class="fa fa-check"></i><b>6.8.6</b> Poisson: Exponential Taylor Series</a></li>
<li class="chapter" data-level="6.8.7" data-path="moments.html"><a href="moments.html#exponential-integration-by-parts"><i class="fa fa-check"></i><b>6.8.7</b> Exponential: Integration By Parts</a></li>
<li class="chapter" data-level="6.8.8" data-path="moments.html"><a href="moments.html#gamma-and-beta-kernel-technique-integration-version"><i class="fa fa-check"></i><b>6.8.8</b> Gamma and Beta: Kernel Technique, Integration Version</a></li>
<li class="chapter" data-level="6.8.9" data-path="moments.html"><a href="moments.html#normal-location-scale-trick-and-polar-integration"><i class="fa fa-check"></i><b>6.8.9</b> Normal: Location-Scale Trick and Polar Integration</a></li>
</ul></li>
<li class="chapter" data-level="6.9" data-path="moments.html"><a href="moments.html#other-moments-for-reference"><i class="fa fa-check"></i><b>6.9</b> Other Moments (for reference)</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="statistics.html"><a href="statistics.html"><i class="fa fa-check"></i><b>7</b> Statistics</a>
<ul>
<li class="chapter" data-level="7.1" data-path="statistics.html"><a href="statistics.html#sufficient-stats"><i class="fa fa-check"></i><b>7.1</b> Sufficient Statistics</a>
<ul>
<li class="chapter" data-level="7.1.1" data-path="statistics.html"><a href="statistics.html#important-exponential-family-result"><i class="fa fa-check"></i><b>7.1.1</b> Important exponential family result</a></li>
<li class="chapter" data-level="7.1.2" data-path="statistics.html"><a href="statistics.html#a-note-on-distributions-of-sufficient-statistics"><i class="fa fa-check"></i><b>7.1.2</b> A Note on Distributions of Sufficient Statistics</a></li>
<li class="chapter" data-level="7.1.3" data-path="statistics.html"><a href="statistics.html#moments-of-the-sufficient-statistic"><i class="fa fa-check"></i><b>7.1.3</b> Moments of the Sufficient Statistic</a></li>
<li class="chapter" data-level="7.1.4" data-path="statistics.html"><a href="statistics.html#table-of-sufficient-statistics"><i class="fa fa-check"></i><b>7.1.4</b> Table of Sufficient Statistics</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="statistics.html"><a href="statistics.html#minimal-sufficiency"><i class="fa fa-check"></i><b>7.2</b> Minimal Sufficiency</a></li>
<li class="chapter" data-level="7.3" data-path="statistics.html"><a href="statistics.html#ancillary-stats"><i class="fa fa-check"></i><b>7.3</b> Ancillary Statistics</a>
<ul>
<li class="chapter" data-level="7.3.1" data-path="statistics.html"><a href="statistics.html#why-are-we-interested-in-ancillary-statistics"><i class="fa fa-check"></i><b>7.3.1</b> Why are we interested in ancillary statistics?</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="point-estimators-finite-samples.html"><a href="point-estimators-finite-samples.html"><i class="fa fa-check"></i><b>8</b> Point Estimators: Finite Samples</a>
<ul>
<li class="chapter" data-level="8.1" data-path="point-estimators-finite-samples.html"><a href="point-estimators-finite-samples.html#method-of-moments"><i class="fa fa-check"></i><b>8.1</b> Method of Moments</a></li>
<li class="chapter" data-level="8.2" data-path="point-estimators-finite-samples.html"><a href="point-estimators-finite-samples.html#mle-theory"><i class="fa fa-check"></i><b>8.2</b> MLE Theory</a></li>
<li class="chapter" data-level="8.3" data-path="point-estimators-finite-samples.html"><a href="point-estimators-finite-samples.html#unbiasedness"><i class="fa fa-check"></i><b>8.3</b> Unbiasedness</a></li>
<li class="chapter" data-level="8.4" data-path="point-estimators-finite-samples.html"><a href="point-estimators-finite-samples.html#minimum-variance-cramer-rao-lower-bound"><i class="fa fa-check"></i><b>8.4</b> Minimum Variance (Cramer-Rao Lower Bound)</a></li>
<li class="chapter" data-level="8.5" data-path="point-estimators-finite-samples.html"><a href="point-estimators-finite-samples.html#uniform-minimum-variance-unbiased-estimators-umvues"><i class="fa fa-check"></i><b>8.5</b> Uniform Minimum Variance Unbiased Estimators (UMVUEs)</a></li>
<li class="chapter" data-level="8.6" data-path="point-estimators-finite-samples.html"><a href="point-estimators-finite-samples.html#inferential-properties-of-exponential-families-distributions"><i class="fa fa-check"></i><b>8.6</b> Inferential Properties of Exponential Families Distributions</a>
<ul>
<li class="chapter" data-level="8.6.1" data-path="point-estimators-finite-samples.html"><a href="point-estimators-finite-samples.html#bernoulli-1"><i class="fa fa-check"></i><b>8.6.1</b> Bernoulli</a></li>
<li class="chapter" data-level="8.6.2" data-path="point-estimators-finite-samples.html"><a href="point-estimators-finite-samples.html#binomial-1"><i class="fa fa-check"></i><b>8.6.2</b> Binomial</a></li>
<li class="chapter" data-level="8.6.3" data-path="point-estimators-finite-samples.html"><a href="point-estimators-finite-samples.html#geometric"><i class="fa fa-check"></i><b>8.6.3</b> Geometric</a></li>
<li class="chapter" data-level="8.6.4" data-path="point-estimators-finite-samples.html"><a href="point-estimators-finite-samples.html#negative-binomial-1"><i class="fa fa-check"></i><b>8.6.4</b> Negative Binomial</a></li>
<li class="chapter" data-level="8.6.5" data-path="point-estimators-finite-samples.html"><a href="point-estimators-finite-samples.html#poisson-1"><i class="fa fa-check"></i><b>8.6.5</b> Poisson</a></li>
<li class="chapter" data-level="8.6.6" data-path="point-estimators-finite-samples.html"><a href="point-estimators-finite-samples.html#normal-1"><i class="fa fa-check"></i><b>8.6.6</b> Normal</a></li>
<li class="chapter" data-level="8.6.7" data-path="point-estimators-finite-samples.html"><a href="point-estimators-finite-samples.html#exponential-1"><i class="fa fa-check"></i><b>8.6.7</b> Exponential</a></li>
<li class="chapter" data-level="8.6.8" data-path="point-estimators-finite-samples.html"><a href="point-estimators-finite-samples.html#gamma-1"><i class="fa fa-check"></i><b>8.6.8</b> Gamma</a></li>
<li class="chapter" data-level="8.6.9" data-path="point-estimators-finite-samples.html"><a href="point-estimators-finite-samples.html#pareto-1"><i class="fa fa-check"></i><b>8.6.9</b> Pareto</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="point-estimators-asymptotics.html"><a href="point-estimators-asymptotics.html"><i class="fa fa-check"></i><b>9</b> Point Estimators: Asymptotics</a>
<ul>
<li class="chapter" data-level="9.1" data-path="point-estimators-asymptotics.html"><a href="point-estimators-asymptotics.html#consistency"><i class="fa fa-check"></i><b>9.1</b> Consistency</a>
<ul>
<li class="chapter" data-level="9.1.1" data-path="point-estimators-asymptotics.html"><a href="point-estimators-asymptotics.html#weak-law-of-large-numbers"><i class="fa fa-check"></i><b>9.1.1</b> Weak Law of Large Numbers</a></li>
<li class="chapter" data-level="9.1.2" data-path="point-estimators-asymptotics.html"><a href="point-estimators-asymptotics.html#direct-proof-via-convergence-in-probability"><i class="fa fa-check"></i><b>9.1.2</b> Direct Proof via Convergence in Probability</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="point-estimators-asymptotics.html"><a href="point-estimators-asymptotics.html#asymptotic-efficiency"><i class="fa fa-check"></i><b>9.2</b> Asymptotic Efficiency</a>
<ul>
<li class="chapter" data-level="9.2.1" data-path="point-estimators-asymptotics.html"><a href="point-estimators-asymptotics.html#central-limit-theorems"><i class="fa fa-check"></i><b>9.2.1</b> Central Limit Theorems</a></li>
<li class="chapter" data-level="9.2.2" data-path="point-estimators-asymptotics.html"><a href="point-estimators-asymptotics.html#the-delta-method"><i class="fa fa-check"></i><b>9.2.2</b> The Delta Method</a></li>
<li class="chapter" data-level="9.2.3" data-path="point-estimators-asymptotics.html"><a href="point-estimators-asymptotics.html#cramer-wold-device"><i class="fa fa-check"></i><b>9.2.3</b> Cramer-Wold Device</a></li>
<li class="chapter" data-level="9.2.4" data-path="point-estimators-asymptotics.html"><a href="point-estimators-asymptotics.html#asymptotic-distribution-in-practice."><i class="fa fa-check"></i><b>9.2.4</b> Asymptotic Distribution in Practice.</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="point-estimators-asymptotics.html"><a href="point-estimators-asymptotics.html#asymptotic-properties-of-mles"><i class="fa fa-check"></i><b>9.3</b> Asymptotic Properties of MLEs</a></li>
<li class="chapter" data-level="9.4" data-path="point-estimators-asymptotics.html"><a href="point-estimators-asymptotics.html#variance-stabilizing-transformations"><i class="fa fa-check"></i><b>9.4</b> Variance Stabilizing Transformations</a></li>
<li class="chapter" data-level="9.5" data-path="point-estimators-asymptotics.html"><a href="point-estimators-asymptotics.html#asymptotic-confidence-intervals"><i class="fa fa-check"></i><b>9.5</b> Asymptotic Confidence Intervals</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="hypothesis-tests-finite-samples.html"><a href="hypothesis-tests-finite-samples.html"><i class="fa fa-check"></i><b>10</b> Hypothesis Tests: Finite Samples</a></li>
<li class="chapter" data-level="11" data-path="hypothesis-tests-asymptotics.html"><a href="hypothesis-tests-asymptotics.html"><i class="fa fa-check"></i><b>11</b> Hypothesis Tests: Asymptotics</a>
<ul>
<li class="chapter" data-level="11.1" data-path="hypothesis-tests-asymptotics.html"><a href="hypothesis-tests-asymptotics.html#wald-test"><i class="fa fa-check"></i><b>11.1</b> Wald Test</a></li>
<li class="chapter" data-level="11.2" data-path="hypothesis-tests-asymptotics.html"><a href="hypothesis-tests-asymptotics.html#score-test"><i class="fa fa-check"></i><b>11.2</b> Score Test</a></li>
<li class="chapter" data-level="11.3" data-path="hypothesis-tests-asymptotics.html"><a href="hypothesis-tests-asymptotics.html#likelihood-ratio-test"><i class="fa fa-check"></i><b>11.3</b> Likelihood Ratio Test</a></li>
<li class="chapter" data-level="11.4" data-path="hypothesis-tests-asymptotics.html"><a href="hypothesis-tests-asymptotics.html#composite-null-hypotheses"><i class="fa fa-check"></i><b>11.4</b> Composite Null Hypotheses</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="generating-random-variables.html"><a href="generating-random-variables.html"><i class="fa fa-check"></i><b>12</b> Generating Random Variables</a></li>
<li class="chapter" data-level="13" data-path="random-processes.html"><a href="random-processes.html"><i class="fa fa-check"></i><b>13</b> Random Processes</a>
<ul>
<li class="chapter" data-level="13.1" data-path="random-processes.html"><a href="random-processes.html#poisson-processes"><i class="fa fa-check"></i><b>13.1</b> Poisson Processes</a>
<ul>
<li class="chapter" data-level="13.1.1" data-path="random-processes.html"><a href="random-processes.html#memorylessness-of-the-exponential"><i class="fa fa-check"></i><b>13.1.1</b> Memorylessness of the Exponential</a></li>
<li class="chapter" data-level="13.1.2" data-path="random-processes.html"><a href="random-processes.html#count-time-duality"><i class="fa fa-check"></i><b>13.1.2</b> Count-Time Duality</a></li>
<li class="chapter" data-level="13.1.3" data-path="random-processes.html"><a href="random-processes.html#poisson-distribution"><i class="fa fa-check"></i><b>13.1.3</b> Poisson Distribution</a></li>
<li class="chapter" data-level="13.1.4" data-path="random-processes.html"><a href="random-processes.html#exponential-distribution"><i class="fa fa-check"></i><b>13.1.4</b> Exponential Distribution</a></li>
<li class="chapter" data-level="13.1.5" data-path="random-processes.html"><a href="random-processes.html#example"><i class="fa fa-check"></i><b>13.1.5</b> Example</a></li>
</ul></li>
<li class="chapter" data-level="13.2" data-path="random-processes.html"><a href="random-processes.html#branching-processes"><i class="fa fa-check"></i><b>13.2</b> Branching Processes</a>
<ul>
<li class="chapter" data-level="13.2.1" data-path="random-processes.html"><a href="random-processes.html#random-variables"><i class="fa fa-check"></i><b>13.2.1</b> Random Variables</a></li>
<li class="chapter" data-level="13.2.2" data-path="random-processes.html"><a href="random-processes.html#probability-generating-function"><i class="fa fa-check"></i><b>13.2.2</b> Probability Generating Function</a></li>
<li class="chapter" data-level="13.2.3" data-path="random-processes.html"><a href="random-processes.html#criticality-theorem"><i class="fa fa-check"></i><b>13.2.3</b> Criticality Theorem</a></li>
<li class="chapter" data-level="13.2.4" data-path="random-processes.html"><a href="random-processes.html#example-1"><i class="fa fa-check"></i><b>13.2.4</b> Example</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Solving Statistical Problems</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="statistics" class="section level1 hasAnchor" number="7">
<h1><span class="header-section-number">Chapter 7</span> Statistics<a href="statistics.html#statistics" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>One of the most important tasks we will be performing as statisticians is inference. Inference is the area of statistics which uses sample data to estimate a population parameter. For this it is necessary to <em>reduce</em> or <em>summarize</em> the recollected data into a measurement that we will refer to as <strong>statistic</strong>.</p>
<div class="definition">
<p><span id="def:unlabeled-div-53" class="definition"><strong>Definition 7.1  (Statistic) </strong></span>Let <span class="math inline">\(X \sim f(x|\theta)\)</span>, where both <span class="math inline">\(X, \theta\)</span> can be vectors. A statistic is a function <span class="math inline">\(T=T(\textbf{X})\)</span> of the sample <span class="math inline">\(\textbf{X}\)</span> from <span class="math inline">\(X\)</span>.</p>
</div>
<p>For example, the <span class="math inline">\(T= T(\textbf{X}) = \frac{1}{n} \sum_{i=1}^{n} X_i\)</span> reduces the sample <span class="math inline">\(\textbf{X}\)</span> to a single measurement.</p>
<p>In general, we would like to choose statistics that satisfy principles that will make inference about the population parameter <em>easier</em>. One of these principles is <strong>sufficiency</strong>. The idea behind <strong>sufficiency</strong> is to retain information about the population parameter, say <span class="math inline">\(\theta\)</span>, while reducing the data.</p>
<div id="sufficient-stats" class="section level2 hasAnchor" number="7.1">
<h2><span class="header-section-number">7.1</span> Sufficient Statistics<a href="statistics.html#sufficient-stats" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><strong>Sufficiency Principle</strong>. If <span class="math inline">\(T\)</span> is sufficient, then any information about the parameter <span class="math inline">\(\theta\)</span> should depend on <span class="math inline">\(X\)</span> only through <span class="math inline">\(T\)</span>.</p>
<div class="definition">
<p><span id="def:unlabeled-div-54" class="definition"><strong>Definition 7.2  (Sufficient Statistic) </strong></span>Let <span class="math inline">\(X \sim f(x|\theta)\)</span>. We say <span class="math inline">\(T=T(X)\)</span> is a sufficient statistic (SS) for <span class="math inline">\(\theta\)</span> if <span class="math inline">\(\mathcal{L}(\theta|X)\)</span> is independent of <span class="math inline">\(\theta\)</span>, i.e. <span class="math inline">\(f(x|T;\theta) = g(x)\)</span>.</p>
</div>
<p>Let’s see some examples.</p>
<div class="example">
<p><span id="exm:unlabeled-div-55" class="example"><strong>Example 7.1  </strong></span>Let <span class="math inline">\(X = (X_1, ..., X_n), X_i\)</span> iid N(<span class="math inline">\(\theta\)</span>, 1) and <span class="math inline">\(T = \bar{X}\)</span>. Recall that <span class="math inline">\(\bar{X} \sim N(\theta, 1/n)\)</span>.
<span class="math display">\[
\begin{aligned}
f(x|T, \theta) = n^{-1/2}(2\pi)^{-(\frac{n-1}{2})} exp\{-\sum_{i=1}^{n}(x_i - \bar{x})^2\}
\end{aligned}
\]</span>
Observe that the conditional density is independent of <span class="math inline">\(\theta\)</span>. By definition, <span class="math inline">\(T = \bar{X}\)</span> is a sufficient statistic for <span class="math inline">\(\theta\)</span>.</p>
</div>
<div class="example">
<p><span id="exm:unlabeled-div-56" class="example"><strong>Example 7.2  </strong></span>Let <span class="math inline">\(X_1, X_2\)</span> iid Poisson(<span class="math inline">\(\lambda\)</span>). The joint distribution of <span class="math inline">\(X_1, X_2\)</span> is <span class="math inline">\(P(X_1=x_1, X_2 = x_2) = \frac{\lambda^{x_1+x_2}exp(-2\lambda)}{x_1! x_2!}\)</span>. Let <span class="math inline">\(T(X_1, X_2) = X_1+X_2\)</span>. Observe that <span class="math inline">\(X_1+X_2 \sim\)</span> Poisson(2<span class="math inline">\(\lambda\)</span>). Thus,</p>
<p><span class="math display">\[
\begin{aligned}
P(X_1=x_1, X_2 = x_2|X_1+X_2=t) &amp;= \frac{P(X_1=x_1, X_2 = t - x_1)}{P(X_1 + X_2 = t)} \\
&amp;=\frac{exp(-\lambda)\lambda^{x_1} exp(-\lambda)\lambda^{t -x_1}t!}{x_1!(t-x_1)! exp(-2\lambda)(2\lambda)^t}\\
&amp;= { t \choose x_1} \bigg(\frac{1}{2}\bigg)^t
\end{aligned}
\]</span>
which is independent of <span class="math inline">\(\lambda\)</span> and <span class="math inline">\(X_1+X_2\)</span> is sufficient for <span class="math inline">\(\lambda\)</span>.</p>
</div>
<p>As can be seen from the example, we need a candidate statistic to prove sufficiency using the definition. Furthermore, checking sufficiency of a statistics is difficult because we need to compute the conditional distribution.</p>
<div class="theorem">
<p><span id="thm:unlabeled-div-57" class="theorem"><strong>Theorem 7.1  (Factorization Theorem) </strong></span><span class="math inline">\(T(X)\)</span> is sufficient for <span class="math inline">\(\theta \Longleftrightarrow \exists g(t|\theta)\)</span> and <span class="math inline">\(h(x)\)</span>, such that</p>
<p><span class="math display">\[f(x|\theta)=g(t|\theta)h(x)\]</span>
<span class="math inline">\(\forall x, \theta\)</span>.</p>
</div>
<p>Note that the factorization theorem tells us that if we can manipulate <span class="math inline">\(f(x|\theta)\)</span> as above, we have a sufficient statistic.</p>
<div class="example">
<p><span id="exm:unlabeled-div-58" class="example"><strong>Example 7.3  </strong></span>Let <span class="math inline">\(\textbf{X}= (X_1, ..., X_n), X_i\)</span> iid <span class="math inline">\(Poisson(\lambda)\)</span>.
<span class="math display">\[
\begin{aligned}
P(X_1=x_1,..., X_n = x_n) &amp;= \frac{exp(-n\lambda)\lambda^{\sum x_i}}{\prod x_i!}\\
&amp;= h(x)g(\sum x_i|\lambda)
\end{aligned}
\]</span>
<span class="math inline">\(\Rightarrow T(X) = \sum X_i\)</span> sufficient for <span class="math inline">\(\lambda\)</span>.</p>
</div>
<div class="example">
<p><span id="exm:unlabeled-div-59" class="example"><strong>Example 7.4  </strong></span>Let <span class="math inline">\(\textbf{X}= (X_1, ..., X_n), X_i\)</span> iid <span class="math inline">\(Uniform(0,\theta)\)</span>.</p>
<p><span class="math display">\[
\begin{aligned}
P_{\theta}(x_1, ..., x_n) &amp;= \frac{1}{\theta^n} \prod_{i=1}^{n}I(0&lt;x_i&lt;\theta)\\
&amp; = \frac{1}{\theta^n} I(x_{(1)}&gt;0) I(x_{(n)}&lt;\theta)
\end{aligned}
\]</span>
<span class="math inline">\(\Rightarrow T(X) = X_{(n)}\)</span> sufficient for <span class="math inline">\(\theta\)</span>.</p>
</div>
<div id="important-facts-about-ss" class="section level4 hasAnchor" number="7.1.0.1">
<h4><span class="header-section-number">7.1.0.1</span> Important facts about SS<a href="statistics.html#important-facts-about-ss" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li>Sufficient statistics may or may not reduce the data.
<ul>
<li>Original data are always sufficient.</li>
<li>In iid sample, the order statistics are sufficient.</li>
</ul></li>
<li>Sufficient statistics are never unique.
<ul>
<li><p>Suppose <span class="math inline">\(X \sim N(0,\sigma^2)\)</span>. Then by the factorization theorem <span class="math inline">\(T(X)=X^2, |X|, X^4, exp(X^2)\)</span> are all sufficient.</p></li>
<li><p>Any 1-1 function <span class="math inline">\(g\)</span> of a sufficient statistic is also sufficient.</p>
<p><strong>Proof</strong>. Let <span class="math inline">\(T^*(X)= g(T(X))\)</span>. By assumption <span class="math inline">\(g^{-1}\)</span> exists since g is 1-1.</p></li>
</ul>
<span class="math display">\[
\begin{aligned}
f(x|\theta) &amp;= g(T(X)|\theta)\\
    &amp;=g(r^{-1}(T^*(x))|\theta)h(x)\\
\end{aligned}
\]</span>
By the factorization theorem, <span class="math inline">\(T^*(X)\)</span> is sufficient for <span class="math inline">\(\theta\)</span>.</li>
</ul>
</div>
<div id="important-exponential-family-result" class="section level3 hasAnchor" number="7.1.1">
<h3><span class="header-section-number">7.1.1</span> Important exponential family result<a href="statistics.html#important-exponential-family-result" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Consider a sample <span class="math inline">\(X=(X_1,...,X_n)\)</span> from <span class="math inline">\(f_X(x|\theta)= h(x)c(\theta)\exp\Big(\sum_{i=1}^k w_i(\theta)t_i(x)\Big)\)</span>. Then <span class="math inline">\(T(X) = \Big(\sum_{i=1}^n t_1(x), ..., \sum_{i=1}^n t_k(x)\Big)\)</span> is a <a href="statistics.html#statistics">sufficient statistic</a>. Note that this result follows directly from the <a href="#id_%20sufficient-stats">factorization theorem</a>.</p>
</div>
<div id="a-note-on-distributions-of-sufficient-statistics" class="section level3 hasAnchor" number="7.1.2">
<h3><span class="header-section-number">7.1.2</span> A Note on Distributions of Sufficient Statistics<a href="statistics.html#a-note-on-distributions-of-sufficient-statistics" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Recall a convenient property of <a href="known-distributions.html#exponential-family">exponential families</a>: that their maximum likelihood estimate is a function of their sufficient statistic. Because of this, in order to prove <a href="point-estimators-finite-samples.html#point-estimators-finite-samples">Finite Sample Properties</a> of an estimator or construct <a href="hypothesis-tests-finite-samples.html#hypothesis-tests-finite-samples">Hypothesis Tests</a>, it is often useful to understand their distributions. This is why the distributions of each of the sufficient statistics are included in the fourth column of the table below These distributions are mostly derived from additive, location-scale, and other properties in <a href="known-distributions.html#known-distributions">Chapter 4 - Known Distributions</a></p>
</div>
<div id="moments-of-the-sufficient-statistic" class="section level3 hasAnchor" number="7.1.3">
<h3><span class="header-section-number">7.1.3</span> Moments of the Sufficient Statistic<a href="statistics.html#moments-of-the-sufficient-statistic" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>As stated by <span class="citation">Casella and Berger (<a href="#ref-Casella1990">1990</a>)</span>, if <span class="math inline">\(X\)</span> is an exponential family, the moments of its exponential family can be easily computed using certain properties.</p>
<div class="theorem">
<p><span id="thm:unlabeled-div-60" class="theorem"><strong>Theorem 7.2  (I Don't Know What To Call This) </strong></span>If <span class="math inline">\(X\)</span> is an <a href="known-distributions.html#exponential-family">exponential family</a>, then</p>
<p><span class="math display">\[E\Big(\sum_{i=1}^k \frac{\partial w_i(\theta)}{\partial \theta_j}t_i(X)\Big) = -\frac{\partial}{\partial\theta_j} \log c(\theta)\]</span>
and</p>
<p><span class="math display">\[Var\Big(\sum_{i=1}^k \frac{\partial w_i(\theta)}{\partial \theta_j}t_i(X)\Big) = -\frac{\partial}{\partial\theta_j} \log c(\theta) - E\Big(\sum_{i=1}^k \frac{\partial^2w_i(\theta)}{\partial \theta_j^2}t_i(X)\Big)\]</span></p>
</div>
<p>If <span class="math inline">\(X\)</span> is a <a href="known-distributions.html#natural-exponential-family">natural exponential family</a>, then these identities simplify even further.</p>
<div class="theorem">
<p><span id="thm:unlabeled-div-61" class="theorem"><strong>Theorem 7.3  (I Don't Know What To Call This 2) </strong></span>If <span class="math inline">\(X\)</span> is a natural exponential family, then$</p>
<p><span class="math display">\[E(t_j(X)) = -\frac{\partial}{\partial\eta_j}\log(c^*(\eta))\]</span>
and</p>
<p><span class="math display">\[Var(t_j(X)) = -\frac{\partial^2}{\partial\eta_j^2}\log(c^*(\eta))\]</span></p>
</div>
<p>Using these theorems, we can calculate the moments of the sufficient statistics of exponential families directly. This is because if <span class="math inline">\(T(X)\)</span> is a sufficient statistic, then <span class="math inline">\(T(X) = \Big(\sum_{i=1}^nt_1(X), ..., \sum_{i=1}^nt_k(X)\Big)\)</span>. Suppose, for simplicity, that <span class="math inline">\(T(X)\)</span> is one-dimensional and <span class="math inline">\(X_i\)</span> are iid. Then, if <span class="math inline">\(X_i\)</span> is a natural exponential family,</p>
<p><span class="math display">\[E(T(X)) = E\Big(\sum_{i=1}^nt(X_i)\Big) = nE(t(X_i)) = n\cdot-\frac{\partial}{\partial \eta}\log(c^*(\eta))\]</span>
In fact, letting <span class="math inline">\(A(\eta) = -\log(c^*(\eta))\)</span>, we can obtain all of the moments of <span class="math inline">\(T(X)\)</span> by simply differentiating <span class="math inline">\(A(\eta)\)</span></p>
</div>
<div id="table-of-sufficient-statistics" class="section level3 hasAnchor" number="7.1.4">
<h3><span class="header-section-number">7.1.4</span> Table of Sufficient Statistics<a href="statistics.html#table-of-sufficient-statistics" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Translated from <a href="https://en.wikipedia.org/wiki/Exponential_family" class="uri">https://en.wikipedia.org/wiki/Exponential_family</a></p>
<table>
<colgroup>
<col width="25%" />
<col width="25%" />
<col width="25%" />
<col width="25%" />
</colgroup>
<thead>
<tr class="header">
<th align="center">Distribution</th>
<th align="center">Parameter</th>
<th align="center">Sufficient Statistic</th>
<th align="center">S.S. Distribution</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">Bernoulli</td>
<td align="center">p</td>
<td align="center"><span class="math inline">\(\sum_{i=1}^n x_i\)</span></td>
<td align="center"><span class="math inline">\(Binomial(n, p)\)</span></td>
</tr>
<tr class="even">
<td align="center">Binomial</td>
<td align="center">p</td>
<td align="center"><span class="math inline">\(\sum_{i=1}^n x_i\)</span></td>
<td align="center"><span class="math inline">\(Binomial(nm, p)\)</span></td>
</tr>
<tr class="odd">
<td align="center">Poisson</td>
<td align="center"><span class="math inline">\(\lambda\)</span></td>
<td align="center"><span class="math inline">\(\sum_{i=1}^n x_i\)</span></td>
<td align="center"><span class="math inline">\(Poisson(n\lambda)\)</span></td>
</tr>
<tr class="even">
<td align="center">Negative Binomial</td>
<td align="center">p</td>
<td align="center"><span class="math inline">\(\sum_{i=1}^n x_i\)</span></td>
<td align="center"></td>
</tr>
<tr class="odd">
<td align="center">Exponential</td>
<td align="center"><span class="math inline">\(\lambda\)</span></td>
<td align="center"><span class="math inline">\(\sum_{i=1}^n x_i\)</span></td>
<td align="center"><span class="math inline">\(Gamma(n, \lambda)\)</span></td>
</tr>
<tr class="even">
<td align="center">Normal (known <span class="math inline">\(\sigma^2\)</span>)</td>
<td align="center"><span class="math inline">\(\mu\)</span></td>
<td align="center"><span class="math inline">\(\frac{1}{n}\sum_{i=1}^n x_i\)</span></td>
<td align="center"><span class="math inline">\(Normal(\mu, \sigma^2/n)\)</span></td>
</tr>
<tr class="odd">
<td align="center">Normal</td>
<td align="center"><span class="math inline">\(\mu\)</span>, <span class="math inline">\(\sigma^2\)</span></td>
<td align="center"><span class="math inline">\((\sum_{i=1}^n x_i, \sum_{i=1}^n x_i^2)\)</span></td>
<td align="center"></td>
</tr>
<tr class="even">
<td align="center">Chi-Squared</td>
<td align="center"><span class="math inline">\(\nu\)</span></td>
<td align="center"><span class="math inline">\(\sum_{i=1}^n \log(x_i)\)</span></td>
<td align="center"></td>
</tr>
<tr class="odd">
<td align="center">Pareto (known min <span class="math inline">\(x_m\)</span>)</td>
<td align="center"><span class="math inline">\(\alpha\)</span></td>
<td align="center"><span class="math inline">\(\sum_{i=1}^n \log(x_i)\)</span></td>
<td align="center"></td>
</tr>
<tr class="even">
<td align="center">Gamma</td>
<td align="center"><span class="math inline">\(\alpha, \beta\)</span></td>
<td align="center"><span class="math inline">\((\sum_{i=1}^n\log(x_i), \sum_{i=1}^n x_i)\)</span></td>
<td align="center"></td>
</tr>
<tr class="odd">
<td align="center">Beta</td>
<td align="center"><span class="math inline">\(\alpha, \beta\)</span></td>
<td align="center"><span class="math inline">\((\sum_{i=1}^n\log(x_i), \sum_{i=1}^n\log(1 - x_i))\)</span></td>
<td align="center"></td>
</tr>
<tr class="even">
<td align="center">Weibull (known shape <span class="math inline">\(k\)</span>)</td>
<td align="center"><span class="math inline">\(\lambda\)</span></td>
<td align="center"><span class="math inline">\(\sum_{i=1}^n x^k\)</span></td>
<td align="center"></td>
</tr>
</tbody>
</table>
</div>
</div>
<div id="minimal-sufficiency" class="section level2 hasAnchor" number="7.2">
<h2><span class="header-section-number">7.2</span> Minimal Sufficiency<a href="statistics.html#minimal-sufficiency" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In any setting there are many sufficient statistics. However, we should aim at dealing with the statistic that summarizes the data as concisely as possible. Let <span class="math inline">\(S\)</span> be any a sufficient statistic for <span class="math inline">\(\theta\)</span>. In principle, <span class="math inline">\(W=(S,T)\)</span> is also a sufficient statistic, but we rather deal with <span class="math inline">\(S\)</span> reduces the data to one-dimension. When no further reduction from a sufficient statistic is possible, then that statistic is <strong>minimal sufficient</strong>.</p>
<div class="definition">
<p><span id="def:unlabeled-div-62" class="definition"><strong>Definition 7.3  (Minimal Sufficient Statistic) </strong></span>If <span class="math inline">\(T\)</span> is sufficient for <span class="math inline">\(\theta\)</span>, then it is a <strong>minimal sufficient statistic</strong> (MSS) if for any other sufficient statistic <span class="math inline">\(T^*, T\)</span> is a function of <span class="math inline">\(T^*\)</span>.</p>
</div>
<p><strong>Remark:</strong> Of all sufficient statistics, a minimal sufficient statistic offers the maximal reduction of the data.</p>
<p>Some “intuition” behind this definition goes as follows. A minimal sufficient statistic <span class="math inline">\(T(X)\)</span> creates a partition of the sample space, <span class="math inline">\(\Omega\)</span> into sets <span class="math inline">\(A_t\)</span>, where <span class="math inline">\(t \in \mathcal(T) = \{t : t = T(x) \text{ for some } x\in \Omega\}\)</span>. Now consider another sufficient statistic <span class="math inline">\(T^*(X)\)</span> that creates another partition of the sample space such that <span class="math inline">\(A^*_s = \{x: T^*(x)=s\}\)</span>. Then for ever <span class="math inline">\(s\)</span> there is a <span class="math inline">\(t\)</span> such that <span class="math inline">\(A^*_s \subset A_t\)</span>. Thus the partition associated with the minimal sufficient statistic is the coarsest possible partition for a sufficient statistic.</p>
<p>As before, the definition for MSS is conceptually useful but it does not help us find a MSS, or how to prove a statistic is a MSS. For this task, we invoke Casella-Berger’s Theorem 6.2.13.</p>
<div class="theorem">
<p><span id="thm:unlabeled-div-63" class="theorem"><strong>Theorem 7.4  (Casella-Berger's Theorem 6.2.13) </strong></span>Let <span class="math inline">\(X \sim f(x|\theta)\)</span>. Suppose the exists a statistic <span class="math inline">\(T=T(X)\)</span> such that for every <span class="math inline">\(x,y\)</span> in the support of <span class="math inline">\(X\)</span></p>
<p><span class="math display">\[(*) \frac{f(x|\theta)}{f(y|\theta)} = g(x,y) \Longleftrightarrow T(x)=T(y)\]</span>
then <span class="math inline">\(T\)</span> is a MSS.</p>
</div>
<p>In practice this theorem is used both to find a MSS and to prove that a given statistic is a MSS. Let us see some examples of applications of this theorem.</p>
<div class="example">
<p><span id="exm:unlabeled-div-64" class="example"><strong>Example 7.5  </strong></span>Let <span class="math inline">\(\textbf{X} = (X_1,..., X_n), X_i\)</span> iid <span class="math inline">\(N(\mu, \sigma^2), \theta = (\mu, \sigma^2)\)</span>. Consider <span class="math inline">\(T=(\bar{X}, S^2)\)</span> where <span class="math inline">\(S^2\)</span> is the sample variance. Let <span class="math inline">\(\textbf{x} = (x_1, ..., x_n), \textbf{y} = (y_1, ..., y_n)\)</span>. Then,</p>
<p><span class="math display">\[\frac{f(\textbf{x}|\theta)}{f(\textbf{y}|\theta)} = ... = exp\{-\frac{1}{2\sigma^2}[n(\bar{x}-\bar{y})^2 + 2 n\mu(\bar{x}-\bar{y}) -(n-1)(s^2_x - s^2_y)]\}.\]</span>
<span class="math inline">\(\Rightarrow\)</span> Suppose the ratio is independent of <span class="math inline">\(\theta = (\mu, \sigma^2)\)</span>. Then we must have that <span class="math inline">\(\bar{x} = \bar{y}, s^2_x = s^2_y\)</span>.</p>
<p><span class="math inline">\(\Leftarrow\)</span> Suppose that <span class="math inline">\(T(x)=T(y)\)</span>. Then <span class="math inline">\(\bar{x} = \bar{y}, s^2_x = s^2_y\)</span> and the ratio is 1.</p>
<p>By the previous theorem we have that <span class="math inline">\(T\)</span> is a MSS.</p>
</div>
<div class="example">
<p><span id="exm:unlabeled-div-65" class="example"><strong>Example 7.6  </strong></span>Let <span class="math inline">\(\textbf{X} = (X_1,..., X_n), X_i\)</span> iid <span class="math inline">\(U(\theta -1, \theta + 1)\)</span>.
<span class="math display">\[f(\textbf{x}|\theta) = \frac{1}{2^n} I(X_{(1)} &gt; \theta -1) I(X_{(n)} &lt; \theta + 1)\]</span>
Then,</p>
<p><span class="math display">\[\frac{f(\textbf{x}|\theta)}{f(\textbf{y}|\theta)} = \frac{I(x_{(1)} &gt; \theta -1) I(x_{(n)} &lt; \theta + 1)}{I(y_{(1)} &gt; \theta -1) I(y_{(n)} &lt; \theta + 1)}\]</span>
and the ratio is independent of <span class="math inline">\(\theta\)</span> if and only if <span class="math inline">\((x_{(1)}, x_{(n)}) = (y_{(1)}, y_{(n)})\)</span>. Thus <span class="math inline">\(T = (X_{(1)}, X_{(n)})\)</span> is MSS.</p>
</div>
<p>Note that in the previous example, <span class="math inline">\(dim(T(X))&gt;dim(\theta)\)</span>. This means that there is no estimator of <span class="math inline">\(\theta\)</span> that is sufficient.</p>
<div class="theorem">
<p><span id="thm:unlabeled-div-66" class="theorem"><strong>Theorem 7.5  </strong></span>If <span class="math inline">\(X_1,..., X_n (n\ge 1)\)</span> are iid with <span class="math inline">\(X_i \sim k\)</span>-parameter exponential family, then <span class="math inline">\(T(X) = \Big(\sum_{i=1}^n t_1(x), ..., \sum_{i=1}^n t_k(x)\Big)\)</span> is a MSS.</p>
</div>
<p><strong>Fact</strong>: Like sufficient statistics, MSSs are not unique. Any 1-1 function of a MSS is also a MSS.</p>
</div>
<div id="ancillary-stats" class="section level2 hasAnchor" number="7.3">
<h2><span class="header-section-number">7.3</span> Ancillary Statistics<a href="statistics.html#ancillary-stats" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Sufficiency describes where all the information in the data is contained. Ancillarity is the dual of sufficiency, describing where there is no information.</p>
<div class="definition">
<p><span id="def:unlabeled-div-67" class="definition"><strong>Definition 7.4  (Ancillary Statistic) </strong></span>Let <span class="math inline">\(X \sim f(x|\theta)\)</span>. The statistic <span class="math inline">\(S(X)\)</span> is <strong>ancillary</strong> for <span class="math inline">\(\theta\)</span> if <span class="math inline">\(\mathcal{L}(S), g(s|\theta)\)</span> are independent of <span class="math inline">\(\theta\)</span>, i.e. for any <span class="math inline">\(\theta_1, \theta_2 \in \Theta\)</span>,
<span class="math display">\[g(s|\theta_1)=g(s|\theta_2) \text{ for all } s.\]</span></p>
</div>
<div class="example">
<p><span id="exm:unlabeled-div-68" class="example"><strong>Example 7.7  </strong></span>Let <span class="math inline">\(\textbf{X} = (X_1,..., X_n), X_i\)</span> iid <span class="math inline">\(U(\theta, \theta + 1)\)</span>. Consider the order statistics of the sample <span class="math inline">\(\textbf{X}\)</span>. The range statistic <span class="math inline">\(R = X_{(n)} - X_{(1)}\)</span> is ancillary for <span class="math inline">\(\theta\)</span> by showing that the pdf of <span class="math inline">\(R\)</span> is independent of <span class="math inline">\(\theta\)</span>. Intuitive, the range does not tell anything about the location of <span class="math inline">\(\theta\)</span> in the real line. In this case the ancillarity of <span class="math inline">\(R\)</span> does not depend on the uniformity of the observations, but on the parameter of the distribution being a location parameter.</p>
</div>
<div class="theorem">
<p><span id="thm:unlabeled-div-69" class="theorem"><strong>Theorem 7.6  (Location family ancillary statistic) </strong></span>Let <span class="math inline">\(\textbf{X} = (X_1,..., X_n), X_i\)</span> iid <span class="math inline">\(f_{\mathcal{L}}(x|\theta)\)</span>, a <a href="#location-scale">location family</a>, then <span class="math inline">\(R = X_{(n)} - X_{(1)}\)</span> is ancillary for <span class="math inline">\(\theta\)</span>.</p>
</div>
<div class="theorem">
<p><span id="thm:unlabeled-div-70" class="theorem"><strong>Theorem 7.7  (Scale family ancillary statistic) </strong></span>Let <span class="math inline">\(\textbf{X} = (X_1,..., X_n), X_i\)</span> iid <span class="math inline">\(f_{\mathcal{S}}(x|\sigma)\)</span>, a <a href="#location-scale">scale family</a>, and let <span class="math inline">\(S_j = X_j/X_n\)</span>. Then <span class="math inline">\(S=(S_1, S_2,..., S_n)\)</span> is ancillary for <span class="math inline">\(\sigma\)</span>.</p>
</div>
<p><strong>Note:</strong> Since <span class="math inline">\(S\)</span> is ancillary, any function of <span class="math inline">\(S=(S_1, S_2,..., S_n)\)</span> is also ancillary. For example, <span class="math inline">\(S_1 + ... + S_n\)</span> is also ancillary.</p>
<div class="theorem">
<p><span id="thm:unlabeled-div-71" class="theorem"><strong>Theorem 7.8  (Location-Scale family ancillary statistic) </strong></span>Let <span class="math inline">\(\textbf{X} = (X_1,..., X_n), X_i\)</span> iid <span class="math inline">\(f_{\mathcal{L,S}}(x|\mu,\sigma)\)</span>, a <a href="#location-scale">location-scale family</a> where <span class="math inline">\(\theta = (\mu, \sigma)\)</span>. Let <span class="math inline">\(T_1= T_1(\textbf{X})\)</span> and <span class="math inline">\(T_2= T_2(\textbf{X})\)</span> be any two statistics such that</p>
<p><span class="math display">\[ (*) \quad T_j(aX_1 + b, ..., aX_n+b) = aT_j(X_1,..., X_n) \quad j = 1,2.\]</span></p>
<p>Then, <span class="math inline">\(T_1/T_2\)</span> is ancillary for <span class="math inline">\(\theta\)</span></p>
</div>
<div class="example">
<p><span id="exm:unlabeled-div-72" class="example"><strong>Example 7.8  </strong></span>Consider the range <span class="math inline">\(R = X_{(n)} - X_{(1)}\)</span> and <span class="math inline">\(S = \sqrt{\frac{1}{n-1}\sum(X_1 -\bar{X})^2}\)</span> satisfy (*) so <span class="math inline">\(R/S\)</span> is ancillary for <span class="math inline">\(\theta = (\mu, \sigma)\)</span>.</p>
</div>
<div id="why-are-we-interested-in-ancillary-statistics" class="section level3 hasAnchor" number="7.3.1">
<h3><span class="header-section-number">7.3.1</span> Why are we interested in ancillary statistics?<a href="statistics.html#why-are-we-interested-in-ancillary-statistics" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>There is some relationship between ancillarity and minimal sufficiency. Suppose there is a statistic <span class="math inline">\(c=C(\textbf{X})\)</span> and an ancillary statistic <span class="math inline">\(S=S(\textbf{X})\)</span> such that <span class="math inline">\(T=(S,C)\)</span> is minimal sufficient. The <strong>ancillarity principle</strong> states that inference on the parameter should be based on the conditional distribution of <span class="math inline">\(C\)</span> given the ancillary statistic <span class="math inline">\(S\)</span>. Furthermore, we will see later that ancillary statistics are independent of complete and sufficient statistics.</p>
<p><strong>Remark:</strong> Recall the <span class="math inline">\(U(\theta, \theta + 1)\)</span> example in which we found that the MSS, T, is two dimensional, and therefore no sufficient estimator for <span class="math inline">\(\theta\)</span> exists. We can write <span class="math inline">\(T = (S,C)\)</span> where <span class="math inline">\(C\)</span> has a marginal distribution that is ancillary (independent of the parameter), and then <span class="math inline">\(S\)</span> is conditionally sufficient, i.e. sufficient conditional on <span class="math inline">\(C\)</span>.</p>
<div class="example">
<p><span id="exm:unlabeled-div-73" class="example"><strong>Example 7.9  </strong></span>Let <span class="math inline">\(N\)</span> be a random variable with known distribution, <span class="math inline">\(P(N=n) = p_n\)</span>, and let <span class="math inline">\(X_1, ..., X_N\)</span> be iid with exponential family density. Then the likelihood of the data, <span class="math inline">\((N, X_1,..., X_N)\)</span> is</p>
<p><span class="math display">\[p_n (\prod_{i=1}^{n}h(x_i))c(\theta)^{n} exp\bigg\{\sum_{j=1}^{k} w_k(\theta) \sum_{i=1}^{n}t_j(x_i)\bigg\} \]</span>
and thus <span class="math inline">\(\bigg\{N, \{\sum_{i=1}^{N}t_j(x_i)\}_{j=1}^{k}\bigg\}\)</span> is sufficient for <span class="math inline">\(\theta\)</span>. Observe that <span class="math inline">\(N\)</span> is ancillary (independent of <span class="math inline">\(\theta\)</span>) and <span class="math inline">\(\{\sum_{i=1}^{N}t_j(x_i)\}_{j=1}^{k}\)</span> is sufficient for <span class="math inline">\(\theta\)</span> conditional on <span class="math inline">\(N\)</span>.</p>
</div>
<!-- Pitman-Koopman-Darmois Theorem -->

</div>
</div>
</div>
<h3>References<a href="references.html#references" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-Casella1990" class="csl-entry">
Casella, G C, and Roger L Berger. 1990. <em>Statistical Inference</em>. 2nd ed. The Wadsworth &amp; Brooks/Cole Statistics/Probability Series. Florence, KY: Brooks/Cole.
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="moments.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="point-estimators-finite-samples.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown-demo/edit/master/07-statistics.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["Solving-Statistical-Problems.pdf", "Solving-Statistical-Problems.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
