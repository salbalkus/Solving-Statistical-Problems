<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 3 Probability | Solving Statistical Problems</title>
  <meta name="description" content="Chapter 3 Probability | Solving Statistical Problems" />
  <meta name="generator" content="bookdown 0.34 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 3 Probability | Solving Statistical Problems" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="Chapter 3 Probability | Solving Statistical Problems" />
  <meta name="github-repo" content="salbalkus/Solving-Statistical-Problems" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 3 Probability | Solving Statistical Problems" />
  
  <meta name="twitter:description" content="Chapter 3 Probability | Solving Statistical Problems" />
  

<meta name="author" content="Salvador Balkus, Kimberly Greco, and Mónica Robles Fontán" />


<meta name="date" content="2023-08-04" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="math-tricks.html"/>
<link rel="next" href="known-distributions.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>
<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Solving Statistical Problems</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Introduction</a></li>
<li class="chapter" data-level="2" data-path="math-tricks.html"><a href="math-tricks.html"><i class="fa fa-check"></i><b>2</b> Math Tricks</a>
<ul>
<li class="chapter" data-level="2.1" data-path="math-tricks.html"><a href="math-tricks.html#combinatorics"><i class="fa fa-check"></i><b>2.1</b> Combinatorics</a></li>
<li class="chapter" data-level="2.2" data-path="math-tricks.html"><a href="math-tricks.html#binomial-and-multinomial-theorems"><i class="fa fa-check"></i><b>2.2</b> Binomial and Multinomial Theorems</a></li>
<li class="chapter" data-level="2.3" data-path="math-tricks.html"><a href="math-tricks.html#geometric-series"><i class="fa fa-check"></i><b>2.3</b> Geometric Series</a></li>
<li class="chapter" data-level="2.4" data-path="math-tricks.html"><a href="math-tricks.html#exponential-taylor"><i class="fa fa-check"></i><b>2.4</b> Taylor Series for Exponential Function</a></li>
<li class="chapter" data-level="2.5" data-path="math-tricks.html"><a href="math-tricks.html#taylors-formula"><i class="fa fa-check"></i><b>2.5</b> Taylor’s Formula</a></li>
<li class="chapter" data-level="2.6" data-path="math-tricks.html"><a href="math-tricks.html#exponential-limit"><i class="fa fa-check"></i><b>2.6</b> Exponential Limit</a></li>
<li class="chapter" data-level="2.7" data-path="math-tricks.html"><a href="math-tricks.html#ibp"><i class="fa fa-check"></i><b>2.7</b> Integration by Parts</a></li>
<li class="chapter" data-level="2.8" data-path="math-tricks.html"><a href="math-tricks.html#leibniz-rule"><i class="fa fa-check"></i><b>2.8</b> Leibniz’s Rule</a></li>
<li class="chapter" data-level="2.9" data-path="math-tricks.html"><a href="math-tricks.html#fubinis-theorem"><i class="fa fa-check"></i><b>2.9</b> Fubini’s Theorem</a></li>
<li class="chapter" data-level="2.10" data-path="math-tricks.html"><a href="math-tricks.html#gamma-function"><i class="fa fa-check"></i><b>2.10</b> Gamma Function</a></li>
<li class="chapter" data-level="2.11" data-path="math-tricks.html"><a href="math-tricks.html#triangle-inequality"><i class="fa fa-check"></i><b>2.11</b> Triangle Inequality</a></li>
<li class="chapter" data-level="2.12" data-path="math-tricks.html"><a href="math-tricks.html#constrained-optimization"><i class="fa fa-check"></i><b>2.12</b> Constrained Optimization</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="probability.html"><a href="probability.html"><i class="fa fa-check"></i><b>3</b> Probability</a>
<ul>
<li class="chapter" data-level="3.1" data-path="probability.html"><a href="probability.html#basic-axioms"><i class="fa fa-check"></i><b>3.1</b> Basic Axioms</a></li>
<li class="chapter" data-level="3.2" data-path="probability.html"><a href="probability.html#basic-probability-solving-techniques"><i class="fa fa-check"></i><b>3.2</b> Basic Probability Solving Techniques</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="probability.html"><a href="probability.html#disjointify"><i class="fa fa-check"></i><b>3.2.1</b> Disjointification</a></li>
<li class="chapter" data-level="3.2.2" data-path="probability.html"><a href="probability.html#demorgan"><i class="fa fa-check"></i><b>3.2.2</b> DeMorgan’s Laws</a></li>
<li class="chapter" data-level="3.2.3" data-path="probability.html"><a href="probability.html#proving-inequalities-subsetting"><i class="fa fa-check"></i><b>3.2.3</b> Proving Inequalities: Subsetting</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="probability.html"><a href="probability.html#conditional-probability"><i class="fa fa-check"></i><b>3.3</b> Conditional Probability</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="probability.html"><a href="probability.html#conditional-probability-in-practice"><i class="fa fa-check"></i><b>3.3.1</b> Conditional Probability in Practice</a></li>
<li class="chapter" data-level="7.1.2" data-path="statistics.html"><a href="statistics.html"><i class="fa fa-check"></i><b>7.1.2</b> Exponential Family Sufficient Statistics</a></li>
<li class="chapter" data-level="7.1.3" data-path="statistics.html"><a href="statistics.html#a-note-on-distributions-of-sufficient-statistics"><i class="fa fa-check"></i><b>7.1.3</b> A Note on Distributions of Sufficient Statistics</a></li>
<li class="chapter" data-level="7.1.4" data-path="statistics.html"><a href="statistics.html#moments-of-the-sufficient-statistic"><i class="fa fa-check"></i><b>7.1.4</b> Moments of the Sufficient Statistic</a></li>
<li class="chapter" data-level="7.1.5" data-path="statistics.html"><a href="statistics.html#table-ss"><i class="fa fa-check"></i><b>7.1.5</b> Table of Sufficient Statistics</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="statistics.html"><a href="statistics.html#minimal-sufficiency"><i class="fa fa-check"></i><b>7.2</b> Minimal Sufficiency</a></li>
<li class="chapter" data-level="7.3" data-path="statistics.html"><a href="statistics.html#ancillary-stats"><i class="fa fa-check"></i><b>7.3</b> Ancillary Statistics</a>
<ul>
<li class="chapter" data-level="7.3.1" data-path="statistics.html"><a href="statistics.html#why-are-we-interested-in-ancillary-statistics"><i class="fa fa-check"></i><b>7.3.1</b> Why are we interested in ancillary statistics?</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="statistics.html"><a href="statistics.html#complete-stats"><i class="fa fa-check"></i><b>7.4</b> Complete Statistics</a>
<ul>
<li class="chapter" data-level="7.4.1" data-path="statistics.html"><a href="statistics.html#techniques-for-finding-css"><i class="fa fa-check"></i><b>7.4.1</b> Techniques for Finding CSS</a></li>
<li class="chapter" data-level="7.4.2" data-path="statistics.html"><a href="statistics.html#basus-theorem"><i class="fa fa-check"></i><b>7.4.2</b> Basu’s Theorem</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="point-estimators-finite-samples.html"><a href="point-estimators-finite-samples.html"><i class="fa fa-check"></i><b>8</b> Point Estimators: Finite Samples</a>
<ul>
<li class="chapter" data-level="8.1" data-path="point-estimators-finite-samples.html"><a href="point-estimators-finite-samples.html#identifiability"><i class="fa fa-check"></i><b>8.1</b> Identifiability</a></li>
<li class="chapter" data-level="8.2" data-path="point-estimators-finite-samples.html"><a href="point-estimators-finite-samples.html#finding-estimators"><i class="fa fa-check"></i><b>8.2</b> Finding estimators</a>
<ul>
<li class="chapter" data-level="8.2.1" data-path="point-estimators-finite-samples.html"><a href="point-estimators-finite-samples.html#method-of-moments"><i class="fa fa-check"></i><b>8.2.1</b> Method of Moments</a></li>
<li class="chapter" data-level="8.2.2" data-path="point-estimators-finite-samples.html"><a href="point-estimators-finite-samples.html#maximum-likelihood-estimation"><i class="fa fa-check"></i><b>8.2.2</b> Maximum Likelihood Estimation</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="point-estimators-finite-samples.html"><a href="point-estimators-finite-samples.html#properties-of-estimators"><i class="fa fa-check"></i><b>8.3</b> Properties of Estimators</a></li>
<li class="chapter" data-level="8.4" data-path="point-estimators-finite-samples.html"><a href="point-estimators-finite-samples.html#uniform-minimum-variance-unbiased-estimators-umvues"><i class="fa fa-check"></i><b>8.4</b> Uniform Minimum Variance Unbiased Estimators (UMVUEs)</a>
<ul>
<li class="chapter" data-level="8.4.1" data-path="point-estimators-finite-samples.html"><a href="point-estimators-finite-samples.html#finding-umvues-i-cramér-rao-bound"><i class="fa fa-check"></i><b>8.4.1</b> Finding UMVUEs I: Cramér-Rao Bound</a></li>
<li class="chapter" data-level="8.4.2" data-path="point-estimators-finite-samples.html"><a href="point-estimators-finite-samples.html#rao-blackwell-and-lehmann-scheffé-theorem"><i class="fa fa-check"></i><b>8.4.2</b> Rao-Blackwell and Lehmann-Scheffé Theorem</a></li>
<li class="chapter" data-level="8.4.3" data-path="point-estimators-finite-samples.html"><a href="point-estimators-finite-samples.html#finding-umvues-ii-using-the-lehmann-scheffé-theorem"><i class="fa fa-check"></i><b>8.4.3</b> Finding UMVUEs II: Using the Lehmann-Scheffé Theorem</a></li>
<li class="chapter" data-level="8.4.4" data-path="point-estimators-finite-samples.html"><a href="point-estimators-finite-samples.html#finding-umvues-iii-lehmann-scheffé-corollary"><i class="fa fa-check"></i><b>8.4.4</b> Finding UMVUEs III: Lehmann-Scheffé Corollary</a></li>
<li class="chapter" data-level="8.4.5" data-path="point-estimators-finite-samples.html"><a href="point-estimators-finite-samples.html#proving-an-umvue-does-not-exist"><i class="fa fa-check"></i><b>8.4.5</b> Proving an UMVUE Does Not Exist</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="point-estimators-finite-samples.html"><a href="point-estimators-finite-samples.html#inferential-properties-of-exponential-families-distributions"><i class="fa fa-check"></i><b>8.5</b> Inferential Properties of Exponential Families Distributions</a>
<ul>
<li class="chapter" data-level="8.5.1" data-path="point-estimators-finite-samples.html"><a href="point-estimators-finite-samples.html#bernoulli-1"><i class="fa fa-check"></i><b>8.5.1</b> Bernoulli</a></li>
<li class="chapter" data-level="8.5.2" data-path="point-estimators-finite-samples.html"><a href="point-estimators-finite-samples.html#binomial-1"><i class="fa fa-check"></i><b>8.5.2</b> Binomial</a></li>
<li class="chapter" data-level="8.5.3" data-path="point-estimators-finite-samples.html"><a href="point-estimators-finite-samples.html#geometric"><i class="fa fa-check"></i><b>8.5.3</b> Geometric</a></li>
<li class="chapter" data-level="8.5.4" data-path="point-estimators-finite-samples.html"><a href="point-estimators-finite-samples.html#negative-binomial-1"><i class="fa fa-check"></i><b>8.5.4</b> Negative Binomial</a></li>
<li class="chapter" data-level="8.5.5" data-path="point-estimators-finite-samples.html"><a href="point-estimators-finite-samples.html#poisson-1"><i class="fa fa-check"></i><b>8.5.5</b> Poisson</a></li>
<li class="chapter" data-level="8.5.6" data-path="point-estimators-finite-samples.html"><a href="point-estimators-finite-samples.html#normal-1"><i class="fa fa-check"></i><b>8.5.6</b> Normal</a></li>
<li class="chapter" data-level="8.5.7" data-path="point-estimators-finite-samples.html"><a href="point-estimators-finite-samples.html#exponential-1"><i class="fa fa-check"></i><b>8.5.7</b> Exponential</a></li>
<li class="chapter" data-level="8.5.8" data-path="point-estimators-finite-samples.html"><a href="point-estimators-finite-samples.html#gamma-1"><i class="fa fa-check"></i><b>8.5.8</b> Gamma</a></li>
<li class="chapter" data-level="8.5.9" data-path="point-estimators-finite-samples.html"><a href="point-estimators-finite-samples.html#pareto-1"><i class="fa fa-check"></i><b>8.5.9</b> Pareto</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="point-estimators-asymptotics.html"><a href="point-estimators-asymptotics.html"><i class="fa fa-check"></i><b>9</b> Point Estimators: Asymptotics</a>
<ul>
<li class="chapter" data-level="9.1" data-path="point-estimators-asymptotics.html"><a href="point-estimators-asymptotics.html#consistency"><i class="fa fa-check"></i><b>9.1</b> Consistency</a>
<ul>
<li class="chapter" data-level="9.1.1" data-path="point-estimators-asymptotics.html"><a href="point-estimators-asymptotics.html#technique-weak-law-of-large-numbers"><i class="fa fa-check"></i><b>9.1.1</b> Technique: Weak Law of Large Numbers</a></li>
<li class="chapter" data-level="9.1.2" data-path="point-estimators-asymptotics.html"><a href="point-estimators-asymptotics.html#technique-direct-proof-via-convergence-in-probability"><i class="fa fa-check"></i><b>9.1.2</b> Technique: Direct Proof via Convergence in Probability</a></li>
<li class="chapter" data-level="9.1.3" data-path="point-estimators-asymptotics.html"><a href="point-estimators-asymptotics.html#technique-continuous-mapping-theorem."><i class="fa fa-check"></i><b>9.1.3</b> Technique: Continuous Mapping Theorem.</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="point-estimators-asymptotics.html"><a href="point-estimators-asymptotics.html#asymptotic-efficiency"><i class="fa fa-check"></i><b>9.2</b> Asymptotic Efficiency</a>
<ul>
<li class="chapter" data-level="9.2.1" data-path="point-estimators-asymptotics.html"><a href="point-estimators-asymptotics.html#central-limit-theorems"><i class="fa fa-check"></i><b>9.2.1</b> Central Limit Theorems</a></li>
<li class="chapter" data-level="9.2.2" data-path="point-estimators-asymptotics.html"><a href="point-estimators-asymptotics.html#the-delta-method"><i class="fa fa-check"></i><b>9.2.2</b> The Delta Method</a></li>
<li class="chapter" data-level="9.2.3" data-path="point-estimators-asymptotics.html"><a href="point-estimators-asymptotics.html#cramer-wold-device"><i class="fa fa-check"></i><b>9.2.3</b> Cramer-Wold Device</a></li>
<li class="chapter" data-level="9.2.4" data-path="point-estimators-asymptotics.html"><a href="point-estimators-asymptotics.html#asymptotic-distribution-in-practice"><i class="fa fa-check"></i><b>9.2.4</b> Asymptotic Distribution in Practice</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="point-estimators-asymptotics.html"><a href="point-estimators-asymptotics.html#asymptotic-properties-of-mles"><i class="fa fa-check"></i><b>9.3</b> Asymptotic Properties of MLEs</a></li>
<li class="chapter" data-level="9.4" data-path="point-estimators-asymptotics.html"><a href="point-estimators-asymptotics.html#variance-stabilizing-transformations"><i class="fa fa-check"></i><b>9.4</b> Variance Stabilizing Transformations</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="hypothesis-tests-finite-samples.html"><a href="hypothesis-tests-finite-samples.html"><i class="fa fa-check"></i><b>10</b> Hypothesis Tests: Finite Samples</a>
<ul>
<li class="chapter" data-level="10.1" data-path="hypothesis-tests-finite-samples.html"><a href="hypothesis-tests-finite-samples.html#constructing-a-test"><i class="fa fa-check"></i><b>10.1</b> Constructing a Test</a></li>
<li class="chapter" data-level="10.2" data-path="hypothesis-tests-finite-samples.html"><a href="hypothesis-tests-finite-samples.html#lrt"><i class="fa fa-check"></i><b>10.2</b> Likelihood Ratio Tests</a></li>
<li class="chapter" data-level="10.3" data-path="hypothesis-tests-finite-samples.html"><a href="hypothesis-tests-finite-samples.html#power"><i class="fa fa-check"></i><b>10.3</b> Power</a></li>
<li class="chapter" data-level="10.4" data-path="hypothesis-tests-finite-samples.html"><a href="hypothesis-tests-finite-samples.html#how-to-find-optimal-tests"><i class="fa fa-check"></i><b>10.4</b> How to Find Optimal Tests</a>
<ul>
<li class="chapter" data-level="10.4.1" data-path="hypothesis-tests-finite-samples.html"><a href="hypothesis-tests-finite-samples.html#properties-1"><i class="fa fa-check"></i><b>10.4.1</b> Properties</a></li>
<li class="chapter" data-level="10.4.2" data-path="hypothesis-tests-finite-samples.html"><a href="hypothesis-tests-finite-samples.html#optimality-for-simple-hypotheses-the-neyman-pearson-lemma"><i class="fa fa-check"></i><b>10.4.2</b> Optimality for Simple Hypotheses: The Neyman-Pearson Lemma</a></li>
<li class="chapter" data-level="10.4.3" data-path="hypothesis-tests-finite-samples.html"><a href="hypothesis-tests-finite-samples.html#optimality-for-one-sided-hypotheses-the-karlin-rubin-theorem"><i class="fa fa-check"></i><b>10.4.3</b> Optimality for One-Sided Hypotheses: The Karlin-Rubin Theorem</a></li>
<li class="chapter" data-level="10.4.4" data-path="hypothesis-tests-finite-samples.html"><a href="hypothesis-tests-finite-samples.html#optimality-for-two-sided-hypotheses."><i class="fa fa-check"></i><b>10.4.4</b> Optimality for Two-Sided Hypotheses.</a></li>
</ul></li>
<li class="chapter" data-level="10.5" data-path="hypothesis-tests-finite-samples.html"><a href="hypothesis-tests-finite-samples.html#nuisance-parameters"><i class="fa fa-check"></i><b>10.5</b> Nuisance Parameters</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="hypothesis-tests-asymptotics.html"><a href="hypothesis-tests-asymptotics.html"><i class="fa fa-check"></i><b>11</b> Hypothesis Tests: Asymptotics</a>
<ul>
<li class="chapter" data-level="11.1" data-path="hypothesis-tests-asymptotics.html"><a href="hypothesis-tests-asymptotics.html#wald-test"><i class="fa fa-check"></i><b>11.1</b> Wald Test</a></li>
<li class="chapter" data-level="11.2" data-path="hypothesis-tests-asymptotics.html"><a href="hypothesis-tests-asymptotics.html#score-test"><i class="fa fa-check"></i><b>11.2</b> Score Test</a></li>
<li class="chapter" data-level="11.3" data-path="hypothesis-tests-asymptotics.html"><a href="hypothesis-tests-asymptotics.html#likelihood-ratio-test"><i class="fa fa-check"></i><b>11.3</b> Likelihood Ratio Test</a></li>
<li class="chapter" data-level="11.4" data-path="hypothesis-tests-asymptotics.html"><a href="hypothesis-tests-asymptotics.html#composite-null-hypotheses"><i class="fa fa-check"></i><b>11.4</b> Composite Null Hypotheses</a>
<ul>
<li class="chapter" data-level="11.4.1" data-path="hypothesis-tests-asymptotics.html"><a href="hypothesis-tests-asymptotics.html#multiple-parameters"><i class="fa fa-check"></i><b>11.4.1</b> Multiple Parameters</a></li>
<li class="chapter" data-level="11.4.2" data-path="hypothesis-tests-asymptotics.html"><a href="hypothesis-tests-asymptotics.html#nuisance-parameters-1"><i class="fa fa-check"></i><b>11.4.2</b> Nuisance Parameters</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="12" data-path="confidence-intervals.html"><a href="confidence-intervals.html"><i class="fa fa-check"></i><b>12</b> Confidence Intervals</a>
<ul>
<li class="chapter" data-level="12.1" data-path="confidence-intervals.html"><a href="confidence-intervals.html#definition-1"><i class="fa fa-check"></i><b>12.1</b> Definition</a></li>
<li class="chapter" data-level="12.2" data-path="confidence-intervals.html"><a href="confidence-intervals.html#test-inversion"><i class="fa fa-check"></i><b>12.2</b> When You’ve Constructed a Hypothesis Test…</a></li>
<li class="chapter" data-level="12.3" data-path="confidence-intervals.html"><a href="confidence-intervals.html#when-youve-found-a-pivot-including-asymptotic-normality"><i class="fa fa-check"></i><b>12.3</b> When You’ve Found a Pivot (Including Asymptotic Normality)…</a></li>
<li class="chapter" data-level="12.4" data-path="confidence-intervals.html"><a href="confidence-intervals.html#when-all-you-have-is-a-distribution"><i class="fa fa-check"></i><b>12.4</b> When All You Have Is a Distribution…</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="random-processes.html"><a href="random-processes.html"><i class="fa fa-check"></i><b>13</b> Random Processes</a>
<ul>
<li class="chapter" data-level="13.1" data-path="random-processes.html"><a href="random-processes.html#branching-process"><i class="fa fa-check"></i><b>13.1</b> Branching Processes</a>
<ul>
<li class="chapter" data-level="13.1.1" data-path="random-processes.html"><a href="random-processes.html#random-variables-1"><i class="fa fa-check"></i><b>13.1.1</b> Random Variables</a></li>
<li class="chapter" data-level="13.1.2" data-path="random-processes.html"><a href="random-processes.html#probability-generating-function"><i class="fa fa-check"></i><b>13.1.2</b> Probability Generating Function</a></li>
<li class="chapter" data-level="13.1.3" data-path="random-processes.html"><a href="random-processes.html#finding-the-pgf-of-a-branching-process"><i class="fa fa-check"></i><b>13.1.3</b> Finding the PGF of a Branching Process</a></li>
<li class="chapter" data-level="13.1.4" data-path="random-processes.html"><a href="random-processes.html#finding-the-probability-of-extinction-criticality-theorem"><i class="fa fa-check"></i><b>13.1.4</b> Finding the Probability of Extinction: Criticality Theorem</a></li>
<li class="chapter" data-level="13.1.5" data-path="random-processes.html"><a href="random-processes.html#example"><i class="fa fa-check"></i><b>13.1.5</b> Example</a></li>
</ul></li>
<li class="chapter" data-level="13.2" data-path="random-processes.html"><a href="random-processes.html#poisson-processes"><i class="fa fa-check"></i><b>13.2</b> Poisson Processes</a>
<ul>
<li class="chapter" data-level="13.2.1" data-path="random-processes.html"><a href="random-processes.html#memorylessness-of-the-exponential"><i class="fa fa-check"></i><b>13.2.1</b> Memorylessness of the Exponential</a></li>
<li class="chapter" data-level="13.2.2" data-path="random-processes.html"><a href="random-processes.html#count-time-duality"><i class="fa fa-check"></i><b>13.2.2</b> Count-Time Duality</a></li>
<li class="chapter" data-level="13.2.3" data-path="random-processes.html"><a href="random-processes.html#poisson-distribution"><i class="fa fa-check"></i><b>13.2.3</b> Poisson Distribution</a></li>
<li class="chapter" data-level="13.2.4" data-path="random-processes.html"><a href="random-processes.html#exponential-distribution"><i class="fa fa-check"></i><b>13.2.4</b> Exponential Distribution</a></li>
<li class="chapter" data-level="13.2.5" data-path="random-processes.html"><a href="random-processes.html#example-1"><i class="fa fa-check"></i><b>13.2.5</b> Example</a></li>
<li class="chapter" data-level="13.2.6" data-path="random-processes.html"><a href="random-processes.html#merging-and-splitting"><i class="fa fa-check"></i><b>13.2.6</b> Merging and Splitting</a></li>
<li class="chapter" data-level="13.2.7" data-path="random-processes.html"><a href="random-processes.html#thinning"><i class="fa fa-check"></i><b>13.2.7</b> Thinning</a></li>
<li class="chapter" data-level="13.2.8" data-path="random-processes.html"><a href="random-processes.html#restarting"><i class="fa fa-check"></i><b>13.2.8</b> Restarting</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Solving Statistical Problems</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="probability" class="section level1 hasAnchor" number="3">
<h1><span class="header-section-number">Chapter 3</span> Probability<a href="probability.html#probability" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="basic-axioms" class="section level2 hasAnchor" number="3.1">
<h2><span class="header-section-number">3.1</span> Basic Axioms<a href="probability.html#basic-axioms" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>How do we reason about events that may or may not occur? The field of <em>probability</em> provides the mathematical foundations for reasoning under uncertainty.</p>
<p>The foundation of probability is set theory. When you think about “something that could happen”, you can conceptualize that “something” as an experiment. We can define all of the details of that experiment in terms of sets:</p>
<ul>
<li>Any given outcome of the experiment we represent as a set - call it <span class="math inline">\(A\)</span>.</li>
<li>We represent the set of all possible that could have occurred - the <em>sampling space</em> - as <span class="math inline">\(\Omega\)</span>. Hence, <span class="math inline">\(A \subseteq \Omega\)</span></li>
</ul>
<p>Now suppose we could repeat this experiment infinitely. The <em>probability</em> of an event A is defined as the proportion of experiments in which that event will occurs as the number of experiments increases towards infinity. Mathematically, we represent probability as a function: <span class="math inline">\(P(A)\)</span>. Then,</p>
<p><span class="math display">\[P(A) = \lim_{n\rightarrow\infty}\frac{\text{# of times A is drawn}}{n}\]</span></p>
<p>In the simplest case, we can compute this proportion directly as <span class="math inline">\(P(A) = \frac{\text{# of outcomes in }A}{\text{# of outcomes in }\Omega}\)</span> - a typical grade-school exercise. Furthermore, from this definition, three properties (called Kolmogorov’s axioms) arise:</p>
<div class="definition">
<p><span id="def:unlabeled-div-5" class="definition"><strong>Definition 3.1  (Kolmogorov's Axioms of Probability) </strong></span>For all probability functions <span class="math inline">\(P(\cdot)\)</span> defined on a sampling space <span class="math inline">\(\Omega\)</span>:</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(0 \leq P(A) \leq 1\)</span> for all <span class="math inline">\(A\subseteq \Omega\)</span></li>
<li><span class="math inline">\(P(\Omega) = 1\)</span>.</li>
<li>If $A_1, A_2, …, $ are pairwise disjoint, then</li>
</ol>
<p><span class="math display">\[P(\cup_{i=1}^\infty A_i) = \sum_{i=1}^\infty P(A_i)\]</span></p>
</div>
<p>Two sets <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are <em>pairwise disjoint</em> if <span class="math inline">\(A\)</span> does not share any elements in common with <span class="math inline">\(B\)</span>. This means <span class="math inline">\(A \cup B = \emptyset\)</span>.</p>
<p>Consequently, to find a probability involving multiple events, you can use any knowledge of set theory you might have in conjunction with these axioms. Here’s one example:</p>
<div class="example">
<p><span id="exm:unlabeled-div-6" class="example"><strong>Example 3.1  (Probability of a Set Complement) </strong></span><span class="math inline">\(P(A^c) = 1 - P(A)\)</span>. Why?
- Since <span class="math inline">\(A^c\)</span> and <span class="math inline">\(A\)</span> are disjoint, and naturally <span class="math inline">\(A^c \cup A = \Omega\)</span> we first note, by axiom 3, that <span class="math inline">\(P(\Omega) = P(A^c) + P(A)\)</span>.
- Then, by axiom 2, <span class="math inline">\(P(\Omega) = 1\)</span>, so <span class="math inline">\(P(A^c) + P(A) = 1\)</span>. Rearrange to complete the proof.</p>
</div>
<p>These axioms are the foundation of all probability. Learn to recognize them when working directly with probability functions <span class="math inline">\(P(\cdot)\)</span>. For instance, any time you see a sum, you should think “Aha! Kolmogorov Axiom 3!”</p>
</div>
<div id="basic-probability-solving-techniques" class="section level2 hasAnchor" number="3.2">
<h2><span class="header-section-number">3.2</span> Basic Probability Solving Techniques<a href="probability.html#basic-probability-solving-techniques" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="disjointify" class="section level3 hasAnchor" number="3.2.1">
<h3><span class="header-section-number">3.2.1</span> Disjointification<a href="probability.html#disjointify" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Kolmogorov’s Axiom 3 is by far the most useful, but to use it requires a trick called <em>disjointification</em>, or “partitioning into disjoint subsets”. In this technique, we rewrite a set into a union of disjoint subsets:</p>
<p><span class="math display">\[A = (A \cap B) \cup (A \cap B^c)\]</span></p>
<p>Then, if we know the probability of the intersections, we apply Axiom 3 to solve: <span class="math inline">\(P(A) = P(A \cap B) + P(A \cap B^c)\)</span>. Let’s prove three classic equalities by disjointification:</p>
<div class="example">
<p><span id="exm:intersection-complement" class="example"><strong>Example 3.2  (Probability of Set Intersection with Complement) </strong></span>To compute the probability of set intersection, use the following:</p>
<p><span class="math display">\[P(B \cap A^c) = P(B) - P(A \cap B)\]</span></p>
<p><em>Proof</em>:</p>
<ol style="list-style-type: decimal">
<li>Disjointify: Note <span class="math inline">\(B = (B \cap A) \cup (B \cap A^c)\)</span>. Therefore, <span class="math inline">\(P(B) = P(B \cap A^c) + P(A \cap B)\)</span> by Probability Axiom 3</li>
<li>Rearrange: <span class="math inline">\(P(B \cap A^c) = P(B) - P(A \cap B)\)</span></li>
</ol>
</div>
<div class="example">
<p><span id="exm:subset-inequality" class="example"><strong>Example 3.3  (Subset Inequality) </strong></span>Probability inequalities can be constructed using subsets:</p>
<p><span class="math display">\[A \subset B \implies P(A) \leq P(B)\]</span></p>
<p><em>Proof</em>:</p>
<ol style="list-style-type: decimal">
<li>Disjointify: If <span class="math inline">\(A \subset B\)</span>, then <span class="math inline">\(B = A \cup (A^c \cap B)\)</span>, with <span class="math inline">\(A\)</span> and <span class="math inline">\((A^c \cap B)\)</span> disjoint (since <span class="math inline">\((A^c \cap B)\)</span> is the part of <span class="math inline">\(B\)</span> not contained by <span class="math inline">\(A\)</span>).</li>
<li>Consequently, <span class="math inline">\(P(B) = P(A) + P(A^c \cap B)\)</span>.</li>
<li>By Kolmogorov Axiom 1, <span class="math inline">\(P(A^c \cap B) \geq 0\)</span> which implies that <span class="math inline">\(P(A) \leq P(B)\)</span>, completing the proof.</li>
</ol>
</div>
<p>Of course, these examples are famous properties that you can simply memorize and apply directly on an example. But, in case you forget, now you know how to derive it!</p>
</div>
<div id="demorgan" class="section level3 hasAnchor" number="3.2.2">
<h3><span class="header-section-number">3.2.2</span> DeMorgan’s Laws<a href="probability.html#demorgan" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>If you ever need to turn a union into an intersection or vice versa, first take the complement <span class="math inline">\(P(A) = 1 - P(A^c)\)</span>. Then, apply DeMorgan’s Laws from set theory:</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\((A \cup B)^c = A^c \cap B^c\)</span></li>
<li><span class="math inline">\((A \cap B)^c = A^c \cup B^c\)</span></li>
</ol>
<p>Here’s an example:</p>
<div class="example">
<p><span id="exm:unlabeled-div-7" class="example"><strong>Example 3.4  (Probability of a Set Union) </strong></span><span class="math display">\[P(A \cup B) = P(A) + P(B) - P(A \cap B)\]</span></p>
<ol style="list-style-type: decimal">
<li>Start by taking the complement: <span class="math inline">\(P(A \cup B) = 1 - P((A\cup B)^c\)</span></li>
<li>Apply DeMorgan’s Laws: <span class="math inline">\(1 - P((A\cup B)^c = 1 - P(A^c \cap B^c)\)</span>.</li>
<li>Now, we could <em>disjointify</em> <span class="math inline">\(A^c\)</span>, but a faster way is to recall the <a href="#intersection-complement">previous set intersection example</a>: <span class="math inline">\(1 - P(A^c \cap B^c) = 1 - (P(A^c) - P(A^c \cap B))\)</span></li>
<li>Revert the complement: <span class="math inline">\(1 - (P(A^c) - P(A^c \cap B)) = P(A) + P(A^c \cap B)\)</span></li>
<li>Repeat Step 3 on the other set intersection: <span class="math inline">\(P(A) + P(A^c \cap B) = P(A) + P(B) - P(A \cap B)\)</span>, completing the proof.</li>
</ol>
</div>
</div>
<div id="proving-inequalities-subsetting" class="section level3 hasAnchor" number="3.2.3">
<h3><span class="header-section-number">3.2.3</span> Proving Inequalities: Subsetting<a href="probability.html#proving-inequalities-subsetting" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The <a href="#subset-inequality">Subset Inequality</a> proven above - <span class="math inline">\(A \subset B \implies P(A) \leq P(B)\)</span> can be applied to prove other, more famous inequalities in probability as well. We’ll look at two.</p>
<div class="theorem">
<p><span id="thm:unlabeled-div-8" class="theorem"><strong>Theorem 3.1  (Boole's Inequality) </strong></span>Boole’s Inequality is a general inequality for <em>unions</em> of events. For a set of events <span class="math inline">\(A_1,...,A_n\)</span>,</p>
<p><span class="math display">\[P(\cup_{i=1}^n A_i) \leq \sum_{i=1}^n P(A_i)\]</span>
<em>Proof</em>:</p>
<p>We want to use the subset inequality, so we need to construct a set of subsets <span class="math inline">\(\{B_i\}\)</span>. This subset must have the properties:</p>
<ul>
<li><span class="math inline">\(\cup_{i=1}^n A_i = \cup_{i=1}^n B_i\)</span>, so we can replace this on the LHS of Boole’s inequality above</li>
<li><span class="math inline">\(B_i \subset A_i \subset B_i\)</span>, so we can use the subset inequality.</li>
<li><span class="math inline">\(\{B_i\}\)</span> is disjoint, so we can construct the sum on the RHS using Kolmogorov’s Axiom 3.</li>
</ul>
<p>We can accomplish this by <a href="probability.html#disjointify">disjointifying</a>. Let</p>
<p><span class="math display">\[\begin{align}
B_1 = A_1 \\
B_2 = A_2 \cap A_1^c \\
B_3 = A_3 \cap A_2^c \cap A_1^c \\
... \\
B_n = A_n \cap A_{n-1}^c \cap ... \cap A_1^c
\end{align}\]</span></p>
<p>Since <span class="math inline">\(\{B_i\}\)</span> are all disjoint, by Kolmogorov Axiom 3, <span class="math inline">\(P(\cup_{i=1}^n A_i) = \sum_{i=1}^nP(B_i)\)</span>.</p>
<p>Finally, use the <a href="#subset-inequality">subset inequality</a> to note that <span class="math inline">\(B_i \subseteq A_i \implies P(B_i) \leq P(A_i)\)</span> and therefore <span class="math inline">\(P(\cup_{i=1}^n A_i) \leq \sum_{i=1}^n P(B_i) = \sum_{i=1}^n P(A_i)\)</span></p>
</div>
<div class="theorem">
<p><span id="thm:unlabeled-div-9" class="theorem"><strong>Theorem 3.2  (Bonferroni's Inequality) </strong></span>Bonferroni’s Inequality is the <em>set intersection</em> counterpart of Boole’s Inequality. In general, it states</p>
<p><span class="math display">\[P(\cap_{i=1}^n A_i) \geq \sum_{i=1}^n P(A_i) - (n-1)\]</span>
<em>Proof</em>:
1. We need the LHS. Start by taking the complement <span class="math inline">\(P(\cap_{i=1}^n A_i) = 1 - P((\cap_{i=1}^n)^c)\)</span>.
2. Apply <a href="probability.html#demorgan">DeMorgan’s Laws</a>: <span class="math inline">\(1 - P((\cap_{i=1}^n)^c) = 1 - P(\cup_{i=1}^nA_i^c)\)</span>.
3. This is Boole’s Inequality which we already proved. <span class="math inline">\(1 - P(\cup_{i=1}^nA_i^c) \geq 1 - \sum_{i=1}^n P(A_i^c)\)</span>
4. Undo the complement:</p>
<p><span class="math display">\[
1 - \sum_{i=1}^n P(A_i^c) = 1 - \sum_{i=1}^n (1 - P(A_i)) \\
= \sum_{i=1}^n P(A_i) - (n-1)
\]</span></p>
</div>
<p>Next, we’ll discuss conditional probability, where computing set intersections and their bounds is critical for problem-solving.</p>
</div>
</div>
<div id="conditional-probability" class="section level2 hasAnchor" number="3.3">
<h2><span class="header-section-number">3.3</span> Conditional Probability<a href="probability.html#conditional-probability" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Sometimes in a problem, we are given additional information about an event. For example, we might wish to compute the probability of <span class="math inline">\(A\)</span> given that we know another event <span class="math inline">\(B\)</span> has already occurred. In this case, we must compute conditional probabilities.</p>
<div class="definition">
<p><span id="def:unlabeled-div-10" class="definition"><strong>Definition 3.2  (Conditional Probability) </strong></span>The probability of event <span class="math inline">\(A\)</span> occurring given that we know event <span class="math inline">\(B\)</span> has occurred is given by</p>
<p><span class="math display">\[P(A|B) = \frac{P(A\cap B)}{P(B)}\]</span></p>
</div>
<p>When solving problems involving conditional probabilities, we often convert them via the above definition, and then manipulate the intersection <span class="math inline">\(A\cap B\)</span> using set theory to rewrite into the given form.</p>
<div class="definition">
<p><span id="def:unlabeled-div-11" class="definition"><strong>Definition 3.3  (Bayes' Theorem) </strong></span>If we need to invert the order of <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> in a conditional probability express, we can use the property</p>
<p><span class="math display">\[P(A|B) = \frac{P(B|A)P(A)}{P(B)}\]</span>
More generally, let <span class="math inline">\(A_1, A_2, ...\)</span> be a partition of the sample space and let <span class="math inline">\(B\)</span> be any set</p>
<p><span class="math display">\[P(A_i|B) = \frac{P(B|A_i)P(A_i)}{\sum_{j=1}^{\infty} P(B|A_j)P(A_j)}\]</span></p>
</div>
<p>Bayes Theorem often arises in word problems involving given probabilities</p>
<div class="definition">
<p><span id="def:unlabeled-div-12" class="definition"><strong>Definition 3.4  (Law of Total Probability) </strong></span>Let <span class="math inline">\(A_1, A_2, ..., A_n\)</span> be a partition of the sample space and let <span class="math inline">\(B\)</span> be any set.</p>
<p><span class="math display">\[P(B) = \sum_{i=1}^{n} P(B \cap A_i) = \sum_{i=1}^{n} P(B|A_i) P(A_i)\]</span></p>
</div>
<div id="conditional-probability-in-practice" class="section level3 hasAnchor" number="3.3.1">
<h3><span class="header-section-number">3.3.1</span> Conditional Probability in Practice<a href="probability.html#conditional-probability-in-practice" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Public health research often relies on the task of comparison. There are several standard measures of association that are widely used in public health research. Here we present two of them.</p>
<div class="definition">
<p><span id="def:unlabeled-div-13" class="definition"><strong>Definition 3.5  (Relative Risk, RR) </strong></span>Let <span class="math inline">\(D\)</span> be an outcome event and <span class="math inline">\(S\)</span> be an exposure event. The relative risk (or risk ratio), denoted by <span class="math inline">\(RR\)</span> is given by</p>
<p><span class="math display">\[RR = \frac{P(D|S)}{P(D|\bar{S})}\]</span>
where <span class="math inline">\(\bar{S}\)</span> is the complement of <span class="math inline">\(S\)</span>.</p>
</div>
<div class="definition">
<p><span id="def:unlabeled-div-64" class="definition"><strong>Definition 3.6  (Odds Ratio, OR) </strong></span>Let <span class="math inline">\(D\)</span> be an outcome event and <span class="math inline">\(S\)</span> be an exposure event. The odds ratio, denoted by <span class="math inline">\(OR\)</span>, is given by</p>
<p><span class="math display">\[OR = \frac{P(D|S)/P(\bar{D}|S)}{P(D|\bar{S})/P(\bar{D}|\bar{S})}\]</span>
Note that <span class="math inline">\(P(\bar{D}|S) = 1 - P(D|S)\)</span>.</p>
<ul>
<li>The odds ratio is invariant to switching <span class="math inline">\(D\)</span> and <span class="math inline">\(S\)</span>, ie.
<span class="math display">\[\frac{P(D|S)/P(\bar{D}|S)}{P(D|\bar{S})/P(\bar{D}|\bar{S})} = \frac{P(S|D)/P(\bar{S}|D)}{P(S|\bar{D})/P(\bar{S}|\bar{D})}.\]</span>
<em>Proof.</em></li>
</ul>
<p><span class="math display">\[
\begin{aligned}
\frac{P(D|S)/P(\bar{D}|S)}{P(D|\bar{S})/P(\bar{D}|\bar{S})} &amp;= P(D|S) \cdot \frac{1}{P(\bar{D}|S)} \cdot \frac{1}{P(D|\bar{S})} \cdot P(\bar{D}|\bar{S})\\
&amp;= \frac{P(S|D) P(D)}{P(S)} \cdot \frac{P(S)}{P(S|\bar{D})P(\bar{D})} \cdot \frac{P(\bar{S})}{P(\bar{S}|D)P(D)} \cdot \frac{P(\bar{S}|\bar{D})P(\bar{D})}{P(\bar{S})} \\
&amp;= \frac{P(S|D)/P(\bar{S}|D)}{P(S|\bar{D})/P(\bar{S}|\bar{D})}
\end{aligned}
\]</span>
The second step plugs in the following equalities which follow from Bayes’ Rule</p>
<p><span class="math display">\[P(D|S) = \frac{P(S|D) P(D)}{P(S)}, P(\bar{D}|S) = \frac{P(S|\bar{D})P(\bar{D})}{P(S)},\]</span>
<span class="math display">\[P(D|\bar{S}) = \frac{P(\bar{S}|D)P(D)}{P(\bar{S})}, \text{and } P(\bar{D}|\bar{S}) = \frac{P(\bar{S}|\bar{D})P(\bar{D})}{P(\bar{S})}\]</span></p>
<div id="random-variables" class="section level2 hasAnchor" number="3.4">
<h2><span class="header-section-number">3.4</span> Random Variables<a href="probability.html#random-variables" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="definition" class="section level3 hasAnchor" number="3.4.1">
<h3><span class="header-section-number">3.4.1</span> Definition<a href="probability.html#definition" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Statisticians study data, which typically comes not in the form of “events”, but rather, <em>random variables</em>. A random variable is a real-valued function which maps an event <span class="math inline">\(A\)</span> to some numeric quantity - that is, for an event <span class="math inline">\(A\)</span>, the random variable <span class="math inline">\(X(A) \mapsto x \in \mathbb{R}\)</span>. Even though <span class="math inline">\(X\)</span> is a function, the <span class="math inline">\(A\)</span> is usually omitted, and probability texts usually refer to it as one would a number (i.e. saying it “takes on” a particular value).</p>
<p>Suppose <span class="math inline">\(A\)</span> represents the event where a person contracts heart disease. Then, we can “turn this into a number” by defining a random variable:</p>
<p><span class="math inline">\(X = \begin{cases}1 \text{ if }A\text{ occurs}\\0 \text{ if }A\text{ does not occur}\end{cases}\)</span>.</p>
<p>Since the output of a random variable is real, we can describe the probability of a given numerical output from a random variable in many ways. Here are the most common:</p>
</div>
<div id="functions-describing-the-distribution" class="section level3 hasAnchor" number="3.4.2">
<h3><span class="header-section-number">3.4.2</span> Functions Describing the Distribution<a href="probability.html#functions-describing-the-distribution" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div class="definition">
<p><span id="def:unlabeled-div-14" class="definition"><strong>Definition 3.7  (Probability Mass Function) </strong></span>If <span class="math inline">\(X\)</span> is countable, then the probability mass function (pmf) is</p>
<p><span class="math display">\[f_X(x) = P(X = x)\]</span>.</p>
<p>Properties arising from Kolmogorov’s Axioms:
- <span class="math inline">\(0 \leq f_X(x) \leq 1\)</span> (by Axiom 1)
- <span class="math inline">\(\sum_{x} f_X(x) = 1\)</span> (by Axiom 3)</p>
</div>
<div class="definition">
<p><span id="def:unlabeled-div-15" class="definition"><strong>Definition 3.8  (Cumulative Distribution Function) </strong></span>The cumulative distribution function (cdf) of <span class="math inline">\(X\)</span> is <span class="math inline">\(F_X(x) = P(X \leq x)\)</span>.</p>
<ul>
<li>For discrete RVs with <span class="math inline">\(x \geq 0\)</span>, <span class="math inline">\(F_X(x) = \sum_{k = 0}^xP(X = x)\)</span></li>
<li>For continuous RVs, <span class="math inline">\(F_X(x) = \int_{-\infty}^xf(t)dt\)</span></li>
</ul>
<p>A function <span class="math inline">\(F(x)\)</span> is a cdf iff the following all hold:</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(\lim_{x\rightarrow -\infty}F(x) = 0\)</span> and <span class="math inline">\(\lim_{x\rightarrow \infty}F(x) = 1\)</span>.</li>
<li><span class="math inline">\(F(x)\)</span> is nondecreasing.</li>
<li><span class="math inline">\(F(x)\)</span> is right continuous: <span class="math inline">\(\lim_{x\rightarrow c^+}F(x) = F(c)\)</span>.</li>
</ol>
</div>
<p>We can prove a particular function is a cdf by proving each necessary property. Prove…
- (1) by taking the limit directly
- (2) by taking the derivative of <span class="math inline">\(F_X(x)\)</span> and showing it is positive
- (3) by arguing that <span class="math inline">\(F_X(x)\)</span> is continuous, showing it has no undefined points (continuity implies right-continuity).</p>
<p>In general, we can show two random variables have the same distribution by showing they have the same cdf (<em>example</em>?)</p>
<div class="definition">
<p><span id="def:unlabeled-div-16" class="definition"><strong>Definition 3.9  (Probability Density Function) </strong></span>The probability density function (pdf) is the continuous analogue of a pmf. One way to define the pdf of <span class="math inline">\(X\)</span> is as the derivative of the cdf:</p>
<p><span class="math display">\[f_X(x) = \frac{d}{dx}F_X(x)\]</span></p>
<p>Therefore, <span class="math inline">\(F_X(x) = \int_{-\infty}^xf_X(t)dt\)</span></p>
<p>(It is <strong>not</strong> P(X = x); <span class="math inline">\(X\)</span> is uncountable, so this is technically 0).</p>
<p>It satisfies two properties from Kolmogorov’s Axiom’s:
- <span class="math inline">\(0 \leq f_X(x) \leq 1\)</span> (by Axiom 1)
- <span class="math inline">\(\sum_{x} f_X(x) = 1\)</span> (by Axiom 3)</p>
</div>
<p>When computing the probability of a random variable, it is important to first manipulate the probability function itself algebraically to simply computation. For example, suppose we want to compute <span class="math inline">\(P(X_1 \leq x_1, X_2 \leq x_2)\)</span>. However, it may be easier to use the previously-discussed probability properties to note</p>
<p><span class="math display">\[
P(X_1 \leq x_1, X_2 \leq x_2) = 1 - P(X_1 &gt; X_1 \cup X_2 &gt; x_2) \\
= 1 - P(X_1 &gt; x_1) - P(X_2 &gt; x_2) + P(X_1 &gt; x_1, X_2 &gt; x_2)
\]</span></p>
<p>and compute each part separately. A more general approach to this philosophy is discussed next.</p>
</div>
<div id="direct-manipulation" class="section level3 hasAnchor" number="3.4.3">
<h3><span class="header-section-number">3.4.3</span> Technique: Direct Probability Manipulation<a href="probability.html#direct-manipulation" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Functions of random variables are also random variables. Suppose <span class="math inline">\(Y\)</span> and <span class="math inline">\(X\)</span> are both random variables, and <span class="math inline">\(Y = g(X)\)</span>. We often want to find <span class="math inline">\(P(Y\in A)\)</span> where <span class="math inline">\(A\)</span> is some set such as an interval or even a single value. If we know the distribution of <span class="math inline">\(X\)</span>, we can find the distribution of <span class="math inline">\(Y\)</span> like so:</p>
<p><span class="math display">\[
P(Y \in A) = P(g(X) \in A) = P(X \in g^{-1}(A))
\]</span>
where <span class="math inline">\(g^{-1}(A) = \{x \in \mathcal{X}: g(X) \in A\}\)</span> (note <span class="math inline">\(\mathcal{X}\)</span> is the sample space of <span class="math inline">\(X\)</span>)</p>
<p>Since this is such a fundamental technique arising throughout statistics, in lieu of an example, we will point out where this technique is used in future chapters. We can, however, note that if <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are both discrete, then</p>
<p><span class="math display">\[f_Y(y) = P(Y = y) = \sum_{x \in g^{-1}(y)}P(X=x) = \sum_{x \in g^{-1}(y)}f_X(x)\]</span></p>
<p>meaning the pmf of <span class="math inline">\(Y\)</span> is just the sum of the probabilities of <span class="math inline">\(X\)</span> over all <span class="math inline">\(y \in \mathcal{Y}\)</span> (where <span class="math inline">\(\mathcal{Y}\)</span> is the sample space of <span class="math inline">\(Y\)</span>). For more information, see Chapter 2 of <span class="citation">Casella and Berger (<a href="#ref-Casella1990">1990</a>)</span>. <strong>This is very important! When in doubt, fall back on direct manipulation of the probability function.</strong></p>
</div>
</div>
<div id="independence" class="section level2 hasAnchor" number="3.5">
<h2><span class="header-section-number">3.5</span> Independence<a href="probability.html#independence" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div class="definition">
<p><span id="def:unlabeled-div-17" class="definition"><strong>Definition 3.10  (Independence) </strong></span>Two events or random variables are <strong>statistically independent</strong> if the occurrence of one has no impact on the other. Mathematically,</p>
<p><span class="math display">\[P(A | B) = P(A)\]</span></p>
<p>By Bayes’ Theorem,</p>
<p><span class="math display">\[P(A \cap B) = P(A)P(B)\]</span></p>
</div>
<p>Often, we prefer to use the second definition because it is independent and easier to generalize to multiple events or random variables. For example, if we know that the random variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are independent, then</p>
<p><span class="math display">\[f_{X,Y}(x, y) = f_X(x)f_Y(y)\]</span></p>
<p>This property is the foundation of statistical inference for iid random variables discussed in Chapters 8 and 10.</p>
</div>
<div id="order-statistics" class="section level2 hasAnchor" number="3.6">
<h2><span class="header-section-number">3.6</span> Order Statistics<a href="probability.html#order-statistics" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div class="definition">
<p><span id="def:unlabeled-div-18" class="definition"><strong>Definition 3.11  (Order statistic) </strong></span>The <span class="math inline">\(k\)</span>th order statistic of a random sample from a random variable <span class="math inline">\(X\)</span> is the <span class="math inline">\(k\)</span>th smallest value. We denote the <span class="math inline">\(k\)</span>th order statistic as <span class="math inline">\(X_{(k)}\)</span>.</p>
<p>Let <span class="math inline">\(X_1, X_2, ..., X_n\)</span> be a random sample from <span class="math inline">\(X \sim f_X(x)\)</span> with cumulative distribution <span class="math inline">\(F_X(x)\)</span>. The order statistics are random variables themselves that satisfy <span class="math inline">\(X_{(1)} \leq X_{(2)}\leq ... \leq X_{(n)}\)</span>. In particular, <span class="math inline">\(X_{(1)} = min(X_1, X_2, ..., X_n)\)</span> and <span class="math inline">\(X_{(n)} = max(X_1, X_2, ..., X_n)\)</span>.</p>
<ul>
<li>The <strong>sample range</strong>, <span class="math inline">\(R = X_{(n)} - X_{(1)}\)</span>, is the distance between the smallest and the largest order statistics of the sample.</li>
<li>The <strong>sample median</strong> is a number <span class="math inline">\(M\)</span> such that approximately half of the observations in the sample are less than <span class="math inline">\(M\)</span> and the other half are greater.</li>
</ul>
<p><span class="math display">\[M = \begin{cases}
      X_{((n+1)/2)}&amp; \text{if } n  \text{ is odd} \\
      (X_{(n/2)} + X_{(n/2 + 1)})/2 &amp; \text{if } n  \text{ is even.}
   \end{cases}\]</span></p>
</div>
<div class="theorem">
<p><span id="thm:unlabeled-div-19" class="theorem"><strong>Theorem 3.3  </strong></span>Let <span class="math inline">\(X_1, X_2, ..., X_n\)</span> be a random sample from a discrete distribution with pmf <span class="math inline">\(f_X(x_i) = p_i\)</span>, where <span class="math inline">\(x_1&lt;x_2&lt; \cdots\)</span> are the possible values of <span class="math inline">\(X\)</span> in ascending order. Define <span class="math inline">\(P_i = p_1 + p_2 + \cdots p_i\)</span>. Let <span class="math inline">\(X_{(1)}, X_{(2)}, ... ,X_{(n)}\)</span> denote the order statistics from the sample. Then</p>
<p><span class="math display">\[P(X_{(j)}\leq x_i) = \sum_{k=j}^{n} {n \choose k} P_i^k(1-P_i)^{n-k}\]</span>
and</p>
<p><span class="math display">\[P(X_{(j)}= x_i) = \sum_{k=j}^{n} {n \choose k} [P_i^k(1-P_i)^{n-k} - P_{i-1}^k(1-P_{i-1})^{n-k}].\]</span></p>
</div>
<div class="theorem">
<p><span id="thm:unlabeled-div-20" class="theorem"><strong>Theorem 3.4  </strong></span>Let <span class="math inline">\(X_1, X_2, ..., X_n\)</span> be a random sample from a continuous distribution with pdf <span class="math inline">\(f_X(x_i)\)</span> and cdf <span class="math inline">\(F_X(x)\)</span>. Then the pdf of <span class="math inline">\(X_{(j)}\)</span> is</p>
<p><span class="math display">\[f_{X_{(j)}}(x) = \frac{n!}{(j-1)!(n-j)!} f_X(x)[F_X(x)]^{j-1}[1-F_X(x)]^{n-j}.\]</span></p>
</div>
<div class="theorem">
<p><span id="thm:unlabeled-div-21" class="theorem"><strong>Theorem 3.5  </strong></span>Let <span class="math inline">\(X_1, X_2, ..., X_n\)</span> be a random sample from a continuous distribution with pdf <span class="math inline">\(f_X(x_i)\)</span> and cdf <span class="math inline">\(F_X(x)\)</span>. Then the joint pdf of <span class="math inline">\(X_{(i)}\)</span> and <span class="math inline">\(X_{(j)}, 1\leq i \leq j \leq n\)</span>, is</p>
<p><span class="math display">\[f_{X_{(i)}, X_{(j)}} (u,v) = \frac{n!}{(i-1)!(j-1-i)!(n-j)!}f_X(u)f_X(v)[F_X(u)]^{i-1} [F_X(v)-F_X(u)]^{j-1-i}[1-F_X(v)]^{n-j}.\]</span></p>
<p>The joint pdf of all order statistics is</p>
<p><span class="math display">\[f_{X_{(1)}, X_{(2)}, ..., X_{(n)}} (x_1,x_2, ..., x_n) = \begin{cases}
n!f_X(x_1)f_X(x_2) \cdots f_X(x_n) &amp; -\infty&lt; x_1&lt;x_2&lt;\cdots&lt;x_n&lt;\infty\\
0 &amp; otherwise
\end{cases}\]</span></p>
</div>
<div class="example">
<p><span id="exm:unlabeled-div-22" class="example"><strong>Example 3.5  (Joint Density of Order Statistic Differences) </strong></span>A useful trick for finding the joint of <span class="math inline">\(D_i = X_{(i)} - X_{(i-1)}\)</span> and proving that they are independent for a given distribution is by rewriting the random variable as</p>
<p>$$
X_{(1)} = D_1 + 0\
X_{(2)} = D_2 + X_{(1)} = D_2 + D_1\
X_{(3)} = D_3 + X_{(2)} = D3 + D2 + D1\</p>
<p>$$
and so on. By induction, <span class="math inline">\(X_{(i)} = \sum_{j=1}^{i-1}X_{(j)}\)</span>. Therefore, we can rewrite the joint density of all order statistics as <span class="math inline">\(n!f(d_1)f(d_2 + d_1)\cdot...\cdot f(d_1 + ... + d_n)I(d_i \geq 0, \forall i)\)</span>, which we can then compute and factorize to show independence.</p>
</div>
</div>
<div id="convergence" class="section level2 hasAnchor" number="3.7">
<h2><span class="header-section-number">3.7</span> Convergence<a href="probability.html#convergence" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="borel-cantelli-lemmas" class="section level3 hasAnchor" number="3.7.1">
<h3><span class="header-section-number">3.7.1</span> Borel-Cantelli Lemmas<a href="probability.html#borel-cantelli-lemmas" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>What do we do when an infinite number of events occur? How can we ascertain their probability? For this, we rely on the Borel-Cantelli Lemmas. Let <span class="math inline">\(E_1\)</span>, <span class="math inline">\(E_2\)</span>,… be a sequence of events. Then:</p>
<div class="lemma">
<p><span id="lem:unlabeled-div-23" class="lemma"><strong>Lemma 3.1  (First Borel-Cantelli) </strong></span>If <span class="math inline">\(\sum_{n=1}^\infty P(E_n) \leq \infty\)</span>, then</p>
<p><span class="math display">\[P\Big(\limsup_{n\rightarrow\infty}E_n\Big) = P\Big(\bigcap_{n=1}^\infty\bigcup_{k=n}^\infty E_k\Big) = 0\]</span></p>
</div>
<div class="lemma">
<p><span id="lem:unlabeled-div-24" class="lemma"><strong>Lemma 3.2  (Second Borel-Cantelli) </strong></span>If <span class="math inline">\(\sum_{n=1}^\infty P(E_n) = \infty\)</span> and <span class="math inline">\(E_k\)</span> are independent, then</p>
<p><span class="math display">\[P\Big(\limsup_{n\rightarrow\infty}E_n\Big) = P\Big(\bigcap_{n=1}^\infty\bigcup_{k=n}^\infty E_k\Big) = 1\]</span></p>
</div>
</div>
<div id="convergence-in-probability" class="section level3 hasAnchor" number="3.7.2">
<h3><span class="header-section-number">3.7.2</span> Convergence in Probability<a href="probability.html#convergence-in-probability" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>What happens when, instead of a set of events or random variables, we observe a <em>sequence</em> of <span class="math inline">\(n\)</span> random variables? There exist two major types of <strong>convergence</strong> of random variables with which we are typically concerned: <strong>convergence in probability</strong> and <strong>convergence in distribution</strong>. Let’s start with the former.</p>
<div class="definition">
<p><span id="def:unlabeled-div-25" class="definition"><strong>Definition 3.12  (Convergence in Probability) </strong></span>If <span class="math inline">\(Z\)</span> is a random variable and <span class="math inline">\(Z_n\)</span> is a sequence of random variables, then</p>
<p><span class="math display">\[Z_n \overset{p}{\rightarrow} Z \iff\lim_{n\rightarrow\infty}P(|Z_n - Z| &gt; \epsilon) = 0\]</span></p>
</div>
<p>If <span class="math inline">\(Z\)</span> is degenerate (meaning it equals a constant value, or <span class="math inline">\(Z = c\)</span>), we can show convergence in probability by <a href="probability.html#disjointify">disjointifying</a> the absolute value into two disjoint events, and using Kolmogorov Axiom 3. For example,</p>
<p><span class="math display">\[
P(|Z_n - c| &gt; \epsilon) = P(( Z_n - c &gt; \varepsilon) \cup (Z_n - c &lt;-\varepsilon)) \text{ with } ( Z_n - c &gt; \varepsilon), (Z_n - c &lt;-\varepsilon) \text{ disjoint} \\
= P(Z_n &gt; c + \epsilon) + P(Z_n &lt; c - \epsilon) = 1 - P(Z_n &lt; c + \epsilon) + P(Z_n &lt; c - \epsilon)
\]</span>
From here, we can use the CDF of <span class="math inline">\(Z_n\)</span> to compute the necessary probability. But be careful - this method may not work if the convergence of the CDF cannot be evaluated easily.</p>
<p>Convergence in probability has several properties - if we <em>know</em> <span class="math inline">\(Z_n\)</span> convergence in probability, we can use these for solving other problems, especially in <a href="point-estimators-asymptotics.html#point-estimators-asymptotics">Chapter 9 - Point Estimator Asymptotic</a> If <span class="math inline">\(A_n \overset{p}{\rightarrow} a\)</span> and <span class="math inline">\(B_n\overset{p}{\rightarrow}b\)</span>, then</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(A_n + B_n \overset{p}{\rightarrow} a + b\)</span></li>
<li><span class="math inline">\(A_n - B_n \overset{p}{\rightarrow} a - b\)</span></li>
<li><span class="math inline">\(A_n \cdot B_n \overset{p}{\rightarrow} a \cdot b\)</span></li>
<li><span class="math inline">\(A_n / B_n \overset{p}{\rightarrow} a / b\)</span></li>
</ol>
<p>Convergence in probability can also be extended to the multivariate setting, where it takes on a slightly different meaning.</p>
<div class="definition">
<p><span id="def:unlabeled-div-26" class="definition"><strong>Definition 3.13  (Multivariate Convergence in Probability) </strong></span>If <span class="math inline">\(X\)</span> is random vector and <span class="math inline">\(X_n\)</span> is a sequence of random vectors, then</p>
<p><span class="math display">\[\begin{align}
X_n \overset{p}{\rightarrow} X \iff\lim_{n\rightarrow\infty}P(||X_n - X|| &gt; \epsilon) = 0 \\
\iff X_{jn} \overset{p}{\rightarrow}X_j, \forall j \in 1,...,k
\end{align}\]</span></p>
</div>
</div>
<div id="convergence-in-distribution" class="section level3 hasAnchor" number="3.7.3">
<h3><span class="header-section-number">3.7.3</span> Convergence in Distribution<a href="probability.html#convergence-in-distribution" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>A sequence of random variables may converge to another random variable as well.</p>
<div class="definition">
<p><span id="def:unlabeled-div-27" class="definition"><strong>Definition 3.14  (Convergence in Distribution) </strong></span><span class="math display">\[Z_n \overset{\mathcal{D}}{\rightarrow} Z \iff\lim_{n\rightarrow\infty}F_n(Z) = F(z), \forall\text{ continuity points of }F\]</span></p>
</div>
<p>Note that convergence in probability implies convergence in distribution, but not the converse. That is,</p>
<p><span class="math display">\[Z_n \overset{p}{\rightarrow} Z \implies Z_n \overset{\mathcal{D}}{\rightarrow} Z\]</span>
Unless, that is, the random variable converges in distribution to a <em>constant</em> — then, convergence in distribution <em>does</em> imply convergence in probability! That is,</p>
<p><span class="math display">\[Z_n \overset{\mathcal{D}}{\rightarrow} c \implies Z_n \overset{p}{\rightarrow} c\]</span>
Note that unlike convergence in probability, convergence in distribution of each entry of a random vector does not necessarily imply convergence in distribution of the entire random vector. Rather, the joint CDF of the entire vector must convergence. This concept is formalized below:</p>
<div class="definition">
<p><span id="def:unlabeled-div-28" class="definition"><strong>Definition 3.15  (Multivariate Convergence in Distribution) </strong></span>If <span class="math inline">\(X\)</span> is random vector and <span class="math inline">\(X_n\)</span> is a sequence of random vectors, then</p>
<p><span class="math display">\[X_n \overset{\mathcal{D}}{\rightarrow} X \iff \lim_{n\rightarrow\infty}F_n(X_1, ..., X_k) = F(X_1,...,X_k), \forall\text{ continuity points of }F\]</span></p>
</div>
<p>How do we prove <span class="math inline">\(X_n\)</span> converges in distribution to <span class="math inline">\(X\)</span>? There are three chief strategies:</p>
<ol style="list-style-type: decimal">
<li>Derive the limiting CDF of <span class="math inline">\(X_n\)</span> and show its limit as <span class="math inline">\(n\rightarrow \infty\)</span> equals the <span class="math inline">\(F_X(x)\)</span>.</li>
<li>Derive the MGF or CF of <span class="math inline">\(X_n\)</span> and show its limit as <span class="math inline">\(n\rightarrow \infty\)</span> equals the <span class="math inline">\(\mathcal{M}_X(t)\)</span>.</li>
<li>Use the CLT with Slutsky’s Theorem or the CMT (will be discussed later in <a href="point-estimators-asymptotics.html#point-estimators-asymptotics">Chapter 9 - Point Estimator Asymptotics</a>)</li>
</ol>
</div>
<div id="important-theorems" class="section level3 hasAnchor" number="3.7.4">
<h3><span class="header-section-number">3.7.4</span> Important Theorems<a href="probability.html#important-theorems" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div class="theorem">
<p><span id="thm:unlabeled-div-29" class="theorem"><strong>Theorem 3.6  (Slutsky's Theorem) </strong></span>If <span class="math inline">\(Z_n \overset{\mathcal{D}}{\rightarrow} Z\)</span> and <span class="math inline">\(Y_n \overset{p}{\rightarrow} c\)</span>, then</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(Z_n + Y_n \overset{\mathcal{D}}{\rightarrow} Z + c\)</span></li>
<li><span class="math inline">\(Z_nY_n \overset{\mathcal{D}}{\rightarrow} cZ\)</span></li>
<li><span class="math inline">\(\frac{Z_n}{Y_n}\overset{\mathcal{D}}{\rightarrow}\frac{Z}{c}\)</span>.</li>
</ol>
</div>
<p>For problem-solving, Slutsky’s theorem is generally applied whenever we deal with both convergence in probability and convergence in distribution together.</p>
<div class="theorem">
<p><span id="thm:cmt" class="theorem"><strong>Theorem 3.7  (Continuous Mapping Theorem) </strong></span>Suppose <span class="math inline">\(Y_n\)</span> is a sequence of random variables (possibly vectors), <span class="math inline">\(Y\)</span> is a random variable (or vector the same length as <span class="math inline">\(Y_n\)</span>), <span class="math inline">\(c\)</span> is a constant, and <span class="math inline">\(g\)</span> is a function. Then,</p>
<ol style="list-style-type: decimal">
<li><p>If <span class="math inline">\(Y_n \overset{p}{\rightarrow} c\)</span>, and <span class="math inline">\(g\)</span> is continuous at <span class="math inline">\(c\)</span>, then <span class="math inline">\(g(Y_n) \overset{p}{\rightarrow} g(c)\)</span></p></li>
<li><p>If <span class="math inline">\(Y_n \overset{\mathcal{D}}{\rightarrow} Y\)</span>, and <span class="math inline">\(g\)</span> is continuous (with <span class="math inline">\(g: \mathbb{R}^k \mapsto \mathbb{R}^m\)</span> for vectors), then <span class="math inline">\(g(Y_n) \overset{\mathcal{D}}{\rightarrow} g(Y)\)</span> (in <span class="math inline">\(\mathbb{R}^m\)</span> for vectors)</p></li>
</ol>
</div>

</div>
</div>
<h3>References<a href="references.html#references" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-Casella1990" class="csl-entry">
Casella, G C, and Roger L Berger. 1990. <em>Statistical Inference</em>. 2nd ed. The Wadsworth &amp; Brooks/Cole Statistics/Probability Series. Florence, KY: Brooks/Cole.
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="math-tricks.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="known-distributions.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown-demo/edit/master/03-probability.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["Solving-Statistical-Problems.pdf", "Solving-Statistical-Problems.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
