[["index.html", "Solving Statistical Problems Chapter 1 Introduction", " Solving Statistical Problems Salvador Balkus, Kimberly Greco, and Mónica Robles Fontán 2023-06-28 Chapter 1 Introduction Welcome to “Solving Statistical Problems”, a compilation of problem-solving tools and tricks for graduate-level Probability and Statistical Inference courses. This site is currently under construction in preparation for the 2023 Harvard Biostatistics PhD Qualifying Exam, so stay tuned! "],["math-tricks.html", "Chapter 2 Math Tricks 2.1 Combinatorics 2.2 Geometric Series 2.3 Exponential Taylor Series 2.4 Exponential Limit 2.5 Integration by Parts 2.6 Leibniz’s Rule 2.7 Gamma Function 2.8 Triangle Inequality 2.9 Fubini’s Theorem", " Chapter 2 Math Tricks This chapter will cover highly specific mathematical techniques used to solve stats problems but themselves require no statistical knowledge, i.e… Exponential function as limits/series Inductive integration by parts substitution tricks for integration binomial theorem Gamma function properties Matrices (determinants, jacobians, etc.) Taylor Series (univariate and multivariate) Lagrange multipliers? (Maybe save for MLEs) Open versus closed intervals 2.1 Combinatorics 2.2 Geometric Series The geometric series is a useful series convergence, defined as follows: \\[\\sum_{x=0}^\\infty ar^x = \\frac{a}{1-r} \\text{ for } |r| &lt; 1\\] One place it arises is computing the moments of the Geometric Distribution. 2.3 Exponential Taylor Series Some distributions such as the Poisson rely on infinite sums. Often, we can simplify these infinite sums to an exponential by rewriting them using the following series convergence: \\[\\sum_{x=0}^\\infty \\frac{\\lambda^x}{x!} = \\exp(\\lambda)\\] This is the Taylor series for the exponential function, sometimes called the “exponential series”. 2.4 Exponential Limit Another way to express the exponential function is by the limit \\[\\exp(x) = \\lim_{n\\rightarrow \\infty}(1 + \\frac{x}{n})^n\\] 2.5 Integration by Parts Another technique, especially for computing moments, is integration by parts, defined by: \\[\\int udv = uv - \\int vdu\\] Integration by parts is typically used for products of functions. By setting \\(u\\) equal to a function which has a finite number of nonzero derivatives (for example, \\(x^k\\)), and \\(v\\) equal to a function which does not (such as \\(e^x\\)), this technique can be repeatedly applied to compute integration. Of course, take caution: If both functions can be repeatedly differentiated infinitely, this may not work! However, it may be possible to use inductive integration by parts to prove that if we apply integration by parts infinitely, the result will yield a series which converges to some value (where?) 2.6 Leibniz’s Rule Theorem 2.1 (Leibniz's Rule: Simple Version) If \\(a, b\\) are constant and \\(f(x, \\theta)\\) is differentiable w.r.t to \\(\\theta\\), then \\[\\frac{d}{d\\theta} \\int_{a}^{b} f(x,\\theta)dx = \\int_a^b \\frac{\\partial}{\\partial\\theta}f(x,\\theta)dx\\] Theorem 2.2 (Leibniz's Rule: Complicated Version) If \\(a(\\theta)\\), \\(b(\\theta)\\), and \\(f(x, \\theta)\\) are differentiable w.r.t to \\(\\theta\\), then \\[\\frac{d}{d\\theta}\\int_{a(\\theta)}^{b(\\theta)}f(x,\\theta)dx = f(b(\\theta), \\theta)\\frac{d}{d\\theta}b(\\theta) - f(a(\\theta), \\theta)\\frac{d}{d\\theta}a(\\theta) + \\int_{a(\\theta)}^{b(\\theta)} \\frac{\\partial}{\\partial\\theta}f(x,\\theta)dx\\] 2.7 Gamma Function Defined as \\[\\Gamma(z) = \\int_{0}^\\infty t^{z-1}e^{-t}dt\\] the Gamma function commonly appears in the probability density function of several random variables, including the Gamma (duh!) and the Beta. The most important property of \\(\\Gamma(z)\\) is that \\[\\Gamma(z + 1) = z\\Gamma(z)\\] This is because we can substitute \\(\\Gamma(z) = \\frac{\\Gamma(z + 1)}{z}\\) to perform the Kernel Technique on distributions involving the Gamma function in their pdf As a corollary, when \\(n\\) is an integer, \\(\\Gamma(n) = (n-1)!\\). 2.8 Triangle Inequality The triangle inequality is defined as \\[|x + y| \\leq |x| + |y|\\] with \\(|x + y| \\leq |x| + |y|\\) unless \\(x, y \\geq 0\\). This inequality is useful for proving moment bounds as well as asymptotic convergence in Chapter 9 and Chapter 11. A general trick for proving the triangular inequality that may be useful in other problems involving absolute value can be termed the “square trick” - noting \\(x^2 = |x|^2\\), we can first show the equality of a square, then take the square root to obtain \\(|x|\\). Here is a proof of the triangle inequality using this trick: Note \\((a + b)^2 = |a + b|^2 \\leq 0\\) Rearrange \\((a + b)^2\\) and \\[ (a + b)^2 = a^2 + 2ab + b^2 = |a|^2 + 2ab + |b|^2\\\\ \\leq |a|^2 + 2|a||b| + |b|^2 = (|a| + |b|)^2\\\\ \\implies |a + b|^2 \\leq (|a| + |b|)^2\\\\ \\] 3. Take the square root of the final implication to get \\(|a + b| \\leq |a| + |b|\\) 2.9 Fubini’s Theorem One useful problem-solving technique is exchanging the order of integrals or infinite series to more easily solve an equation. But when can we do this? Theorem 2.3 (Fubini's Theorem: Integrals) \\(\\int_\\mathcal{X}\\int_\\mathcal{Y}f(x,y)dydx = \\int_\\mathcal{Y}\\int_\\mathcal{X}f(x,y)dxdy\\) if \\(\\int_\\mathcal{X}\\int_\\mathcal{Y}|f(x,y)|dydx &lt; \\infty\\) that is, if the integral of its absolute value is finite. Theorem 2.4 (Fubini's Theorem: Infinite Series) \\(\\sum_{m=1}^\\infty\\sum_{n=1}^\\infty a_{m,n} = \\sum_{n=1}^\\infty\\sum_{m=1}^\\infty a_{m,n}\\) if \\(\\sum_{m=1}^\\infty\\sum_{n=1}^\\infty |a_{m,n}| &lt; \\infty\\) that is, if the original series is absolutely convergent "],["probability.html", "Chapter 3 Probability 3.1 Basic Axioms 3.2 Basic Probability Solving Techniques 3.3 Conditional Probability 3.4 Independence 3.5 Order Statistics 3.6 Convergence", " Chapter 3 Probability 3.1 Basic Axioms How do we reason about events that may or may not occur? The field of probability provides the mathematical foundations for reasoning under uncertainty. The foundation of probability is set theory. When you think about “something that could happen”, you can conceptualize that “something” as an experiment. We can define all of the details of that experiment in terms of sets: Any given outcome of the experiment we represent as a set - call it \\(A\\). We represent the set of all possible that could have occurred - the sampling space - as \\(\\Omega\\). Hence, \\(A \\subseteq \\Omega\\) Now suppose we could repeat this experiment infinitely. The probability of an event A is defined as the proportion of experiments in which that event will occurs as the number of experiments increases towards infinity. Mathematically, we represent probability as a function: \\(P(A)\\). Then, \\[P(A) = \\lim_{n\\rightarrow\\infty}\\frac{\\text{# of times A is drawn}}{n}\\] In the simplest case, we can compute this proportion directly as \\(P(A) = \\frac{\\text{# of outcomes in }A}{\\text{# of outcomes in }\\Omega}\\) - a typical grade-school exercise. Furthermore, from this definition, three properties (called Kolmogorov’s axioms) arise: Definition 3.1 (Kolmogorov's Axioms of Probability) For all probability functions \\(P(\\cdot)\\) defined on a sampling space \\(\\Omega\\): \\(0 \\leq P(A) \\leq 1\\) for all \\(A\\subseteq \\Omega\\) \\(P(\\Omega) = 1\\). If $A_1, A_2, …, $ are pairwise disjoint, then \\[P(\\cup_{i=1}^\\infty A_i) = \\sum_{i=1}^\\infty P(A_i)\\] Two sets \\(A\\) and \\(B\\) are pairwise disjoint if \\(A\\) does not share any elements in common with \\(B\\). This means \\(A \\cup B = \\emptyset\\). Consequently, to find a probability involving multiple events, you can use any knowledge of set theory you might have in conjunction with these axioms. Here’s one example: ::: {.example name=Probability of a Set Complement”} \\(P(A^c) = 1 - P(A)\\). Why? - Since \\(A^c\\) and \\(A\\) are disjoint, and naturally \\(A^c \\cup A = \\Omega\\) we first note, by axiom 3, that \\(P(\\Omega) = P(A^c) + P(A)\\). - Then, by axiom 2, \\(P(\\Omega) = 1\\), so \\(P(A^c) + P(A) = 1\\). Rearrange to complete the proof. ::: These axioms are the foundation of all probability. Learn to recognize them when working directly with probability functions \\(P(\\cdot)\\). For instance, any time you see a sum, you should think “Aha! Kolmogorov Axiom 3!” 3.2 Basic Probability Solving Techniques 3.2.1 Disjointification Kolmogorov’s Axiom 3 is by far the most useful, but to use it requires a trick called disjointification, or “partitioning into disjoint subsets”. In this technique, we rewrite a set into a union of disjoint subsets: \\[A = (A \\cap B) \\cup (A \\cap B^c)\\] Then, if we know the probability of the intersections, we apply Axiom 3 to solve: \\(P(A) = P(A \\cap B) + P(A \\cap B^c)\\). Let’s prove three classic equalities by disjointification: Example 3.1 (Probability of Set Intersection with Complement) To compute the probability of set intersection, use the following: \\[P(B \\cap A^c) = P(B) - P(A \\cap B)\\] Proof: Disjointify: Note \\(B = (B \\cap A) \\cup (B \\cap A^c)\\). Therefore, \\(P(B) = P(B \\cap A^c) + P(A \\cap B)\\) by Probability Axiom 3 Rearrange: \\(P(B \\cap A^c) = P(B) - P(A \\cap B)\\) Example 3.2 (Subset Inequality) Probability inequalities can be constructed using subsets: \\[A \\subset B \\implies P(A) \\leq P(B)\\] Proof: Disjointify: If \\(A \\subset B\\), then \\(B = A \\cup (A^c \\cap B)\\), with \\(A\\) and \\((A^c \\cap B)\\) disjoint (since \\((A^c \\cap B)\\) is the part of \\(B\\) not contained by \\(A\\)). Consequently, \\(P(B) = P(A) + P(A^c \\cap B)\\). By Kolmogorov Axiom 1, \\(P(A^c \\cap B) \\geq 0\\) which implies that \\(P(A) \\leq P(B)\\), completing the proof. Of course, these examples are famous properties that you can simply memorize and apply directly on an example. But, in case you forget, now you know how to derive it! 3.2.2 DeMorgan’s Laws If you ever need to turn a union into an intersection or vice versa, first take the complement \\(P(A) = 1 - P(A^c)\\). Then, apply DeMorgan’s Laws from set theory: \\((A \\cup B)^c = A^c \\cap B^c\\) \\((A \\cap B)^c = A^c \\cup B^c\\) Here’s an example: Example 3.3 (Probability of a Set Union) \\[P(A \\cup B) = P(A) + P(B) - P(A \\cap B)\\] Start by taking the complement: \\(P(A \\cup B) = 1 - P((A\\cup B)^c\\) Apply DeMorgan’s Laws: \\(1 - P((A\\cup B)^c = 1 - P(A^c \\cap B^c)\\). Now, we could disjointify \\(A^c\\), but a faster way is to recall the previous set intersection example: \\(1 - P(A^c \\cap B^c) = 1 - (P(A^c) - P(A^c \\cap B))\\) Revert the complement: \\(1 - (P(A^c) - P(A^c \\cap B)) = P(A) + P(A^c \\cap B)\\) Repeat Step 3 on the other set intersection: \\(P(A) + P(A^c \\cap B) = P(A) + P(B) - P(A \\cap B)\\), completing the proof. 3.2.3 Proving Inequalities: Subsetting The Subset Inequality proven above - \\(A \\subset B \\implies P(A) \\leq P(B)\\) can be applied to prove other, more famous inequalities in probability as well. We’ll look at two. Theorem 3.1 (Boole's Inequality) Boole’s Inequality is a general inequality for unions of events. For a set of events \\(A_1,...,A_n\\), \\[P(\\cup_{i=1}^n A_i) \\leq \\sum_{i=1}^n P(A_i)\\] Proof: We want to use the subset inequality, so we need to construct a set of subsets \\(\\{B_i\\}\\). This subset must have the properties: \\(\\cup_{i=1}^n A_i = \\cup_{i=1}^n B_i\\), so we can replace this on the LHS of Boole’s inequality above \\(B_i \\subset A_i \\subset B_i\\), so we can use the subset inequality. \\(\\{B_i\\}\\) is disjoint, so we can construct the sum on the RHS using Kolmogorov’s Axiom 3. We can accomplish this by disjointifying. Let \\[\\begin{align} B_1 = A_1 \\\\ B_2 = A_2 \\cap A_1^c \\\\ B_3 = A_3 \\cap A_2^c \\cap A_1^c \\\\ ... \\\\ B_n = A_n \\cap A_{n-1}^c \\cap ... \\cap A_1^c \\end{align}\\] Since \\(\\{B_i\\}\\) are all disjoint, by Kolmogorov Axiom 3, \\(P(\\cup_{i=1}^n A_i) = \\sum_{i=1}^nP(B_i)\\). Finally, use the subset inequality to note that \\(B_i \\subseteq A_i \\implies P(B_i) \\leq P(A_i)\\) and therefore \\(P(\\cup_{i=1}^n A_i) \\leq \\sum_{i=1}^n P(B_i) = \\sum_{i=1}^n P(A_i)\\) Theorem 3.2 (Bonferroni's Inequality) Bonferroni’s Inequality is the set intersection counterpart of Boole’s Inequality. In general, it states \\[P(\\cap_{i=1}^n A_i) \\geq \\sum_{i=1}^n P(A_i) - (n-1)\\] Proof: 1. We need the LHS. Start by taking the complement \\(P(\\cap_{i=1}^n A_i) = 1 - P((\\cap_{i=1}^n)^c)\\). 2. Apply DeMorgan’s Laws: \\(1 - P((\\cap_{i=1}^n)^c) = 1 - P(\\cup_{i=1}^nA_i^c)\\). 3. This is Boole’s Inequality which we already proved. \\(1 - P(\\cup_{i=1}^nA_i^c) \\geq 1 - \\sum_{i=1}^n P(A_i^c)\\) 4. Undo the complement: \\[ 1 - \\sum_{i=1}^n P(A_i^c) = 1 - \\sum_{i=1}^n (1 - P(A_i)) \\\\ = \\sum_{i=1}^n P(A_i) - (n-1) \\] Next, we’ll discuss conditional probability, where computing set intersections and their bounds is critical for problem-solving. 3.3 Conditional Probability Sometimes in a problem, we are given additional information about an event. For example, we might wish to compute the probability of \\(A\\) given that we know another event \\(B\\) has already occurred. In this case, we must compute conditional probabilities. Definition 3.2 (Conditional Probability) The probability of event \\(A\\) occurring given that we know event \\(B\\) has occurred is given by \\[P(A|B) = \\frac{P(A\\cap B)}{P(B)}\\] When solving problems involving conditional probabilities, we often convert them via the above definition, and then manipulate the intersection \\(A\\cap B\\) using set theory to rewrite into the given form. Definition 3.3 (Bayes' Theorem) If we need to invert the order of \\(A\\) and \\(B\\) in a conditional probability express, we can use the property \\[P(A|B) = \\frac{P(B|A)P(A)}{P(B)}\\] More generally, let \\(A_1, A_2, ...\\) be a partition of the sample space and let \\(B\\) be any set \\[P(A_i|B) = \\frac{P(B|A_i)P(A_i)}{\\sum_{j=1}^{\\infty} P(B|A_j)P(A_j)}\\] Bayes Theorem often arises in word problems involving given probabilities Definition 3.4 (Law of Total Probability) Let \\(A_1, A_2, ..., A_n\\) be a partition of the sample space and let \\(B\\) be any set. \\[P(B) = \\sum_{i=1}^{n} P(B \\cap A_i) = \\sum_{i=1}^{n} P(B|A_i) P(A_i)\\] 3.3.1 Conditional Probability in Practice Public health research often relies on the task of comparison. There are several standard measures of association that are widely used in public health research. Here we present two of them. Definition 3.5 (Relative Risk, RR) Let \\(D\\) be an outcome event and \\(S\\) be an exposure event. The relative risk (or risk ratio), denoted by \\(RR\\) is given by \\[RR = \\frac{P(D|S)}{P(D|\\bar{S})}\\] where \\(\\bar{S}\\) is the complement of \\(S\\). Definition 3.6 (Odds Ratio, OR) Let \\(D\\) be an outcome event and \\(S\\) be an exposure event. The odds ratio, denoted by \\(OR\\), is given by \\[OR = \\frac{P(D|S)/P(\\bar{D}|S)}{P(D|\\bar{S})/P(\\bar{D}|\\bar{S})}\\] Note that \\(P(\\bar{D}|S) = 1 - P(D|S)\\). The odds ratio is invariant to switching \\(D\\) and \\(S\\), ie. \\[\\frac{P(D|S)/P(\\bar{D}|S)}{P(D|\\bar{S})/P(\\bar{D}|\\bar{S})} = \\frac{P(S|D)/P(\\bar{S}|D)}{P(S|\\bar{D})/P(\\bar{S}|\\bar{D})}.\\] Proof. \\[ \\begin{aligned} \\frac{P(D|S)/P(\\bar{D}|S)}{P(D|\\bar{S})/P(\\bar{D}|\\bar{S})} &amp;= P(D|S) \\cdot \\frac{1}{P(\\bar{D}|S)} \\cdot \\frac{1}{P(D|\\bar{S})} \\cdot P(\\bar{D}|\\bar{S})\\\\ &amp;= \\frac{P(S|D) P(D)}{P(S)} \\cdot \\frac{P(S)}{P(S|\\bar{D})P(\\bar{D})} \\cdot \\frac{P(\\bar{S})}{P(\\bar{S}|D)P(D)} \\cdot \\frac{P(\\bar{S}|\\bar{D})P(\\bar{D})}{P(\\bar{S})} \\\\ &amp;= \\frac{P(S|D)/P(\\bar{S}|D)}{P(S|\\bar{D})/P(\\bar{S}|\\bar{D})} \\end{aligned} \\] The second step plugs in the following equalities which follow from Bayes’ Rule \\[P(D|S) = \\frac{P(S|D) P(D)}{P(S)}, P(\\bar{D}|S) = \\frac{P(S|\\bar{D})P(\\bar{D})}{P(S)},\\] \\[P(D|\\bar{S}) = \\frac{P(\\bar{S}|D)P(D)}{P(\\bar{S})}, \\text{and } P(\\bar{D}|\\bar{S}) = \\frac{P(\\bar{S}|\\bar{D})P(\\bar{D})}{P(\\bar{S})}\\] ## Random Variables 3.3.2 Definition Statisticians study data, which typically comes not in the form of “events”, but rather, random variables. A random variable is a real-valued function which maps an event \\(A\\) to some numeric quantity - that is, for an event \\(A\\), the random variable \\(X(A) \\mapsto x \\in \\mathbb{R}\\). Even though \\(X\\) is a function, the \\(A\\) is usually omitted, and probability texts usually refer to it as one would a number (i.e. saying it “takes on” a particular value). Suppose \\(A\\) represents the event where a person contracts heart disease. Then, we can “turn this into a number” by defining a random variable: \\(X = \\begin{cases}1 \\text{ if }A\\text{ occurs}\\\\0 \\text{ if }A\\text{ does not occur}\\end{cases}\\). Since the output of a random variable is real, we can describe the probability of a given numerical output from a random variable in many ways. Here are the most common: 3.3.3 Functions Describing the Distribution Definition 3.7 (Probability Mass Function) If \\(X\\) is countable, then the probability mass function (pmf) is \\[f_X(x) = P(X = x)\\]. Properties arising from Kolmogorov’s Axioms: - \\(0 \\leq f_X(x) \\leq 1\\) (by Axiom 1) - \\(\\sum_{x} f_X(x) = 1\\) (by Axiom 3) Definition 3.8 (Cumulative Distribution Function) The cumulative distribution function (cdf) of \\(X\\) is \\(F_X(x) = P(X \\leq x)\\). For discrete RVs with \\(x \\geq 0\\), \\(F_X(x) = \\sum_{k = 0}^xP(X = x)\\) For continuous RVs, \\(F_X(x) = \\int_{-\\infty}^xf(t)dt\\) A function \\(F(x)\\) is a cdf iff the following all hold: \\(\\lim_{x\\rightarrow -\\infty}F(x) = 0\\) and \\(\\lim_{x\\rightarrow \\infty}F(x) = 1\\). \\(F(x)\\) is nondecreasing. \\(F(x)\\) is right continuous: \\(\\lim_{x\\rightarrow c^+}F(x) = F(c)\\). We can prove a particular function is a cdf by proving each necessary property. Prove… - (1) by taking the limit directly - (2) by taking the derivative of \\(F_X(x)\\) and showing it is positive - (3) by arguing that \\(F_X(x)\\) is continuous, showing it has no undefined points (continuity implies right-continuity). In general, we can show two random variables have the same distribution by showing they have the same cdf (example?) Definition 3.9 (Probability Density Function) The probability density function (pdf) is the continuous analogue of a pmf. One way to define the pdf of \\(X\\) is as the derivative of the cdf: \\[f_X(x) = \\frac{d}{dx}F_X(x)\\] Therefore, \\(F_X(x) = \\int_{-\\infty}^xf_X(t)dt\\) (It is not P(X = x); \\(X\\) is uncountable, so this is technically 0). It satisfies two properties from Kolmogorov’s Axiom’s: - \\(0 \\leq f_X(x) \\leq 1\\) (by Axiom 1) - \\(\\sum_{x} f_X(x) = 1\\) (by Axiom 3) 3.3.4 Technique: Direct Probability Manipulation Functions of random variables are also random variables. Suppose \\(Y\\) and \\(X\\) are both random variables, and \\(Y = g(X)\\). We often want to find \\(P(Y\\in A)\\) where \\(A\\) is some set such as an interval or even a single value. If we know the distribution of \\(X\\), we can find the distribution of \\(Y\\) like so: \\[ P(Y \\in A) = P(g(X) \\in A) = P(X \\in g^{-1}(A)) \\] where \\(g^{-1}(A) = \\{x \\in \\mathcal{X}: g(X) \\in A\\}\\) (note \\(\\mathcal{X}\\) is the sample space of \\(X\\)) Since this is such a fundamental technique arising throughout statistics, in lieu of an example, we will point out where this technique is used in future chapters. We can, however, note that if \\(X\\) and \\(Y\\) are both discrete, then \\[f_Y(y) = P(Y = y) = \\sum_{x \\in g^{-1}(y)}P(X=x) = \\sum_{x \\in g^{-1}(y)}f_X(x)\\] meaning the pmf of \\(Y\\) is just the sum of the probabilities of \\(X\\) over all \\(y \\in \\mathcal{Y}\\) (where \\(\\mathcal{Y}\\) is the sample space of \\(Y\\)). For more information, see Chapter 2 of Casella and Berger (1990). 3.4 Independence Definition 3.10 (Independence) Two events or random variables are statistically independent if the occurrence of one has no impact on the other. Mathematically, \\[P(A | B) = P(A)\\] By Bayes’ Theorem, \\[P(A \\cap B) = P(A)P(B)\\] Often, we prefer to use the second definition because it is independent and easier to generalize to multiple events or random variables. For example, if we know that the random variables \\(X\\) and \\(Y\\) are independent, then \\[f_{X,Y}(x, y) = f_X(x)f_Y(y)\\] This property is the foundation of statistical inference for iid random variables discussed in Chapters 8 and 10. 3.5 Order Statistics Definition 3.11 (Order statistic) The \\(k\\)th order statistic of a random sample from a random variable \\(X\\) is the \\(k\\)th smallest value. We denote the \\(k\\)th order statistic as \\(X_{(k)}\\). Let \\(X_1, X_2, ..., X_n\\) be a random sample from \\(X \\sim f_X(x)\\) with cumulative distribution \\(F_X(x)\\). The order statistics are random variables themselves that satisfy \\(X_{(1)} \\leq X_{(2)}\\leq ... \\leq X_{(n)}\\). In particular, \\(X_{(1)} = min(X_1, X_2, ..., X_n)\\) and \\(X_{(n)} = max(X_1, X_2, ..., X_n)\\). The sample range, \\(R = X_{(n)} - X_{(1)}\\), is the distance between the smallest and the largest order statistics of the sample. The sample median is a number \\(M\\) such that approximately half of the observations in the sample are less than \\(M\\) and the other half are greater. \\[M = \\begin{cases} X_{((n+1)/2)}&amp; \\text{if } n \\text{ is odd} \\\\ (X_{(n/2)} + X_{(n/2 + 1)})/2 &amp; \\text{if } n \\text{ is even.} \\end{cases}\\] Theorem 3.3 Let \\(X_1, X_2, ..., X_n\\) be a random sample from a discrete distribution with pmf \\(f_X(x_i) = p_i\\), where \\(x_1&lt;x_2&lt; \\cdots\\) are the possible values of \\(X\\) in ascending order. Define \\(P_i = p_1 + p_2 + \\cdots p_i\\). Let \\(X_{(1)}, X_{(2)}, ... ,X_{(n)}\\) denote the order statistics from the sample. Then \\[P(X_{(j)}\\leq x_i) = \\sum_{k=j}^{n} {n \\choose k} P_i^k(1-P_i)^{n-k}\\] and \\[P(X_{(j)}= x_i) = \\sum_{k=j}^{n} {n \\choose k} [P_i^k(1-P_i)^{n-k} - P_{i-1}^k(1-P_{i-1})^{n-k}].\\] Theorem 3.4 Let \\(X_1, X_2, ..., X_n\\) be a random sample from a continuous distribution with pdf \\(f_X(x_i)\\) and cdf \\(F_X(x)\\). Then the pdf of \\(X_{(j)}\\) is \\[f_{X_{(j)}}(x) = \\frac{n!}{(j-1)!(n-j)!} f_X(x)[F_X(x)]^{j-1}[1-F_X(x)]^{n-j}.\\] Theorem 3.5 Let \\(X_1, X_2, ..., X_n\\) be a random sample from a continuous distribution with pdf \\(f_X(x_i)\\) and cdf \\(F_X(x)\\). Then the joint pdf of \\(X_{(i)}\\) and \\(X_{(j)}, 1\\leq i \\leq j \\leq n\\), is \\[f_{X_{(i)}, X_{(j)}} (u,v) = \\frac{n!}{(i-1)!(j-1-i)!(n-j)!}f_X(u)f_X(v)[F_X(u)]^{i-1} [F_X(v)-F_X(u)]^{j-1-i}[1-F_X(v)]^{n-j}.\\] The joint pdf of all order statistics is \\[f_{X_{(1)}, X_{(2)}, ..., X_{(n)}} (x_1,x_2, ..., x_n) = \\begin{cases} n!f_X(x_1)f_X(x_2) \\cdots f_X(x_n) &amp; -\\infty&lt; x_1&lt;x_2&lt;\\cdots&lt;x_n&lt;\\infty\\\\ 0 &amp; otherwise \\end{cases}\\] Example 3.4 (Joint Density of Order Statistic Differences) A useful trick for finding the joint of \\(D_i = X_{(i)} - X_{(i-1)}\\) and proving that they are independent for a given distribution is by rewriting the random variable as $$ X_{(1)} = D_1 + 0\\ X_{(2)} = D_2 + X_{(1)} = D_2 + D_1\\ X_{(3)} = D_3 + X_{(2)} = D3 + D2 + D1\\ $$ and so on. By induction, \\(X_{(i)} = \\sum_{j=1}^{i-1}X_{(j)}\\). Therefore, we can rewrite the joint density of all order statistics as \\(n!f(d_1)f(d_2 + d_1)\\cdot...\\cdot f(d_1 + ... + d_n)I(d_i \\geq 0, \\forall i)\\), which we can then compute and factorize to show independence. 3.6 Convergence What happens when, instead of a set of events or random variables, we observe a sequence of \\(n\\) random variables? There exist two major types of convergence of random variables with which we are typically concerned: convergence in probability and convergence in distribution 3.6.1 Convergence in Probability Definition 3.12 (Convergence in Probability) If \\(Z\\) is a random variable and \\(Z_n\\) is a sequence of random variables, then \\[Z_n \\overset{p}{\\rightarrow} Z \\iff\\lim_{n\\rightarrow\\infty}P(|Z_n - Z| &gt; \\epsilon) = 0\\] If \\(Z\\) is degenerate (meaning it equals a constant value, or \\(Z = c\\)), we can show convergence in probability by disjointifying the absolute value into two disjoint events, and using Kolmogorov Axiom 3. For example, $$ P(|Z_n - c| &gt; ) = P(( Z_n - c &gt; ) (Z_n - c &lt;-)) ( Z_n - c &gt; ), (Z_n - c &lt;-) \\ = P(Z_n &gt; c + ) + P(Z_n &lt; c - ) = 1 - P(Z_n &lt; c + ) + P(Z_n &lt; c - ) $$ From here, we can use the CDF of \\(Z_n\\) to compute the necessary probability. But be careful - this method may not work if the convergence of the CDF cannot be evaluated easily. Convergence in probability has several properties - if we know \\(Z_n\\) convergence in probability, we can use these for solving other problems, especially in Chapter 9 - Point Estimator Asymptotic If \\(A_n \\overset{p}{\\rightarrow} a\\) and \\(B_n\\overset{p}{\\rightarrow}b\\), then \\(A_n + B_n \\overset{p}{\\rightarrow} a + b\\) \\(A_n - B_n \\overset{p}{\\rightarrow} a - b\\) \\(A_n \\cdot B_n \\overset{p}{\\rightarrow} a \\cdot b\\) \\(A_n / B_n \\overset{p}{\\rightarrow} a / b\\) Convergence in probability can also be extended to the multivariate setting, where it takes on a slightly different meaning. Definition 3.13 (Multivariate Convergence in Probability) If \\(X\\) is random vector and \\(X_n\\) is a sequence of random vectors, then \\[\\begin{align} X_n \\overset{p}{\\rightarrow} X \\iff\\lim_{n\\rightarrow\\infty}P(||X_n - X|| &gt; \\epsilon) = 0 \\\\ \\iff X_{jn} \\overset{p}{\\rightarrow}X_j, \\forall j \\in 1,...,k \\end{align}\\] 3.6.2 Convergence in Distribution Definition 3.14 (Convergence in Probability) \\[Z_n \\overset{\\mathcal{D}}{\\rightarrow} Z \\iff\\lim_{n\\rightarrow\\infty}F_n(Z) = F(z), \\forall\\text{ continuity points of }F\\] Note that convergence in probability implies convergence in distribution, but not the converse. That is, \\[Z_n \\overset{p}{\\rightarrow} Z \\implies Z_n \\overset{\\mathcal{D}}{\\rightarrow} Z\\] Unless, that is, the random variable converges in distribution to a constant - then, convergence in distribution does imply convergence in probability! That is, \\[Z_n \\overset{\\mathcal{D}}{\\rightarrow} c \\implies Z_n \\overset{p}{\\rightarrow} c \\] :::{.definition name=“Multivariate Convergence in Distribution”} If \\(X\\) is random vector and \\(X_n\\) is a sequence of random vectors, then \\[X_n \\overset{\\mathcal{D}}{\\rightarrow} X \\iff \\lim_{n\\rightarrow\\infty}F_n(X_1, ..., X_k) = F(X_1,...,X_k), \\forall\\text{ continuity points of }F\\] How do we prove \\(X_n\\) converges in distribution to \\(X\\)? There are three chief strategies: Derive the limiting CDF of \\(X_n\\) and show its limit as \\(n\\rightarrow \\infty\\) equals the \\(F_X(x)\\). Derive the MGF or CF of \\(X_n\\) and show its limit as \\(n\\rightarrow \\infty\\) equals the \\(\\mathcal{M}_X(t)\\). Use the CLT with Slutsky’s Theorem or the CMT (will be discussed later in Chapter 9 - Point Estimator Asymptotics) 3.6.3 Important Theorems Theorem 3.6 (Slutsky's Theorem) If \\(Z_n \\overset{\\mathcal{D}}{\\rightarrow} Z\\) and \\(Y_n \\overset{p}{\\rightarrow} c\\), then \\(Z_n + Y_n \\overset{\\mathcal{D}}{\\rightarrow} Z + c\\) \\(Z_nY_n \\overset{\\mathcal{D}}{\\rightarrow} cZ\\) \\(\\frac{Z_n}{Y_n}\\overset{\\mathcal{D}}{\\rightarrow}\\frac{Z}{c}\\). For problem-solving, Slutsky’s theorem is generally applied whenever we deal with both convergence in probability and convergence in distribution together. Theorem 3.7 (Continuous Mapping Theorem) Suppose \\(Y_n\\) is a sequence of random variables (possibly vectors), \\(Y\\) is a random variable (or vector the same length as \\(Y_n\\)), \\(c\\) is a constant, and \\(g\\) is a function. Then, If \\(Y_n \\overset{p}{\\rightarrow} c\\), and \\(g\\) is continuous at \\(c\\), then \\(g(Y_n) \\overset{p}{\\rightarrow} g(c)\\) If \\(Y_n \\overset{\\mathcal{D}}{\\rightarrow} Y\\), and \\(g\\) is continuous (with \\(g: \\mathbb{R}^k \\mapsto \\mathbb{R}^m\\) for vectors), then \\(g(Y_n) \\overset{\\mathcal{D}}{\\rightarrow} g(Y)\\) (in \\(\\mathbb{R}^m\\) for vectors) References "],["known-distributions.html", "Chapter 4 Known Distributions 4.1 Families of Distributions 4.2 Location and Scale Families (#location-scale) 4.3 Exponential Families 4.4 Known Univariate Exponential Families 4.5 Exponential families with certain parameters fixed 4.6 Non-exponential families 4.7 Multivariate Distributions", " Chapter 4 Known Distributions This will cover proofs and useful properties of commonly used distributions, as well as location-scale and exponential families. 4.1 Families of Distributions Many so-called “distributions” are actually families of distributions, meaning that their pdf involves one or more parameters. That is, their pdfs represent a family of curves, a set of pdfs with variable parameters. For example, the \\(\\text{Normal}(\\mu, \\sigma^2)\\) distribution contains two parameters, \\(\\mu\\) (the mean), and \\(\\sigma^2\\) (the variance). These are also examples of two types of parameters with special properties - called location and scale parameters, respectively - that can be used to simply calculations. 4.2 Location and Scale Families (#location-scale) 4.2.1 Location Families Definition 4.1 (Location Family) Let \\(Z \\sim f_Z(z)\\). Given a constant location parameter \\(b\\), \\(X\\) is a location family if \\(X \\sim f_Z(z - b)\\) or if \\(X = Z + b\\). The two above definitions are equivalent because if \\(X = Z + b\\), then \\(P(X &lt; z) = P(Z + b &lt; z) = P(Z &lt; z - b)\\), so the cdf of \\(Z\\) is \\(F_Z(z - b)\\) and therefore \\(X \\sim f_Z(z - b)\\) (note this makes use of a direct probability argument) 4.2.2 Scale Families Definition 4.2 (Scale Family) Let \\(Z \\sim f_Z(Z)\\). Given a constant scale parameter \\(a\\), \\(X\\) is a scale family if… \\(X \\sim \\frac{1}{a}f_Z(\\frac{z}{a})\\) \\(X \\sim F_Z(\\frac{z}{a})\\) or \\(X = aZ\\) All of the above definitions are equivalent because if \\(X = aZ\\), then \\(P(X &lt; z)\\) = \\(P(aZ &lt; z) = P(Z &lt; \\frac{z}{a} = F_Z(z)\\). Also, \\(f_X(x) = \\frac{d}{dx}F_X(x) = \\frac{d}{dx}F_Z(\\frac{z}{a}) = \\frac{1}{a}f_Z(\\frac{z}{a})\\) 4.2.3 Properties of Location-Scale Families We can compute moments by using general properties of expectation (see Moments) \\(E(X) = aE(Z) + b\\), by linearity of expectation. If the support of \\(Z\\) includes \\(0\\), then we typically define \\(Z\\) such that \\(E(Z) = 0\\) so that \\(E(X) = b\\). \\(Var(X) = a^2Var(Z)\\), since \\(Var(Z + b) = Var(Z)\\). We typically define \\(Z\\) such that \\(Var(Z) = 1\\), so that \\(Var(X) = a^2Var(Z) = a^2\\). An example of this is the standard normal. \\(\\mathcal{M}_X(t) = e^{tb} \\mathcal{M}_Z(at)\\) It may seem like the sum of a scale family should also follow the same family - indeed, this is true for a number of distributions include the Normal, Poisson, and Gamma. However, it is not true always. For instance, \\(X_i \\sim \\text{Uniform}(0,a)\\) is a scale family, but \\(X_1 + X_2\\) does not follow a uniform distribution: X1 = runif(1000, 0, 1) X2 = runif(1000, 0, 1) hist(X1 + X2) 4.3 Exponential Families Definition 4.3 (Exponential Family) \\(X \\sim f_X(x|\\theta)\\) is an exponential family if its pdf can be written in the form \\[f_X(x|\\theta) = h(x)c(\\theta)\\exp\\Big(\\sum_{i=1}^k w_i(\\theta)t_i(x)\\Big)\\] How do we prove that a pdf can be written in the above form? Often, the easiest way is to use a simple trick to get the necessary \\(\\exp\\) function: \\(f(x) = \\exp(\\log(f(x)))\\). Then, we algebraically manipulate to obtain this form. How do we prove that a pdf cannot be written in the above form? FORTHCOMING 4.3.1 Properties Exponential families have a number of incredibly useful properties: Leibniz’s rule holds, meaning that the Cramer-Rao Lower Bound provides a lower bound on the variance of estimators. Among most common families, only exponential families admit sufficient statistics with dimension bounded in \\(n\\). This is proven by the Pitman-Koopman-Darmois theorem for families with smooth, nowhere-vanishing pdfs whose domain does not depend on the parameter being estimated. If \\(X\\) is an exponential family, \\[T(X) = \\Big(\\sum_{i=1}^n t_1(x), ..., \\sum_{i=1}^n t_k(x)\\Big)\\] is a minimal sufficient statistic. Furthermore, if \\(\\{w_1(\\theta),...,w_k(\\theta)\\}\\) contains an open set, then \\(T(X)\\) is a complete sufficient statistic, which we can use to compute an UMVUE. The Method of Moments (MOM) estimator is equal to the Maximum Likelihood Estimator (MLE) The regularity conditions required for consistency and asymptotic normality of the MLE are guaranteed to hold. The family must have a Monotone Likelihood Ratio, meaning that the Karlin-Rubin Theorem may be employed to construct an UMP test. 4.3.2 Natural Exponential Families Definition 4.4 (Natural Exponential Family) \\(X \\sim f_X(x|\\theta)\\) is a natural exponential family if its pdf can be written in the form \\[f_X(x|\\theta) = h(x)c^*(\\boldsymbol{\\eta})\\exp\\Big(\\sum_{i=1}^k \\eta_i t_i(x)\\Big)\\] 4.4 Known Univariate Exponential Families Many common distributions follow exponential families. As you will come to find, virtually all of them arise in order to model variations on a common idea: the Bernoulli trial. Let’s discuss this distribution, and the situations in which it arises. 4.4.1 Bernoulli The Bernoulli distribution, represented mathematically as \\(\\text{Bernoulli}(p)\\), describes the outcome of a random variable \\(X\\) that takes only two possible values, 0 and 1. Such an event is often termed a “Bernoulli trial”. Description Parameters Support pmf Any random variables whose value can be either 0 or 1 \\(0 \\leq p \\leq 1\\) \\(x \\in \\{0, 1\\}\\) \\(p^x(1-p)^{1-x}\\) The Bernoulli distribution occurs very commonly because many situations can be described in terms of 0 or 1 outcomes. For example, all indicator functions \\(I(A)\\) of random variables, where \\(A\\) is a statement about the random variable (for instance, \\(A = \\{x: x &gt; 1\\}\\)) are Bernoulli random variables with \\(p = P(A)\\). The Bernoulli is a special case of the Binomial distribution: \\(\\text{Bernoulli}(p) = \\text{Binomial}(1, p)\\) As we discuss regarding the Binomial distribution, the Bernoulli has an additive property: \\(\\sum_{i=1}^n \\sim \\text{Binomial}(n, p)\\). This can be proven by the additivity technique discussed imminently. If \\(X\\sim \\text{Bernoulli}\\) then \\(E(X) = P(Y = 1)\\), a fact which is often useful for computing moments as well as finding UMVUEs. 4.4.2 Binomial What happens when we repeat a Bernoulli trial many times and count how many 1’s occur? The Binomial distribution is represented mathematically as \\(\\text{Bernoulli}(n, p)\\). It describes the number of successes in a series of Bernoulli trials. Description Parameters Support pmf The number of times an event was successful out of \\(n\\) attempts \\(0 \\leq p \\leq 1\\), \\(n \\in \\mathbb{N}\\) \\(x \\in \\mathbb{N}\\) \\({n\\choose x}p^x(1-p)^{n-x}\\) Example 4.1 (Proving Additive Properties of Distributions) The Binomial distribution is additive: if \\(X \\sim \\text{Binomial}(n,p)\\) and \\(X \\sim \\text{Binomial}(m,p)\\), then \\(X + Y \\sim \\text{Binomial}(m + n, p)\\). One can prove the additivity of any distribution, not just the Binomial, by relying on the convolution property of mgfs: \\(\\mathcal{M}_{X + Y}(t) = \\mathcal{M}_X(t)\\cdot\\mathcal{M}_Y(t)\\). Here’s an example with the Binomial: if \\(X \\sim \\text{Binomial}(n, p)\\) and \\(Y \\sim \\text{Binomial}(m, p)\\), then \\[\\mathcal{M}_{X + Y}(t) = ((1-p) + pe^t)^n\\cdot ((1-p) + pe^t)^m = ((1-p) + pe^t)^{mn}\\] which we can recognize as the mgf of a \\(\\text{Binomial}(n + m, p)\\) distribution. Since, like the cdf and the pdf, the mgf fully characterizes a probability distribution, we’ve proven the additive property mentioned above. Here’s another example with the Binomial, this time generalizing it to an arbitrary summation: If \\(X_i \\overset{iid}{\\sim} \\text{Binomial}(m, p)\\), and \\(Y = \\sum_{i=1}^nX_i\\), then \\[ \\mathcal{M}_Y(t) = \\prod_{i=1}^n\\mathcal{M}_{X_i}(t) = (\\mathcal{M}_{X_i}(t))^n \\\\ = (((1-p) + pe^t)^m)^n = ((1-p) + pe^t)^{mn} \\] proving the generalized additive property that if \\(X_i \\sim \\text{Binomial}(m, p)\\), then \\(\\sum_{i=1}^n X_i \\sim \\text{Binomial}(nm, p)\\). 4.4.3 Geometric Suppose instead of counting the number of successes, we wish to count the number of attempts until a single success occurs? The Geometric distribution is represented mathematically as \\(\\text{Geo}(p)\\). It describes the number of trials before a success occurs in a series of Bernoulli trials. Note that the parametrization below does not include the final success in the number of trials, but alternatives exist in which it may. Description Parameters Support pmf The number of Bernoulli trials attempted before a success occurs \\(0 &lt; p \\leq 1\\) \\(x \\in \\mathbb{N}\\) \\(p(1-p)^{x}\\) The Geometric is a special case of the Negative Binomial: \\(\\text{Geo}(p) = \\text{NegBin}(1, p)\\) Just like the Bernoulli, the Geometric has an additive property: \\(\\sum_{i=1}^n X_i \\sim \\text{NegBin}(n, p)\\), proven via the same addivity technique discussed previously. The Geometric is the only discrete memoryless distribution; that is, for \\(k &gt; i\\), \\(P(X \\geq k | X &gt; i) = P(X \\geq k - i)\\). 4.4.4 Negative Binomial The Negative Binomial distribution generalizes the Geometric distribution to instead represent the number of Bernoulli trials until \\(r\\) successes have occurred. It is represented mathematically as \\(\\text{NegBin}(r, p)\\). Description Parameters Support pmf The number of Bernoulli trials attempted before a success occurs \\(0 &lt; p \\leq 1\\), \\(r \\in \\mathbb{N}\\) \\(x \\in \\mathbb{N}\\) \\({x+r-1\\choose x}p^r(1-p)^{x}\\) The Negative Binomial is additive: If \\(X_i \\sim \\text{NegBin(r, p)}\\), then \\(\\sum_{i=1}^n X_i \\sim \\text{NegBin(nr, p)}\\) 4.4.5 Poisson The Poisson distribution describes one possible behavior of a count random variable. It describes the probability that a certain number of events occur within a fixed interval, such as a time period, distance or area. It is mathematically represented as \\(\\text{Poisson}(\\lambda)\\). Description Parameters Support pmf The number of events occurring in a fixed interval \\(\\lambda \\in (0, \\infty)\\) \\(x \\in \\mathbb{N}_0\\) \\(\\frac{1}{x!}\\lambda^xe^{-\\lambda}\\) Like the Binomial, the Poisson is formulated by counting the number of successes within a set of Bernoulli trials. The distribution describes the asymptotic behavior of the Binomial distribution as \\(n \\rightarrow \\infty\\) and \\(np \\rightarrow \\lambda\\), a fixed rate parameter. The Poisson is additive. If \\(X_i \\sim \\text{Poisson}(\\lambda)\\), then \\(\\sum_{i=1}^n X_i \\sim \\text{Poisson}(n\\lambda)\\) 4.4.6 Normal Often denoted \\(N(\\mu, \\sigma^2)\\), the Normal distribution is especially common in asymptotics - by the Central Limit Theorem, the sample mean converges in distribution to a normal. Many other variables also converge to a normal. For \\(X \\sim N(\\mu, \\sigma^2)\\), the mean is \\(\\mu\\) and the variance is \\(\\sigma^2\\). This means that the Normal is a location-scale family. Description Parameters Support pdf Describes the asymptotic behavior of sample means and many distributions \\(\\mu \\in \\mathbb{R}\\), \\(\\sigma^2 \\in (0, \\infty)\\) \\(x \\in \\mathbb{R}\\) \\(\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\Big(-\\frac{1}{2}\\frac{(x-\\mu)^2}{\\sigma^2}\\Big)\\) The following distributions converge to a Normal: \\(\\text{Binomial}(n, p) \\approx N(np, np(1-p))\\) for large \\(n\\) and \\(p\\) bounded away from 0 or 1 \\(\\text{Pois(\\lambda)} \\approx N(\\lambda, \\lambda)\\) for large \\(\\lambda\\). \\(\\chi^2(\\nu)\\approx N(\\nu, 2\\nu)\\) for large \\(\\nu\\). \\(t(\\nu) \\approx N(0, 1)\\) for large \\(\\nu\\). The Normal is additive. If \\(X_1 \\sim N(\\mu_1, \\sigma_1^2)\\) and \\(X_2 \\sim N(\\mu_2, \\sigma_2^2)\\), then \\(X + Y \\sim N(\\mu_1 + \\mu_2, \\sigma_1^2 + \\sigma_2^2)\\). 4.4.7 Exponential The Poisson process that models the number of events occuring in an interval also gives rise to another distribution: the Exponential. This distribution models the size of the interval (time, distance, etc.) between events. The Exponential is described mathematically as \\(\\text{Exp}(\\lambda)\\). It can be parametrized in two ways: with \\(\\lambda\\) as a rate parameter, describing how often events occur; or with \\(\\lambda\\) as a scale parameter (yes, the same scale parameter discussed in Location Scale Families) that is the inverse of the rate. Hence, the Exponential is a scale family. Note that the parametrization below describes the distribution in terms of the scale parameter. The scale parameter version simply replaces \\(\\lambda\\) with \\(\\frac{1}{\\lambda}\\). Description Parameters Support pdf Models the size of the interval between events in a Poisson process \\(\\lambda \\in (0, \\infty)\\) \\(x \\in (0, \\infty)\\) \\(\\lambda e^{-\\lambda x}\\) The Exponential is a special case of the Gamma distribution: \\(\\text{Exp}(\\lambda) = \\text{Gamma}(1, \\lambda)\\) By extension, the exponential has an additive property: If $X_iExp() $, then \\(\\sum_{i=1}^n X_i \\sim \\text{Gamma}(n, \\lambda)\\). The Exponential is the only continuous memoryless distribution; that is, for \\(k &gt; i\\), \\(P(X \\geq k | X &gt; i) = P(X \\geq k - i)\\). We can prove the memorylessness of the Exponential using conditional probability: \\[ P(X &gt; s | X &gt; t) = \\frac{P(X &gt; s, X &gt; t)}{P(X &gt; t)} = \\frac{P(X &gt; s)}{P(X &gt; t)} \\] since \\(s &gt; t\\). This equals \\(\\frac{e^{-\\lambda s + 1}}{e^{-\\lambda t + 1}} = e^{\\lambda(s - t)} = P(X &gt; s - t)\\) 4.4.8 Gamma Similar to how the Negative Binomial generalizes the Geometric to multiple successes, the Gamma generalizes the Exponential to multiple events. The Gamma is useful for modeling random variables that are known to be greater than 0. It is represented mathematically as \\(\\text{Gamma}(k, \\lambda)\\), where \\(k\\) is a shape parameter and \\(\\lambda\\) is a scale parameter. This means the Gamam is a scale family. It is called “Gamma” because it involves the gamma function. Description Parameters Support pdf Generalization of the exponential distribution \\(k \\in (0, \\infty)\\), \\(\\lambda \\in (0, \\infty)\\) \\(x \\in (0, \\infty)\\) \\(\\frac{1}{\\Gamma(k)\\lambda^k}x^{k-1}\\exp(-\\frac{x}{\\theta})\\) The Gamma is additive: If $X_i(k, ) $, then \\(\\sum_{i=1}^n X_i \\sim \\text{Gamma}(nk, \\lambda)\\). 4.4.9 Beta The Beta distribution models proportions. It is mathematically denoted \\(\\text{Beta}(\\alpha, \\beta)\\) where \\(\\alpha\\) and \\(\\beta\\) are two shape parameters. It is known as “Beta” because its pdf contains the beta function: \\(B(\\alpha, \\beta) = \\frac{\\Gamma(\\alpha)\\Gamma(\\beta)}{\\Gamma(\\alpha + \\beta)}\\) Description Parameters Support pdf Model of a proportion \\(\\lambda \\in (0, \\infty)\\) \\(x \\in (0, 1)\\) \\(\\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)}x^{\\alpha-1}(1-x)^{\\beta-1}\\) 4.4.10 Chi-squared The Chi-squared distribution describes the distribution of the sum of squared standard Normal (\\(N(0,1)\\)) random variables. As a result, it is useful in asymptotics, especially asymptotic hypothesis testing, since if an estimator is asymptotically normal, its square is asymptotically \\(\\chi^2\\). It is represented mathematically as \\(\\chi^2(\\nu)\\) or \\(\\chi^2_\\nu\\) where \\(\\nu\\) is the “degrees of freedom” - the number of squared normal random variables in the sum. Description Parameters Support pdf Squared standard normals \\(\\nu \\in \\mathbb{N}\\) \\(x \\in (0, \\infty)\\) \\(\\frac{1}{\\Gamma(\\nu/2)2^{\\nu / 2}}x^{\\nu/2 - 1}\\exp(-\\nu/2)\\) If \\(X_i \\sim N(0,1)\\), then \\(\\sum_{i=1}^nX_i^2\\sim \\chi^2(n)\\) The Chi-squared distribution is a special case of the Gamma. That is, if \\(X \\sim \\chi^2(\\nu)\\), then \\(X \\sim \\text{Gamma}(\\frac{\\nu}{2}, \\frac{1}{2})\\). This can be observed directly from the pdf. 4.5 Exponential families with certain parameters fixed Some families are exponential, but only when one or more parameters are fixed. 4.5.1 Weibull The Weibull is a generalization of the exponential distribution. The extra parameter \\(k\\) describes how the failure rate changes over time. It is an exponential family when \\(k\\) is fixed. Like the exponential, \\(\\lambda\\) is a scale parameter, meaning that the Weibull is a scale family. Description Parameters Support pdf Models time-to-event variables \\(\\lambda \\in (0, \\infty)\\), \\(k \\in (0, \\infty)\\) \\(x \\in [0, \\infty)\\) \\(\\frac{k}{\\lambda}\\Big(\\frac{x}{\\lambda}\\Big)^{k-1}\\exp(-(x/\\lambda)^k)\\) When \\(k = 1\\), the Weibull is equal to an \\(\\text{Exponential}(\\frac{1}{\\lambda})\\), which can be observed directly from its pdf. 4.5.2 Pareto The Pareto distribution, written \\(\\text{Pareto}(x_m, \\alpha)\\), models variables involving power-law relationships. \\(\\alpha \\in (0, \\infty)\\) is a shape parameter, while \\(x_m\\) is a scale parameter. This means that the Pareto is a scale family. It is an exponential family when \\(x_m\\) is fixed. Description Parameters Support pdf Power-law models \\(\\alpha \\in (0, \\infty)\\), \\(x_m \\in (0, \\infty)\\) \\(x \\in [0, \\infty)\\) \\(\\frac{\\alpha x_m^\\alpha}{x^{\\alpha+1}}\\) The Pareto is related to the Exponential. If \\(X\\sim \\text{Pareto}(x_m, \\alpha)\\), then \\(Y = \\log(\\frac{X}{x_m}) \\sim \\text{Exp}(\\alpha)\\) 4.6 Non-exponential families The following families are not exponential, but still commonly arise. 4.6.1 Uniform The Uniform distribution is parametrized by its minimum \\(a\\) and maximum \\(b\\). Denoted \\(U(a,b)\\), it describes a scenario where every possible value of \\(x\\) has the same probability. Description Parameters Support pdf Every \\(x\\) has same probability \\(-\\infty &lt; a &lt; b &lt; \\infty\\) \\(x \\in [a, b]\\) \\(\\frac{1}{b-a}\\) If \\(X \\sim \\text{Beta}(1,1)\\), then \\(X \\sim U(0,1)\\) If \\(X \\sim U(0,1)\\), then \\(-\\lambda \\log(X) \\sim \\text{Exp}(\\lambda)\\). If \\(X_i \\overset{iid}{\\sim} U(0,1)\\), then \\(X_{(k)} - X_{(j)} \\sim \\text{Beta}(k - j, n - (k - j) + 1)\\) By the Probability Integral Transform, inverse cdfs always follow a standard uniform distribution. That is, if \\(X = F_X^{-1}(Y)\\), then \\(Y \\sim U(0,1)\\). This can be used to generate any random variable with a known cdf. 4.6.2 Cauchy The Cauchy arises in situations involving ratios of standard normal variables, as well as rotations. It is \\(\\text{Cauchy}(x_0, \\gamma)\\), where \\(x_0\\) is a location parameter and \\(\\gamma\\) is a scale parameter, making it a location-scale family. Description Parameters Support pdf Rotations and ratios of normals \\(x_0 \\in \\mathbb{R}\\), \\(\\gamma \\in (0, \\infty)\\) \\(x \\in \\mathbb{R}\\) \\(\\frac{1}{\\pi \\gamma\\Big(1 + \\Big(\\frac{x - x_0}{\\gamma}\\Big)^2\\Big)}\\) If \\(U, V \\sim N(0,1)\\) independently, then \\(U/V \\sim Cauchy(0,1)\\) The Cauchy is often used as a pathological example in statistical problems, since it famously has no mean or variance (\\(E(X) = Var(X) = \\infty\\)) If \\(X\\sim t(1)\\), then \\(X\\sim \\text{Cauchy}(0,1)\\) 4.6.3 t-distribution The t-distribution describes the distribution of the t-statistic for \\(X_1, ... X_n \\overset{iid}{\\sim}N(\\mu, \\sigma^2)\\) \\[t = \\frac{\\bar{X} - \\mu}{\\sqrt{S^2/n}}\\] As a result, it is commonly used in hypothesis testing. Denoted \\(t(\\nu)\\), the parameter \\(\\nu\\) represents the degrees of freedom - the number of \\(X_i\\) in the sample that are being summed in the computation of \\(\\bar{X}\\) and \\(S^2\\). Description Parameters Support pdf Distribution of the t-statistic \\(\\nu \\in \\mathbb{N}\\) \\(x \\in \\mathbb{R}\\) \\(\\frac{\\Gamma((\\nu + 1) / 2)}{\\sqrt{\\nu\\pi}\\Gamma(\\nu/2)}\\Big(1 + \\frac{x^2}{\\nu}\\Big)^{-(\\nu+1)/2}\\) As \\(\\nu \\rightarrow \\infty\\), the t-distribution converges to a \\(N(0,1)\\). If \\(X \\sim t(1)\\) then \\(X \\sim \\text{Cauchy}(0,1)\\). 4.6.4 F-distribution Also useful for [hypothesis testing]](#hypothesis-tests-finite-samples), the F-distribution, denoted \\(F(n, m)\\), describes the distribution of the F-statistic: \\[X = \\frac{S_1 / n}{S_2 / m}\\] where \\(S_1\\) and \\(S_2\\) are the sums of independent standard normal random variables with degrees of freedom \\(n\\) and \\(m\\), respectively - that is, \\(S_1 \\sim \\chi^2(n)\\) and \\(S_2 \\sim \\chi^2(m)\\). Description Parameters Support pdf Distribution of the \\(F\\)-statistic \\(n \\in \\mathbb{N}\\), \\(m \\in \\mathbb{N}\\) \\(x \\in (0, \\infty)\\) \\(\\sqrt{\\frac{(nx)^nm^m}{(nx + m)^{n + m}}}\\frac{\\Gamma(n + m)}{x\\Gamma(n)\\Gamma(m)}\\) If \\(X \\ F(n, m)\\), then \\(\\frac{1}{X} \\sim F(m, n)\\) If \\(X \\sim t(n)\\), then \\(X^2 \\sim F(1, n)\\) If \\(X \\sim \\chi^2(n)\\) and \\(Y \\sim \\chi^2(m)\\), then \\(\\frac{X / n}{Y / m} \\sim F(n, m)\\) If \\(X_i \\overset{iid}{\\sim} \\text{Gamma}(\\alpha_i, \\beta_i)\\), then \\(\\frac{\\alpha_2\\beta1X_1}{\\alpha_1\\beta_2X_2} \\sim F(2\\alpha_1, 2\\alpha_2)\\) If \\(X \\sim \\text{Beta}(n/2, m/2)\\), then \\(\\frac{mX}{n(1-X)}\\sim F(n, m)\\) 4.6.5 Hypergeometric Imagine drawing \\(n\\) samples without replacement from a finite population of size \\(N\\). Suppose \\(K\\) of the units in the population are considered “successes” if drawn. The Hypergeometric distribution, denoted \\(\\text{HGeo}(N, K, n)\\), describes the probability that you will draw \\(x\\) “successes” under these circumstances. Description Parameters Support pmf Sampling without replacement \\(N \\in \\mathbb{N}_0\\), \\(K \\in \\{0, 1, ..., N\\}\\), \\(n \\in \\{0,1,...,N\\}\\) \\(x \\in \\{\\max(0, n+K-N),..., \\min(n,K)\\}\\) \\(\\frac{{K\\choose x}{N - K \\choose n - x -1}}{N \\choose n}\\) Fisher’s Exact Test is based on the Hypergeometric distribution. If \\(X \\sim \\text{HGeo}(N, K, n)\\), and \\(N\\) and \\(K\\) are sufficiently large compared to \\(n\\), then \\(X \\approx \\text{Binom}(n, p)\\) 4.7 Multivariate Distributions 4.7.1 Bivariate Normal The bivariate normal describes a situation where two random variables \\(X\\) and \\(Y\\) are normally distributed, and their sum is also normally distributed. Description Parameters Support pmf Two-dimensional normal \\(\\mu_x, \\mu_y \\in \\mathbb{R}\\), \\(\\sigma_x, \\sigma_y \\in \\mathbb{R} &gt; 0\\), \\(\\rho \\in [-1, 1]\\) \\(x\\in\\mathbb{R}^2\\) \\(\\frac{1}{2\\pi\\sigma_x\\sigma_y\\sqrt{1-\\rho^2}}\\exp\\Big(-\\frac{1}{2(1-\\rho^2)}\\Big((\\frac{x-\\mu_x}{\\sigma_x})^2 - 2\\rho(\\frac{x - \\mu_x}{\\sigma_x})(\\frac{y - \\mu_y}{\\sigma_y}) + (\\frac{y-\\mu_y}{\\sigma_y})^2\\Big)\\Big)\\) Suppose that \\((X, Y)\\) follows the bivariate normal distribution above. Then, The marginal distributions are \\(X \\sim N(\\mu_x, \\sigma_x^2)\\) and \\(Y \\sim N(\\mu_y, \\sigma_y^2)\\) \\(Corr(X, Y) = \\rho\\) Any linear combination of \\(X\\) and \\(Y\\) is univariate normal. That is, \\(aX + bY \\sim N(a\\mu_x + b\\mu_y, a^2\\sigma_x^2 + b^2\\sigma_y^2 +2ab\\rho\\sigma_x\\sigma_y\\). The conditional distribution \\(Y|X = x \\sim N\\Big(\\mu_Y + \\frac{\\sigma_Y}{\\sigma_X}\\rho(x - \\mu_X), \\sigma_Y^2(1-\\rho^2)\\Big)\\) 4.7.1.1 Multivariate Normal The Multivariate Normal, often denoted \\(MVN(\\mu, \\Sigma)\\), generalizes the normal to a random vector \\(X\\) where all linear combinations of its components have a univariate normal distribution. Description Parameters Support pmf \\(k\\)-dimension normal \\(\\mu \\in \\mathbb{R}^k\\), \\(\\Sigma \\in \\mathbb{R}^{k \\times k}\\) \\(x\\in\\mathbb{R}^k\\) \\(\\frac{1}{\\sqrt{(2\\pi)^{k}\\det(\\Sigma)}}\\exp\\Big(-\\frac{1}{2}(x - \\mu)^\\top\\Sigma^{-1}(x-\\mu)\\Big)\\) If a vector \\(X\\sim MVN(\\mu, \\Sigma)\\), then All marginal distributions of \\(X\\) follow a multivariate normal with the marginalized means and rows/columns in the covariance matrix dropped. For example, \\(X_1 \\sim N(\\mu_1, \\sigma_1)\\) If \\(Y = c + BX\\), then \\(Y \\sim MVN(c + B\\mu, B\\Sigma B^\\top)\\) Note that two normally distributed random variables may not be jointly bivariate normal! 4.7.2 Multinomial Consider an event in which one of \\(k\\) discrete outcomes is guaranteed to occur - for instance, rolling a 6-sided die, where there are \\(k = 6\\) possible outcomes. Repeat this event \\(n\\) times. If outcome \\(i\\) occurs with probability \\(p_i\\) (not necessarily equal), then the number of times \\(X_i\\) that each outcome \\(i = 1,...,k\\) occurs after \\(n\\) trials is modeled by the Multinomial distribution. \\(X\\) is a vector representing the number of successes of each event. Description Parameters Support pmf \\(k\\) joint binomials \\(n \\in \\mathbb{N}\\), \\(k \\in \\mathbb{N}\\), \\(p_1,...,p_k\\in (0,1)\\), \\(\\sum_{i=1}^kp_i = 1\\) \\(x_i\\in \\mathbb{N}\\), \\(\\sum_{i=1}^kx_i = n\\) \\(\\frac{n!}{\\prod_{i=1}^kx_i!}\\prod_{i=1}^kp_i^{x_i}\\) If the vector \\(X\\) is multinomial, then All marginal distributions of \\(X\\) are multinomial All conditional distributions are multinomial \\(X_i \\sim \\text{Binomial}(n, p_i)\\) \\(Cov(X_i, X_j) = E((X_i - mp_i)(X_j - mp_j)) = -mp_ip_j, \\forall i\\neq j\\) "],["new-distributions.html", "Chapter 5 New Distributions 5.1 Transformations 5.2 Computing Joint Probabilities 5.3 Probability Integral Transform", " Chapter 5 New Distributions This chapter will demonstrate how to derive the PDF or CDF of a random variable that is a function of other random variables, including hierarchical models. 5.1 Transformations 5.1.1 Theorems We’ll start by introducing two useful theorems for finding the PDF of a transformed random variable \\(Y=g(X)\\) where \\(g(X)\\) is a monotone, one-to-one function over the domain of interest.   Definition 5.1 (Theorem 2.1.5) Let \\(X\\) have pdf \\(f_X(x)\\) and let \\(Y=g(X)\\), where \\(g\\) is a monotone function. Let \\(\\mathcal{X}\\) and \\(\\mathcal{Y}\\) be defined by (2.1.7). Suppose that \\(f_X(x)\\) is continuous on \\(\\mathcal{X}\\) and that \\(g^{-1}(y)\\) has a continuous derivative on \\(\\mathcal{Y}\\). Then the PDF of \\(Y\\) is given by \\[ f_Y(y)= \\begin{cases}f_X\\left(g^{-1}(y)\\right)\\left|\\frac{d}{d y} g^{-1}(y)\\right| &amp; y \\in \\mathcal{Y} \\\\ 0 &amp; \\text { otherwise}\\end{cases} \\] A non-rigorous sketch of the proof of this follows from the Direction Probability Manipulation of the CDF described in Chapter 2. Note \\[ F(Y &lt; y) = P(Y &lt; y) = P(g(X) &lt; y) \\\\ = P(X &lt; g^{-1}(y)) = F_x(g^{-1}(y)) \\] Then, \\(f(y) = \\frac{d}{dy}F_Y(y) = f_X(g^{-1}(y)\\cdot|\\frac{d}{dy}g^{-1}(y)|\\) by the chain rule (the absolute value is necessary to handle the case where \\(g\\) is decreasing) In some cases, \\(g(X)\\) will only be a monotone, one-to-one function over subsets of the domain of interest. Below, Theorem 2.1.8 provides a generalization of Theorem 2.1.5 that can be applied in these situations. Definition 5.2 (Theorem 2.1.8) Let \\(X\\) have PDF \\(f_X(x)\\), let \\(Y=g(X)\\), and define the sample space \\(\\mathcal{X}\\) as in (2.1.7). Suppose there exists a partition, \\(A_0, A_1, \\ldots, A_k\\), of \\(\\mathcal{X}\\) such that \\(P\\left(X \\in A_0\\right)=0\\) and \\(f_X(x)\\) is continuous on each \\(A_i\\). Further, suppose there exist functions \\(g_1(x), \\ldots, g_k(x)\\), defined on \\(A_1, \\ldots, A_k\\), respectively, satisfying \\(g(x)=g_i(x)\\), for \\(x \\in A_i\\), \\(g_i(x)\\) is monotone on \\(A_i\\), the set \\(\\mathcal{Y}=\\left\\{y: y=g_i(x)\\right.\\) for some \\(\\left.x \\in A_i\\right\\}\\) is the same for each \\(i=1, \\ldots, k\\), and \\(g_i^{-1}(y)\\) has a continuous derivative on \\(\\mathcal{Y}\\), for each \\(i=1, \\ldots, k\\). Then \\[ f_Y(y)= \\begin{cases}\\sum_{i=1}^k f_X\\left(g_i^{-1}(y)\\right)\\left|\\frac{d}{d y} g_i^{-1}(y)\\right| &amp; y \\in \\mathcal{Y} \\\\ 0 &amp; \\text { otherwise }\\end{cases} \\] 5.1.2 Practical Strategy Now, we will outline a practical strategy for solving problems that involve transformations. Given a transformation \\(y=g(x)\\) (univariate) or \\(u,v=g(x,y)\\) (multivariate), perform a transformation by following these steps: Step 1: Define PMF/PDF of the untransformed random variable(s). Univariate: \\(f(x)\\) (given) Multivariate: calculate joint density \\(f(x,y)\\) (not always given) Step 2: Find the inverse. Univariate: \\(y=g(x) \\quad \\Rightarrow \\quad x=g^{-1}(y)\\) Multivariate: \\(u=g(x,y), v=g(x,y) \\quad \\Rightarrow \\quad x=g^{-1}(u,v),y=g^{-1}(u,v)\\) (Note: for multivariate case, solve system of equations) Step 3: Calculate jacobian of the transformation. For transformations of multiple random variables, recall bivariate Jacobian: \\[ |J|=\\left|\\begin{array}{ll} \\frac{\\partial x}{\\partial u} &amp; \\frac{\\partial x}{\\partial v} \\\\ \\frac{\\partial y}{\\partial u} &amp; \\frac{\\partial y}{\\partial v} \\end{array}\\right|=\\left| \\frac{\\partial x}{\\partial u} \\frac{\\partial y}{\\partial v}-\\frac{\\partial y}{\\partial u} \\frac{\\partial x}{\\partial v} \\right| \\] Step 4: Calculate PMF/PDF of the transformed random variable. \\[ f_Y(y)= f_X\\left(g^{-1}(y)\\right)\\left|\\frac{d}{d y} g^{-1}(y)\\right| \\] \\[ f_{U,V}(u,v)= f_{X,Y}\\left(g^{-1}(u,v)\\right) \\left|\\begin{array}{ll} \\frac{\\partial x}{\\partial u} &amp; \\frac{\\partial x}{\\partial v} \\\\ \\frac{\\partial y}{\\partial u} &amp; \\frac{\\partial y}{\\partial v} \\end{array}\\right| \\] Example 5.1 (Chi-squared density) Suppose \\(Z \\sim N(0,1)\\). What is the distribution of \\(Y = Z^2\\)? Here, \\(g\\) is not monotone. If we want to proceed using the Transformation Theorem, we need split up the domain into areas where \\(g\\) is monotone. This, trivially, is \\(z &lt; 0\\) and \\(z &gt; 0\\). Then, \\[g(z) = z^2 \\implies z = g^{-1}(y) = \\begin{cases}-\\sqrt{y} &amp; z &lt; 0\\\\\\sqrt{y} &amp; z&gt;0\\end{cases}\\]. An intuition for why we need to sum the pdfs for non-monotone functions is that \\(Y = Z^2\\) maps multiple values of \\(Z\\) to the same value of \\(Y\\). For example, \\(g(z)\\) maps both \\(Z = -1\\) and \\(Z = 1\\) to \\(Y = 1\\). Therefore, \\(f_Y(1)\\) needs to include both the probability of \\(Z = -1\\) and \\(Z = 1\\). Hence, we apply the Non-Monotone Transformation Theorem to obtain \\[ f_Y(y) = f_Z(\\sqrt{y})\\Big|\\frac{d}{dy}\\sqrt{y}\\Big| + f_Z(-\\sqrt{y})\\Big|\\frac{d}{dy}-\\sqrt{y}\\Big|\\\\ = \\frac{1}{\\sqrt{2\\pi}}\\cdot \\exp(-\\frac{1}{2}(\\sqrt{y})^2\\cdot\\frac{1}{2\\sqrt{y}} + \\frac{1}{\\sqrt{2\\pi}}\\cdot \\exp(-\\frac{1}{2}(-\\sqrt{y})^2\\cdot\\frac{1}{2\\sqrt{y}}\\\\ = \\frac{1}{\\sqrt{2\\pi}y^\\frac{1}{2}}\\exp(-\\frac{y}{2}) \\] which is a \\(\\chi^2(1)\\) distribution. Alternatively, we could have proceeded directly using Direct Probability Manipulation like so: \\[ F(y) = P(Y \\leq y) = P(Z^2 \\leq y)\\\\ = P(-\\sqrt{y}&lt; Z &lt; \\sqrt{y}) = \\Phi(\\sqrt{y}) - \\Phi(-\\sqrt{y}) \\] where \\(\\Phi(z)\\) is the normal cdf. Then, \\[ f_Y(y) = \\frac{d}{dy}F_Y(y) = f_y(\\sqrt{y})\\cdot\\frac{d}{dy}{\\sqrt{y}} - f_y(\\sqrt{y})\\cdot(\\frac{d}{dy}{-\\sqrt{y}})\\\\ = \\frac{1}{\\sqrt{2\\pi}y^\\frac{1}{2}}\\exp\\Big(-\\frac{y}{2}\\Big) \\] the same result as the theorem. 5.1.3 Proving Independence From a Joint Transformation Some problems may involve proving that two transformations \\(U = g_1(x,y)\\) and \\(V = g_2(x,y)\\) are independent. To do this, we can first compute the joint distribution using the Transformation Theorem, and then factorize the joint distribution into components containing only \\(u\\) and \\(v\\) respectively. Example 5.2 (Independence From a Transformation) test 5.2 Computing Joint Probabilities Besides finding the joint pdf of a function, one may also be asked to compute a probability involving two random variables, such as \\(P(X &gt; Y)\\), given a joint distribution \\(f_{X, Y}(x,y)\\). Solving these is generally a two-step process: Define the Region of interest. This will be a set \\(\\{(x, y)\\}\\) with restriction on \\(x\\) and \\(y\\) given in the problem. For example, to compute \\(P(X &gt; Y)\\) where \\(x, y \\in [-1, 1]\\), it might take the form \\(\\{(x,y) : -1 &lt; y &lt; x &lt; 1\\}\\). Set up and solve a double integral over the region of interest. Following the above, you might write \\[P(X &gt; Y) = \\int_{-1}^1\\int_{-1}^x f(x,y)dydx\\] Setting up the bounds of these integrals can be very tricky, so make sure they are computed carefully! 5.3 Probability Integral Transform The probability integral transform is a powerful technique that allows for the simulation of random variables from any distribution, given that you have access to a source of uniformly distributed random numbers. This method is particularly useful because it provides a straightforward way to transform uniform random variables into random variables following a desired distribution. The general idea is as follows: When you plug any continuous random variable \\(X\\) into its own CDF, you get \\(U \\sim\\) Uniform(0,1): \\[ F_X(X) \\sim \\text{Uniform} (0,1) \\] When you plug \\(U \\sim\\) Uniform(0,1) into an inverse CDF, you get a continuous random variable \\(X\\) with that CDF: \\[ F_X^{-1}(U) \\text{ has CDF } F_X(x) \\] Procedure: Construct a random variable \\(X\\) with a particular CDF \\(F_X(x)\\) Generate \\(U \\sim\\) Uniform(0,1) Plug \\(U\\) into the inverse CDF: \\(X=F^{-1}_X(U)\\) \\(X\\) is distributed according to the CDF \\(F_X(x)\\)   Click Here: Visual Explanation of Universality of the Uniform 5.3.1 Hiearchical Models (Iterated Moments) Definition 5.3 (Theorem 4.4.3 and Theorem 4.4.7) If \\(X\\) and \\(Y\\) are any two random variables and the relevant expectations exist, then \\[ \\begin{aligned} E_X(X) &amp; =E_Y\\left(E_{X \\mid Y}(X \\mid Y)\\right) \\\\ \\operatorname{Var}_X(X) &amp; =E_Y\\left(\\operatorname{Var}_{X \\mid Y}(X \\mid Y)\\right)+\\operatorname{Var}_Y\\left(E_{X \\mid Y}(X \\mid Y)\\right) \\end{aligned} \\] 5.3.2 Convolutions Convolutions arise when we calculate the probability distribution of the sum or linear combination of independent random variables. In the following table, we provide a compilation of convolutions involving random variables from well-established probability distributions. Acquainting yourself with these convolutions can significantly improve the efficiency of computing moment generating functions and other statistical expressions of interest. \\[ \\begin{array}{l|l} f_{X_i} &amp; f_{\\Sigma X_i}\\left(X_i \\text { independent }\\right) \\\\ \\hline \\operatorname{Ber}(p) &amp; \\operatorname{Bin}(n, p) \\\\ \\operatorname{Bin}\\left(n_i, p\\right) &amp; \\operatorname{Bin}\\left(\\sum_i n_i, p\\right) \\\\ \\operatorname{Geo}(p) &amp; \\operatorname{NBin}(n, p) \\\\ \\operatorname{Exp}(\\beta) &amp; \\operatorname{Gam}(n, \\beta) \\quad \\quad \\text { Note: } E\\left(X_i\\right)=\\frac{1}{\\beta}, E\\left(\\sum X_i\\right)=\\frac{n}{\\beta} \\\\ \\operatorname{Gam}\\left(n_i, \\beta\\right) &amp; \\operatorname{Gam}\\left(\\sum_i n_i, \\beta\\right) \\\\ \\operatorname{Pois}\\left(\\lambda_i\\right) &amp; \\operatorname{Pois}\\left(\\sum_i \\lambda_i\\right) \\\\ \\chi^2(1) &amp; \\chi^2(n) \\\\ \\chi^2\\left(n_i\\right) &amp; \\chi^2\\left(\\sum_i n_i\\right) \\\\ \\operatorname{Norm}\\left(\\mu_i, \\sigma_i^2\\right) &amp; \\operatorname{Norm}\\left(\\sum_i \\mu_i, \\sum_i \\sigma_i^2\\right) \\end{array} \\]   Convolutions can also be computed directly as follows: Definition 5.4 (Theorem) Let \\(X\\) and \\(Y\\) be two independent continuous random variables with possible values \\(\\mathcal{X}\\) and \\(\\mathcal{Y}\\) and let \\(Z=X+Y\\). Then, the probability density function of \\(Z\\) is given by \\[ \\begin{aligned} f_Z(z) &amp; =\\int_{-\\infty}^{+\\infty} f_X(z-y) f_Y(y) \\mathrm{d} y \\\\ \\text { or } \\quad f_Z(z) &amp; =\\int_{-\\infty}^{+\\infty} f_Y(z-x) f_X(x) \\mathrm{d} x \\end{aligned} \\] where \\(f_X(x), f_Y(y)\\) and \\(f_Z(z)\\) are the probability density functions of \\(X, Y\\) and \\(Z\\). Tip: For convolutions, this theorem provides an alternative to the multivariate Jacobian above.   Lastly, we can apply the below theorem to easily compute the moment generating function and characteristic function of a convolution: Definition 5.5 (Theorem 4.2.12) Let \\(X\\) and \\(Y\\) be independent random variables with moment generating \\(M_X(t)\\) and \\(M_Y(t)\\). Then the moment generating and characteristic functions of the random variable \\(Z=X+Y\\) are given by \\[ M_Z(t)=M_X(t) \\cdot M_Y(t) \\] \\[ \\varphi_Z(t) =\\varphi_X(t) \\cdot \\varphi_Y(t) \\] For the random variable \\(Z = X-Y\\), the moment generating and characteristic functions are given by: \\[ M_Z(t)=M_X(t) \\cdot M_Y(-t) \\] \\[ \\varphi_Z(t) =\\varphi_X(t) \\cdot \\varphi_Y(-t) \\] "],["moments.html", "Chapter 6 Moments 6.1 Basic Definitions 6.2 \\(E(X)\\) Properties 6.3 \\(Var(X)\\) Properties 6.4 Covariance and Correlation 6.5 Conditional Expectation 6.6 Moment Generating Functions 6.7 Moment Inequalities 6.8 Techniques for Deriving Moments 6.9 Other Moments (for reference)", " Chapter 6 Moments Finding the moments of a random variable is a chief problem in statistics. This is because moments characterize important properties about a distribution - for example, the mean measures the central tendency of a random variable, while the variance measures its dispersion. This chapter will define expected value and moments, summarize their useful properties, and discuss strategies for finding moments, especially for common distributions. 6.1 Basic Definitions Definition 6.1 (Expected Value) The expected value \\(E(g(X))\\) is the average value of a random variable \\(g(X)\\) across its support \\(\\mathcal{X}\\), weighted by their probability. If \\(X\\) is discrete, then this is defined \\[E(g(X)) = \\sum_{x \\in \\mathcal{X}}g(x)P(X = x)\\] If \\(X\\) is continuous, then the expected value is \\[E(g(x)) = \\int_{\\mathcal{X}}g(x)f_X(x)dx\\] Definition 6.2 (Multivariate Expected Value) If \\(X_1, X_2,...X_n\\) are discrete, the expected value over a function of multiple random variables is defined as \\[E(g(X_1, X_2,...X_n)) = \\sum_{x_1}\\sum_{x_2}...\\sum_{x_n}g(x_1, x_2, ..., x_n)P(X_1 = x_1, X_2 = x_2, ...X_n = x_n)\\] If \\(X_1, X_2,...X_n\\) are continuous, the expected value is instead \\[E(g(X_1, X_2,...X_n)) = \\int_{x_1\\in \\mathcal{X}_1}\\int_{x_2\\in \\mathcal{X}_2}...\\int_{x_n\\in \\mathcal{X}_n}g(x_1, x_2, ..., x_n)f_{X_1, X_2,...,X_n}(x_1, x_2, ..., x_n)\\] Definition 6.3 (Moments) The \\(n\\)th moment of a random variable \\(X\\) is defined as \\(E(X^n)\\). Similarly, the \\(n\\)th central moment is defined as \\(E((X - E(X))^n)\\). The first moment \\(E(X)\\) is also known as the mean. The second central moment is the variance, denoted \\(Var(X) = E((X - E(X))^2)\\) 6.2 \\(E(X)\\) Properties The linearity of expectation is defined as \\(E(aX + b) = aE(X) + b\\). If multiple random variables \\(X\\) and \\(Y\\) are involved, then \\[E(ag_1(X) + bg_2(Y) + c) = aE(g_1(X)) + bE(g_2(Y)) + c\\] This follows from the linearity of the integral operator. Since sums of random variables are so common, this property is incredibly useful, especially for proving the unbiasedness of estimators (see Chapter 8). For example, we can use the linearity of expectation to prove that the sample mean \\(\\bar{X} = \\frac{1}{n}\\sum_{i=1}^nX_i\\) is unbiased for a set of iid \\(X_i\\) by noting, by linearity of expectation, \\[E(\\bar{X}) = E\\Big(\\frac{1}{n}\\sum_{i=1}^nX_i\\Big) = \\frac{1}{n}\\sum_{i=1}^nE(X_i) = \\frac{n}{n}E(X_i) = E(X_i)\\] When \\(X\\) and \\(Y\\) are independent, \\[E(XY) = E(X)E(Y)\\] This property can also be useful for computing the expectation of iid random variables in statistical inference problems. 6.3 \\(Var(X)\\) Properties The most important variance property is its alternative definition: \\[Var(X) = E(X^2) - E(X)^2\\] Often, we are interested in both the mean and the variance. By simplifying \\(Var(X)\\) into a function of the first and second moments, we can compute \\(E(X^2)\\) (which is often much easier) and use what we know about \\(E(X)\\) to more easily compute \\(Var(X)\\). While variance is not exactly linear, the variance of a linear transformation of a random variable is \\[Var(aX + b) = a^2Var(X)\\] When multiple random variables are involved in a linear expression, then \\[Var(aX + bY) = a^2Var(X) + b^2Var(Y) + 2ab\\cdot Cov(X, Y)\\] where \\(Cov(X,Y)\\) is described in the next subsection. If \\(X\\) and \\(Y\\) are independent then \\(Cov(X,Y) = 0\\) and \\(Var(aX + bY) = a^2Var(X) + b^2Var(X)\\) 6.4 Covariance and Correlation From the expected value function and moments, we can also define the covariance and correlation Definition 6.4 (Covariance) Covariance measures the joint dispersion of two random variables. Mathematically, \\[Cov(X,Y) = E((X - E(X))(Y - E(Y)))\\] Equivalently, \\[Cov(X,Y) = E(XY) - E(X)E(Y)\\] which is usually the more convenient definition. Note that if \\(X\\) and \\(Y\\) are independent, then \\(Cov(X,Y) = 0\\), which simplifies calculations. However, make no mistake, this implication is not bidirectional: \\(Cov(X,Y) = 0\\) does NOT imply X, Y$ are independent! Occasionally, we might want to work with a measure of joint dispersion that is normalized. This is called the correlation. Definition 6.5 (Covariance) Correlation is a measure of the joint dispersion of two random variables normalized to [0,1], with \\(Corr(X,Y) = 0\\) indicating that the variables are independent and \\(Corr(X,Y) = 1\\) indicating perfect collinearity. Mathematically, \\[Cov(X,Y) = E\\Big(\\frac{X - E(X)}{\\sqrt{Var(X)}}\\cdot \\frac{Y - E(Y)}{\\sqrt{Var(Y)}}\\Big) = \\frac{Cov(X,Y)}{\\sqrt{Var(X)Var(Y)}}\\] 6.5 Conditional Expectation When an expectation is computed using a conditional probability, it is known as a conditonal expectation. For discrete random variables (or when \\(Y\\) is simply an event), it is denoted \\[E(g(X)|Y) = \\sum_{\\mathcal{X}}g(x)P(X = x | Y)\\] and for continuous, \\[E(g(X) | Y) = \\int_{\\mathcal{X}}g(x)f_{X|Y}(x|y)\\] Two fundamental properties regarding conditional expectations exist that simply the computation of its unconditional counterpart. Definition 6.6 (Law of Iterated Expectation (Adam's Law)) \\(E(X) = E(E(X|Y))\\) Definition 6.7 (Law of Total Variance (EVVE's Law)) \\(Var(X) = E(Var(X|Y)) + Var(E(X|Y))\\) These two properties can be used to compute \\(E(X)\\) when only \\(E(X|Y)\\) is known. For example, this occurs in Chapter 8 when finding UMVUEs. 6.6 Moment Generating Functions Definition 6.8 (MGF) The moment generating function (MGF) is defined as \\[\\mathcal{M}_X(t) = E(e^{tx})\\] This function has three important properties: The MGF fully characterizes a distribution. That is, if \\(\\mathcal{M}_X(t) = \\mathcal{M}_Y(t)\\), then \\(X\\) and \\(Y\\) are identically distributed. \\(E(X^n) = \\frac{d^n}{dt^n}\\mathcal{M}_X(t)\\Big|_{t=0}\\). This means the MGF can also be used to compute any given moment simply by taking a derivative! Convolution: If \\(X\\) and \\(Y\\) and independent, then the mgf of \\(X+Y\\) is \\(\\mathcal{M}_{X+Y}(t) = \\mathcal{M}_{X}(t)\\mathcal{M}_{Y}(t)\\). This is useful for finding the distribution of sums of random variables. The Convolution property can also be extended to subtraction. If \\(X = Y_1 - Y_2\\), \\[\\mathcal{M}_X(t) = \\mathcal{M}_{Y_1}(t) \\cdot \\mathcal{M}_{Y_2}(-t)\\] Do note, however, that the MGF may not exist for some distributions. In this case it may be preferable to work with the characteristic function, which follows fundamentally the same principles, except one solves for \\(\\phi_X(t) = E(e^{itx})\\), where \\(i = \\sqrt{-1}\\). 6.7 Moment Inequalities Several inequalities exist that can bound moments. A general bound is that \\(a &lt; g(X) &lt; b \\implies a &lt; E(g(X)) &lt; b\\). This, coupled with the triangle inequality can be used to prove the following inequalities. The next two inequalities bound probabilities based on moments. They are named the student-teacher pair that developed them. Definition 6.9 (Markov's Inequality) \\[P(X \\geq t) \\leq \\frac{E(X)}{t}\\] Definition 6.10 (Chebychev's Inequality) This inequality can be stated in several ways: Chebychev’s inequality is instrumental in proving the Weak Law of Large Numbers. Next, we present an equality regarding functions of moments. Definition 6.11 (Jensen's Inequality) For a convex function \\(f\\), \\[E(f(X)) \\geq f(E(X))\\] If \\(f\\) is instead concave, \\[E(f(x)) \\leq f(E(X))\\] For both definitions, equality only holds if \\(f(x)\\) is a linear function of \\(x\\). Jensen’s inequality is useful for showing that an estimator is biased. The next three inequalities are less commonly used, but are still useful is certain situations (where?) Definition 6.12 (Cauchy-Schwarz Inequality) \\[E(XY)^2 \\leq E(X)^2E(Y)^2\\] This equality can be generalized as follows Definition 6.13 (Holder's Inequality) For \\(p, q \\geq 1\\) such that \\(\\frac{1}{p} + \\frac{1}{q} = 1\\), \\[E(|XY|) \\leq E(|X|^p)^\\frac{1}{p}E(|Y|^q)^\\frac{1}{q}\\] While the above inequalities deal with products of moments, the following handles sums: Definition 6.14 (Minkowski's Inequality) \\[E(|X + Y|^p)^\\frac{1}{p} \\leq E(|X|^p)^\\frac{1}{p} + E(|Y|^p)^\\frac{1}{p}\\] 6.8 Techniques for Deriving Moments Below, we’ve listed the important moments of each distribution that can be reasonably derived by hand. In this section, we discuss a myriad of techniques to derive these moments, each of which can be applied in general for their respective distributions. Distribution \\(E(Y)\\) \\(Var(Y)\\) mgf \\(\\text{Bernoulli}(p)\\) \\(p\\) \\(p(1-p)\\) \\((1-p) + pe^t\\) \\(\\text{Binom}(n, p)\\) \\(np\\) \\(np(1-p)\\) \\(((1-p) + pe^t)^n\\) \\(\\text{Geo}(p)\\) \\(\\frac{1-p}{p}\\) \\(\\frac{1-p}{p^2}\\) \\(\\frac{p}{1 - (1 - p)e^t}\\) for \\(t &lt; -\\log(1-p)\\) \\(\\text{NegBinom(r, p)}\\) \\(\\frac{r(1-p)}{p}\\) \\(\\frac{r(1-p)}{p^2}\\) \\(\\Big(\\frac{p}{1 - (1 - p)e^t}\\Big)^r\\) for \\(t &lt; -\\log(p)\\) \\(\\text{Pois}(\\lambda)\\) \\(\\lambda\\) \\(\\lambda\\) \\(\\exp(\\lambda(e^t - 1))\\) \\(\\text{Normal}(\\mu, \\sigma^2)\\) \\(\\mu\\) \\(\\sigma^2\\) \\(\\exp(\\mu t + \\sigma^2 t^2 / 2)\\) \\(\\text{Exp}(\\lambda)\\) \\(\\lambda\\) \\(\\lambda^2\\) \\(\\frac{1}{1-\\lambda t}\\) for \\(t &gt; \\lambda\\) \\(\\text{Gamma}(k, \\lambda)\\) \\(k\\lambda\\) \\(k\\lambda^2\\) \\((1 - \\lambda t)^{-k}\\) for \\(t &lt; \\frac{1}{\\lambda}\\) \\(\\text{Beta}(\\alpha, \\beta)\\) \\(\\frac{\\alpha}{\\alpha + \\beta}\\) \\(\\frac{\\alpha\\beta}{(\\alpha + \\beta)^2(\\alpha + \\beta + 1)}\\) \\(1 + \\sum_{k=1}^\\infty\\Big(\\prod_{r=0}^{k-1}\\frac{\\alpha + r}{\\alpha + \\beta + r}\\Big)\\frac{t^k}{k!}\\) \\(\\chi^2(\\nu)\\) \\(\\nu\\) \\(2\\nu\\) \\((1 - 2t)^{-\\nu/2}\\) for \\(t &lt; \\frac{1}{2}\\) \\(\\text{Uniform}(a, b)\\) \\(\\frac{1}{2}(a+b)\\) \\(\\frac{1}{12}(b-a)^2\\) \\(\\begin{cases}\\frac{e^{tb}-e^{ta}}{t(b-a)} &amp; t \\neq 0 \\\\ 1 &amp; t = 0 \\\\\\end{cases}\\) \\(F(n, m)\\) \\(\\frac{m}{m - 2}\\) for \\(m &gt; 2\\) \\(\\frac{2m^2(n + m - 2)}{n(m - 2)^2(m - 4)}\\) for \\(m &gt; 4\\) Does Not Exist \\(\\text{HyperGeo}(N, K, n)\\) \\(\\frac{nK}{N}\\) \\(\\frac{nK(N-K)(N-n)}{N^2(N-1)}\\) Too complicated to reproduce here! 6.8.1 Bernoulli: Direct Summation The moments of a Bernoulli distribution are simple to compute, because \\(x\\) is only 0 or 1. When \\(x = 0\\), \\(0 * P(X = 0) = 0\\). Hence, \\(E(X) = 1 * P(X = 1) = p^1(1 - p)^{1 - 1} = p\\). Since \\(1^k = 1\\), a convenient property follows: \\[X \\sim \\text{Bernoulli}(p) \\implies E(X^k) = p\\] This also suggests that \\(E(X^k) = P(X = 1)\\), which can be exceptionally useful in many statistical problems, including finding UMVUEs. Similarly, we can compute the variance using the property \\(Var(X) = E(X^2) - E(X^2) = p - p^2 = p(1-p)\\). Finally, the moment generating function follows directly by noting \\[E(e^{tx}) = e^{t*0}p^0(1-p)^{1-0} + e^{t*1}p^1(1-p)^{1-1} = (1-p) + pe^t\\] 6.8.2 Uniform: Direct Integration Moments of distributions with simple pdfs bounded at both ends such as the Uniform can be solved directly by integrating their pdf. The Uniform has pdf \\(f_X(x) = \\frac{1}{b-a}\\), so its moments are as follows: \\[\\begin{align} E(X) = \\int_{a}^b \\frac{x}{b-a} dx = \\frac{x^2}{2(b-a)}\\Big|_{a}^b\\\\ = \\frac{b^2 - a^2}{2(b-a)} = \\frac{(b+a)(b-a)}{2(b-a)} = \\frac{1}{2}(b+a)\\\\ \\end{align}\\] Generalizing this, \\(E(X^k) = \\frac{b^k - a^k}{2(b-a)}\\). As a result, solving the variance is easier to do directly: \\[\\begin{align} Var(X) = \\int_{a}^b \\frac{(x - \\frac{1}{2}(b-a))^2}{b-a} dx \\\\ = \\frac{(x - \\frac{1}{2}(b-a))^3}{3(b-a)}\\Big|_{a}^b \\\\ = \\frac{(b - \\frac{1}{2}b - \\frac{1}{2}a)^3 - (a - \\frac{1}{2}b - \\frac{1}{2}a)^3}{3(b-a)}\\Big|_{a}^b\\\\ = \\frac{2(b-a)^3}{24(b-a)} = \\frac{1}{12}(b-a)^2\\\\ \\end{align}\\] 6.8.3 Geometric: Series Convergence Deriving the moments of the Geometric distribution requires the use of the Geometric Series (from where we can speculate its name originates). Note \\(E(X) = \\sum_{x=0}^\\infty xp(1-p)^x\\), which does not quite match the geometric series; first, we need to take the derivative, and then interchange differentiation and summation like so: \\[\\begin{align} \\sum_{x=0}^\\infty xp(1-p)^x = p(1-p)\\sum_{x=0}^\\infty x(1-p)^{x-1} &amp;&amp; \\text{factor out for correct form}\\\\ = p(1-p)\\sum_{x=0}^\\infty \\frac{d}{dx}-(1-p)^{x} &amp;&amp; \\text{notice derivative}\\\\ = -p(1-p) \\frac{d}{dx}\\sum_{x=0}^\\infty (1-p)^{x} &amp;&amp; \\text{interchange derivative}\\\\ = -p(1-p) \\frac{d}{dx}\\frac{1}{1 - (1 - p)} &amp;&amp; \\text{geometric series}\\\\ = \\frac{p(1-p)}{p^2} = \\frac{1-p}{p} &amp;&amp; \\\\ \\end{align}\\] This can also be performed to compute \\(E(X)\\) for the variance, though the computation is relatively long to be reproduced here. A quicker way might be to employ the moment generating function from which all moments can be computed, which can easily be found by substituting the geometric series: \\[\\begin{align} E(e^{tx}) = \\sum_{x=0}^\\infty e^{tx}(p(1-p)^x) \\\\ = p\\sum_{x=0}^\\infty ((1-p)e^t)^x) \\\\ = \\frac{p}{1 - (1-p)e^t} \\end{align}\\] 6.8.4 Binomial: Kernel Technique, Series Version Moments of the Binomial are trickier since they involve an infinite sum. Since the Binomial is a discrete distribution, we can compute its moments using the discrete moment formula \\(E(X) = \\sum_{i=1}^\\infty xP(X = x)\\). This introduces one technique for moment calculations: the Kernel Technique Example 6.1 (Kernel Technique with Infinite Series) Fact: pmfs integrate to 1. That is, \\(\\sum_{x=0}^\\infty f_X(x) = 1\\). We can use this fact to compute moments by: Recognizing the kernel of the distribution within the moment formula Factoring out appropriate constants to turn the kernel into the full pmf, and simplifying the infinite series to \\(\\sum_{x=0}^\\infty f_X(x) = 1\\). The left-over components then, are the value of the moment. Binomial Distribution: We can use this technique to compute moments of the Binomial distribution like so: \\[\\begin{align} E(X) = \\sum_{x=0}^\\infty xP(X = x) = \\sum_{x=0}^\\infty x{n\\choose x}p^x(1-p)^{n-x} &amp;&amp; \\\\ = \\sum_{x=0}^\\infty \\frac{x\\cdot n!}{x!(n-x)!}p^x(1-p)^{n-x} &amp;&amp; \\text{(kernel)} \\\\ = 0 + \\sum_{x=1}^\\infty \\frac{n \\cdot (n-1)!}{(x-1)!((n-1) - (x - 1)!}\\cdot p \\cdot p^{x-1}(1-p)^{(n - 1) - (x - 1)} &amp;&amp; \\text{(form pmf)}\\\\ = np\\sum_{x=0}^\\infty \\cdot {n - 1 \\choose x}p^x(1-p)^{(n-1) - x} &amp;&amp; \\text{(sum pmf to 1)}\\\\ = np \\end{align}\\] To compute the \\(E(X^2)\\) component of the variance, this process needs to be repeated twice: \\[\\begin{align} E(X^2) = \\sum_{x=0}^\\infty x^2P(X = x) = \\sum_{x=0}^\\infty x^2{n\\choose x}p^x(1-p)^{n-x} &amp;&amp; \\\\ = np \\sum_{x=0}^\\infty(x+1) \\frac{(n-1)!}{x!(n-1-x)!}p^x (1-p)^{n-x-1} &amp;&amp; \\text{(from E(X))}\\\\ = np(0 + (n-1)p\\sum_{x=1}^\\infty \\frac{(n-2)!}{(x-1)!((n-2)-(x-1))!}p^{x-1}(1-p)^{(n-2)-(x-1)} + 1 &amp;&amp; \\\\ = np((n-1)p + 1) = (np)^2 + np(1-p) &amp;&amp; \\text{(pmf sums to 1)}&amp;&amp; \\\\ \\end{align}\\] Then, \\(Var(X) = E(X^2) - E(X)^2 = (np)^2 + np(1-p) - (np)^2 = np(1-p)\\) Alternatively, we could have computed this using the fact that the Binomial is equal to a sum of Bernoulli random variables. By the linearity of expectation, if \\(X_i \\sim \\text{Bernoulli}(p)\\), then \\(E(\\sum_{i=1}^n X_i) = n\\cdot E(X_i) = np\\). \\(Var(X)\\) follows similarly. Even the MGF can be found easily this way - the MGF of a Bernoulli is \\(1 - p + pe^t\\), so by convolution \\(\\mathcal{M}_{X} = (1 - p + pe^t)^n\\) for \\(X\\sim\\text{Binomial(n,p)}\\). Multinomial Distribution: We can use the same technique to compute \\(Cov(X_i, X_j)\\) for a multivariate distribution as well. Suppose \\((X_1, ..., X_n)\\sim \\text{Multinomial}(m, p1_,...,p_n\\). Then, \\(Cov(X_i, X_j) = E(X_iX_j) - E(X_i)E(X_j)\\). Since the marginals are binomial, \\(E(X_i)E(X_j) = (mp_i)(mp_j)\\) based on the moments computed earlier. Then, \\[ E(X_iX_j) = \\sum_{x_i=0}^m\\sum_{x_j=0}^m x_ix_jf_{X_i, X_j}(x_i, x_j)\\\\ = \\sum_{x_i=0}^m\\sum_{x_j=0}^m\\frac{m!}{(x_i - 1)!(x_j - 1)!}p_i^{x_i}p_j^{x_j} \\\\ = m(m-1)p_ip_j\\sum_{x_i=0}^m\\sum_{x_j=0}^m\\frac{(m-2)!}{(x_i - 1)!(x_j - 1)!}p_i^{x_i-1}p_j^{x_j-1}\\\\ = m^2p_ip_j - mp_ip_j \\] by recognizing the \\(\\text{Multinomial}\\) kernel in the second-to-last line. Then, \\(Cov(X_i, X_j) = m^2p_ip_j - mp_ip_j - (mp_i)(mp_j) = -mp_ip_j\\), completing the proof. 6.8.5 Negative Binomial and Hypergeometric: Computing \\(E(X(X-1))\\) The second moment of distributions with pmfs/pdfs involving factorials can often be computing more easily by finding \\(E(X(X-1))\\) instead of \\(E(X^2)\\) directly. The Negative Binomial is one such distribution. Its mean can be computed by the same Kernel Technique used for the Binomial: \\[\\begin{align} E(X) = \\sum_{x=0}^\\infty x{x + r - 1 \\choose x} \\cdot (1-p)^x p^r &amp;&amp; \\\\ = (1-p)\\sum_{x=1}^\\infty \\frac{((x-1) + r)!}{(x-1)!(r-1)!}\\cdot (1-p)^{x-1}p^{r} &amp;&amp; \\\\ = \\frac{r(1-p)}{p}\\sum_{x=1}^\\infty \\frac{((x-1) + r)!}{(x-1)!(r-1)!}\\cdot (1-p)^{x-1}p^{r} &amp;&amp; \\\\ = \\frac{r(1-p)}{p}\\sum_{x=0}^\\infty \\frac{((x-1) + r)!}{(x-1)!r!}\\cdot (1-p)^{x-1}p^{r+1} &amp;&amp; \\\\ = \\frac{r(1-p)}{p} &amp;&amp; \\text{pmf sums to 1} \\end{align}\\] With \\(E(X)\\) known, we can now take advantage of the factorial to compute \\(E(X(X-1))\\), using the same technique of simplifying the combinatorial fraction and “pulling out” components to reform a pdf: \\[\\begin{align} E(X(X-1)) = \\sum_{x=0}^\\infty x(x-1){x + r - 1 \\choose x}(1-p)^x p^r \\\\ = \\frac{r(r+1)}{1-p)^2}{p^2}\\sum_{x=0}^\\infty \\frac{((x-2) + (r+1))!}{(x-2)!(r+1)!}(1-p)^{x-2}p^{r+2}\\\\ = \\frac{r(r+1)}{1-p)^2}{p^2}\\Big(0 + 0 + \\sum_{x=2}^\\infty {(x-2) + r + 1\\choose x-2}(1-p)^{x-2}p^{r+2}\\\\ = \\frac{r(r + 1)(1-p)^2}{p^2} \\text{ pmf sums to 1}\\\\ \\end{align}\\] Since, by linearity of expectation, \\(E(X(X-1)) = E(X^2) - E(X)\\), we can write \\[\\begin{align} Var(X) = E(X^2) - E(X) + E(X) - E(X)^2 \\\\ = E(X(X-1)) + E(X) - E(X)^2 \\\\ = \\frac{r(r+1)(1-p)^2}{p^2} + \\frac{r(1-p)}{p} - \\frac{r^2(1-p)^2}{p^2}\\\\ = \\frac{r(1-p)^2}{p^2} + \\frac{r(1-p)}{p} \\\\ = \\frac{r(1-p)(1 - p + p)}{p^2} = \\frac{r(1-p)}{p^2} \\end{align}\\] Note: The moments of a Negative Binomial can also be computed simply by relying on its additive property in relation to the geometric, and then using the linearity expectation. That is, if \\(Y \\sim \\text{NegBin}(r, p)\\), then \\(Y = \\sum_{i=1}^r X_i\\) where \\(X_i \\sim \\text{Geo}(p)\\). Then, \\(E(Y) = E(\\sum_{i=1}^r X_i) = \\frac{r(1-p)}{p}\\). Hypergeometric Distribution: The above technique can also be used to find the variance of a \\(\\text{HGeo}(N, K, n)\\) random variable. First, let us use the Kernel Technique to compute its first moment. Writing the combinatorial functions in their full form, noting in general \\(y - x = (y - 1) - (x - 1)\\), we can rewrite this as the pmf of a \\(\\text{HGeo}(N-1, K-1, n-1)\\): \\[\\begin{align} E(X) = \\sum_{x=0}^n x \\cdot \\frac{K!}{x!(K-x)}\\cdot\\frac{(N-K)!}{(n-x)!(N-K-n+x)!} \\cdot \\frac{n!(N-n)!}{N!}\\\\ = \\sum_{x=1}^n \\frac{K(K-1)!}{(x-1)!((K-1) - (x-1))!} \\\\ \\cdot \\frac{((N-1) - (K - 1))!}{((n-1) - (x-1))!((N-1) - (k-1) - (n-1) + (x-1))!}\\cdot\\frac{n(n-1)!((N-1) - (n-1))!}{N(N-1)!}\\\\ = \\frac{NK}{n}\\sum_{x=0}^{n-1} \\frac{{K-1\\choose x}{(N-1) - (K-1)\\choose (n-1) - x}}{{N-1\\choose n-1}}\\\\ = \\frac{NK}{n} \\end{align}\\] For the variance, we first compute \\(E(X(X-1))\\) in the same fashion. For brevity, we exclude writing down \\(y - x = (x - 2) - (y-2)\\) expansion, but the calculations below do rely on this principle. \\[\\begin{align} E(X(X-1)) = \\sum_{x=0}^n x(x-1) \\cdot \\frac{K!}{x!(K-x)}\\cdot\\frac{(N-K)!}{(n-x)!(N-K-n+x)!} \\cdot \\frac{n!(N-n)!}{N!}\\\\ = \\sum_{x=2}^n \\frac{K(K-1)!}{(x-1)!(K - X)!} \\\\ \\cdot \\frac{(N - K)!}{((n-2) - x-2))!(N - k - (n-2) + (x-2))!}\\cdot\\frac{n(n-1)(n-2)!((N-2) - (n-2))!}{N(N-1)(N-2)!}\\\\ = \\frac{N(N-1)K(K-1)}{n(n-1)}\\sum_{x=0}^{n-2} \\frac{{K-2\\choose x}{(N-2) - (K-2)\\choose (n-2) - x}}{{N-2\\choose n-2}}\\\\ = \\frac{N(N-1)K(K-1)}{n(n-1)} \\end{align}\\] Then, \\[\\begin{align} Var(X) = E(X(X-1)) + E(X) - E(X)^2\\\\ = \\frac{K(K-1)N(N-1)}{n(n-1)} + \\frac{KN}{n} - \\frac{KN}{n^2}\\\\ = n\\frac{K}{N} \\cdot \\frac{N - K}{N}\\cdot \\frac{N - n}{N-1} \\end{align}\\] after lengthy algebra. 6.8.6 Poisson: Exponential Taylor Series Like the Geometric, we can also derive the moments of the Poisson by relying the convergence of an infinite series. This time, we rely on the Taylor Series for the exponential distribution, which is \\(\\sum_{x=0}^\\infty \\frac{\\lambda^x}{x!} = e^\\lambda\\). Proceed as follows: \\[\\begin{align} E(X) = \\sum_{x=0}^\\infty x\\frac{e^{-\\lambda}\\lambda^x}{x!} &amp;&amp; \\\\ = \\lambda e^{-\\lambda}\\sum_{x=1}^\\infty \\frac{\\lambda^{x-1}}{(x-1)!} &amp;&amp; \\text{when }x = 0, \\text{ the series term is 0}\\\\ = \\lambda e^{-\\lambda}e^{\\lambda} = \\lambda &amp;&amp; \\text{exponential series} \\end{align}\\] The variance follows similarly: \\[\\begin{align} E(X^2) = \\sum_{x=0}^\\infty x^2\\frac{e^{-\\lambda}\\lambda^x}{x!} \\\\ = \\lambda e^{-\\lambda}\\sum_{x=1}^\\infty x\\frac{\\lambda^{x-1}}{(x-1)!} \\\\ = \\lambda e^{-\\lambda}\\sum_{x=1}^\\infty x\\frac{\\lambda^{x-1}}{(x-1)!} \\\\ = \\lambda e^{-\\lambda}\\sum_{x=0}^\\infty (x + 1)\\frac{\\lambda^{x}}{x!} \\\\ = e^{-\\lambda} \\cdot \\lambda \\cdot \\lambda \\cdot e^{\\lambda} + e^{-\\lambda} \\cdot \\lambda \\cdot e^{\\lambda} = \\lambda^2 + \\lambda \\\\ \\implies Var(X) = E(^2) - E(X)^2 = \\lambda^2 + \\lambda - \\lambda^2 = \\lambda \\end{align}\\] The MGF can also be found using an exponential Taylor series: \\[\\begin{align} \\mathcal{M}_X(t) = \\sum_{x=0}^\\infty e^{tx} \\cdot e^{-\\lambda} \\cdot \\frac{\\lambda^x}{x!} = e^{-\\lambda}\\sum_{x=0}^\\infty \\frac{(\\lambda e^{tx})^x}{x!} \\\\ = e^{-\\lambda}\\cdot e^{\\lambda e^{t}} = e^{\\lambda(e^t - 1)} \\end{align}\\] 6.8.7 Exponential: Integration By Parts Sometimes, a pdf can be integrated directly using more advanced integration techniques. The Exponential is one such distribution - its moments can be computed using integration by parts For the mean, note \\(E(X) = \\int_0^\\infty \\lambda e^{-\\lambda x}dx\\). Let \\(u = x \\implies du = 1\\) and \\(du = \\lambda e^{-\\lambda x} \\implies v = -e^{-\\lambda x}\\). Then, \\[\\begin{align} E(X) = uv - \\int_{0}^\\infty vdu = -xe^{-\\lambda x}\\Big|_{0}^\\infty + \\int_0^\\infty e^{-\\lambda x}dx\\\\ = 0 + 0 - \\frac{1}{\\lambda}e^{-\\lambda x} \\Big|_0^\\infty = \\frac{1}{\\lambda} \\end{align}\\] noting that \\(\\lim_{x\\rightarrow\\infty}xe^{-\\lambda x} = \\lim_{x\\rightarrow\\infty}-\\lambda e^{-\\lambda x} = 0\\) by applying L’Hopital’s rule. Hence \\(E(X) = \\frac{1}{\\lambda}\\) For the variance, start by computing \\(E(X^2)\\). Applying integration by parts with \\(u = x^2 \\implies du = 2x\\) and \\(dv = \\lambda e^{-\\lambda x} \\implies -e^{-\\lambda x}\\), \\[E(X^2) = uv - \\int_{0}^\\infty vdu = -x^2e^{-\\lambda x} \\Big|_0^\\infty + \\int_{0}^\\infty 2xe^{-\\lambda x}\\] Now, we could apply integration by parts again, but a faster way to solve this is by moment recognization: noting that the second term can be transformed to equal \\(E(X)\\) like so: \\(\\int_{0}^\\infty 2xe^{-\\lambda x} = \\frac{2}{\\lambda}\\int_{0}^\\infty x\\lambda e^{-\\lambda x} = E(X) = \\lambda\\) as we’ve already solved. As before, the first term is \\(\\lim_{x\\rightarrow \\infty}-x^2e^{-\\lambda x} \\Big|_0^\\infty = \\lim_{x\\rightarrow\\infty}\\lambda^2e^{-\\lambda x} = 0\\) by applying L’Hopital’s rule twice. Plugging in \\(E(X) = \\frac{1}{\\lambda}\\), we get \\(E(X^2)\\) = \\(\\frac{2}{\\lambda^2}\\) and \\[Var(X) = E(X^2) - E(X)^2 = \\frac{2}{\\lambda^2} - \\frac{1}{\\lambda^2} = \\frac{1}{\\lambda^2}\\] 6.8.8 Gamma and Beta: Kernel Technique, Integration Version Similar to discrete distributions, we can also use the Kernel Technique to more easily integrate continuous distributions as well. For example, noting Gamma function property $(k) = , \\[\\begin{align} E(X) = \\int_{0}^\\infty \\frac{x}{\\Gamma(k)\\lambda^k}x^{k-1}e^{-\\frac{1}{\\lambda}x}dx &amp;&amp;\\\\ = \\int_{0}^\\infty \\frac{\\lambda}{(\\Gamma(k+1) / k)\\lambda^{k+1}}x^k e^{-\\frac{1}{\\lambda}x}dx &amp;&amp; \\text{recognize Gamma kernel}\\\\ = \\lambda k \\int_{0}^\\infty \\frac{1}{\\Gamma(k+1)\\lambda^{k+1}}x^k e^{-\\frac{1}{\\lambda}x}dx &amp;&amp; \\text{pull out excess terms}\\\\ = \\lambda k &amp;&amp; \\text{ integrate } Gamma(k+1, \\lambda) \\text{ to 1} \\end{align}\\] Similarly, the variance of the Gamma can be calculated like so: Solve for \\(E(X^2)\\) \\[\\begin{align} E(X^2) = \\int_{0}^\\infty \\frac{x^2}{\\Gamma(k)\\lambda^k}x^{k-1}e^{-\\frac{1}{\\lambda}x}dx = \\int_{0}^\\infty \\frac{\\lambda^2}{(\\Gamma(k+2) / (k(k+1)))\\lambda^{k+2}}x^{k+1} e^{-\\frac{1}{\\lambda}x}dx\\\\ = \\lambda^2k(k+1) \\text{ Pull out excess terms from } Gamma(k+1, \\lambda) \\text{ kernel, integrate to 1} \\end{align}\\] Compute variance using \\(Var(X) = E(X^2) - E(X)^2\\) using the components solved previously. \\[Var(X) = \\lambda^2k(k+1) - \\lambda^2k^2 = \\lambda^2k\\] Since the Beta distribution also involves Gamma function, we can compute the moments in a similar fashion. Let’s start with the mean: \\[\\begin{align} E(X) = \\int_{0}^\\infty x \\cdot\\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)}x^{\\alpha-1}(1-x)^{\\beta-1}dx &amp;&amp; \\\\ = \\frac{\\alpha}{\\alpha + \\beta}\\int_{0}^{1} \\frac{\\Gamma(\\alpha + 1 + \\beta)}{\\Gamma(\\alpha + 1)\\Gamma(\\beta)}x^\\alpha (1-x)^{\\beta-1}dx &amp;&amp; \\text{form kernel of } Beta(\\alpha+1, \\beta)\\\\ = \\frac{\\alpha}{\\alpha + \\beta} &amp;&amp; \\text{ integrate Beta pdf to 1}\\\\ \\end{align}\\] Now, we apply this same principle twice to compute the variance: Compute \\(E(X^2)\\) by using \\(\\Gamma(\\alpha) = \\alpha(\\alpha+1)\\Gamma(\\alpha+2)\\) and \\(\\Gamma(\\alpha + \\beta) = (\\alpha + \\beta)(\\alpha + \\beta + 2)\\) to form the Beta kernel: \\[\\begin{align} E(X^2) = \\int_{0}^1 x^2 \\cdot \\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)}x^{\\alpha-1}(1 - x)^{\\beta-1}dx &amp;&amp; \\\\ = \\frac{\\alpha(\\alpha+1)}{(\\alpha + \\beta)(\\alpha + \\beta + 1)}\\int_{0}^1x^{\\alpha-1}(1-x)^{\\beta-1}dx &amp;&amp; \\text{form kernel of } Beta(\\alpha + 2, \\beta)\\\\ = \\frac{\\alpha(\\alpha+1)}{(\\alpha + \\beta)(\\alpha + \\beta + 1)} \\text{integrate Beta pdf to 1} \\end{align}\\] Compute the variance using \\(Var(X) = E(X^2) - E(X)^2\\) \\[\\begin{align} Var(X) = E(X^2)\\frac{\\alpha^2 + \\alpha}{(\\alpha + \\beta)(\\alpha + \\beta + 1)} - \\frac{\\alpha^2}{(\\alpha + \\beta)^2}\\\\ = \\frac{\\alpha^2(\\alpha + \\beta) + \\alpha(\\alpha + \\beta) - \\alpha^2(\\alpha + \\beta + 1)}{(\\alpha + \\beta)^2(\\alpha + \\beta + 1)}\\\\ = \\frac{\\alpha\\beta}{(\\alpha + \\beta)^2(\\alpha + \\beta + 1)} \\end{align}\\] Virtually identical steps are used to compute the moments of the Chi-squared (which is a special case of the Gamma) and F distributions as well. 6.8.9 Normal: Location-Scale Trick and Polar Integration Proving that the moments of the \\(\\text{Normal}(\\mu, \\sigma^2)\\) can be a bit tricky. However, if we use the fact that it is a location-scale family, it becomes much easier. Letting \\(X = \\sigma Z + \\mu\\), then \\[E(X) = \\sigma E(Z) + \\mu\\] by linearity of expectation. Since \\(Z \\sim N(0,1)\\), noting that the antiderivative of \\(z\\exp(-\\frac{1}{2}z^2)\\) is \\(-\\exp(-\\frac{1}{2}z^2)\\), we get \\[\\begin{align} E(Z) = \\int_{\\infty}^\\infty \\frac{z}{\\sqrt{2\\pi}}\\exp(-\\frac{1}{2}z^2)dx\\\\ = -\\exp(-\\frac{1}{2}z^2)\\Big|_{-\\infty}^\\infty = 0 \\end{align}\\] since \\(\\lim_{z^2\\rightarrow\\infty} \\exp(-\\frac{1}{2}z^2) = 0\\). Therefore, \\(E(X) = E(\\sigma Z + \\mu) = \\sigma E(Z) +\\mu = \\mu\\). Computing the variance necessitates a more advanced integration technique: Polar Coordinates. Proceed with Integration by Parts, letting \\(u = \\frac{z}{\\sqrt{2\\pi}} \\implies du = \\frac{1}{\\sqrt{2\\pi}}\\) and \\(dv = z\\exp(-\\frac{1}{2}z^2)\\implies v = -\\exp(-\\frac{1}{2}z^2)\\) as used previously. Then, \\[\\begin{align} E(Z^2) = \\int_{\\infty}^\\infty \\frac{z^2}{\\sqrt{2\\pi}}\\exp(-\\frac{1}{2}z^2)dx\\\\ = \\frac{z}{\\sqrt{2\\pi}}\\exp(-\\frac{1}{2}z^2)\\Big|_{-\\infty}^\\infty + \\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^\\infty \\exp(-\\frac{1}{2}z^2)dz \\end{align}\\] By L’Hopital’s rule, \\[\\lim_{z\\rightarrow\\infty} \\frac{z}{\\sqrt{2\\pi}}\\exp(-\\frac{1}{2}z^2) = \\lim_{z\\rightarrow\\infty}\\frac{1}{\\sqrt{2\\pi}z\\exp(\\frac{1}{2}z^2)} = 0\\] so \\(\\frac{z}{\\sqrt{2\\pi}}\\exp(\\frac{1}{2}z^2)\\Big|_{-\\infty}^\\infty = 0\\). But how do we solve the second integral? This is where polar coordinates come into play. From Strang (2010), if \\(A = \\int_{-\\infty}^\\infty \\exp(-\\frac{1}{2}x^2)dx\\), then we can solve the integral by converting into polar like so: \\[\\begin{align} A^2 = \\int_{-\\infty}^\\infty \\exp(-\\frac{1}{2}x^2)dx \\cdot \\int_{-\\infty}^\\infty \\exp(-\\frac{1}{2}xy^2)dy\\\\ = \\int_{-\\infty}^\\infty\\int_{-\\infty}^\\infty \\exp(-\\frac{1}{2}(x^2 + y^2))dxdy \\\\ = \\int_{0}^{2\\pi}\\int_{0}^\\infty r\\exp(-\\frac{1}{2}r^2(\\cos^2(\\theta) + \\sin^2(\\theta)))drd\\theta\\\\ = \\int_{0}^{2\\pi}\\int_{0}^\\infty r\\exp(-\\frac{1}{2}r^2)drd\\theta\\\\ = 2\\pi \\implies A = \\sqrt{2\\pi} \\end{align}\\] Hence, \\(E(Z^2) = 0 + \\frac{\\sqrt{2\\pi}}{\\sqrt{2\\pi}} = 1\\) and therefore, \\[\\begin{align} Var(X) = E((\\sigma Z + \\mu - E(Z))^2) \\\\ = E((\\sigma Z + \\mu - \\mu)^2) = \\sigma^2E(Z^2) \\\\ = \\sigma^2 \\end{align}\\] Hence, we have proven that, for \\(X \\sim \\text{Normal}(\\mu, \\sigma^2)\\), that the mean is \\(\\mu\\) and variance is \\(\\sigma^2\\) - as we expected (ba-dum tss). 6.9 Other Moments (for reference) Distribution \\(E(Y)\\) \\(Var(Y)\\) mgf \\(\\text{Weibull}(k, \\lambda)\\) \\(\\lambda\\Gamma(1 + \\frac{1}{k})\\) \\(\\lambda^2\\Big(\\Gamma(1 + \\frac{2}{k}) - (\\Gamma(1 + \\frac{1}{k}))^2\\Big)\\) \\(\\sum_{n=0}^\\infty\\frac{t^n \\lambda^n}{n!}\\Gamma(1 + \\frac{n}{k})\\), \\(k \\geq 1\\) \\(\\text{Pareto}(x_m, \\alpha)\\) \\(\\begin{cases}\\infty &amp; \\alpha \\leq 1 \\\\ \\frac{\\alpha x_m}{a - 1} &amp; \\alpha &gt; 1\\end{cases}\\) \\(\\begin{cases}\\infty &amp; \\alpha \\leq 2 \\\\ \\frac{x_m^2 \\alpha}{(a-1)^2(\\alpha - 2)} &amp; \\alpha &gt; 2\\end{cases}\\) Does not exist \\(\\text{Cauchy}(x_0, \\gamma)\\) Does Not Exist Does Not Exist \\(\\exp(x_0it - \\gamma|t|\\) (cf) \\(t(\\nu)\\) 0 \\(\\begin{cases}\\frac{\\nu}{\\nu-2} &amp; \\nu &gt; 2\\\\ \\infty &amp; 1 &lt; \\nu \\leq 2 \\\\ \\text{undefined} &amp; \\text{otherwise}\\end{cases}\\) Does Not Exist References "],["statistics.html", "Chapter 7 Statistics 7.1 Sufficient Statistics 7.2 Minimal Sufficiency 7.3 Ancillary Statistics", " Chapter 7 Statistics One of the most important tasks we will be performing as statisticians is inference. Inference is the area of statistics which uses sample data to estimate a population parameter. For this it is necessary to reduce or summarize the recollected data into a measurement that we will refer to as statistic. Definition 7.1 (Statistic) Let \\(X \\sim f(x|\\theta)\\), where both \\(X, \\theta\\) can be vectors. A statistic is a function \\(T=T(\\textbf{X})\\) of the sample \\(\\textbf{X}\\) from \\(X\\). For example, the \\(T= T(\\textbf{X}) = \\frac{1}{n} \\sum_{i=1}^{n} X_i\\) reduces the sample \\(\\textbf{X}\\) to a single measurement. In general, we would like to choose statistics that satisfy principles that will make inference about the population parameter easier. One of these principles is sufficiency. The idea behind sufficiency is to retain information about the population parameter, say \\(\\theta\\), while reducing the data. 7.1 Sufficient Statistics Sufficiency Principle. If \\(T\\) is sufficient, then any information about the parameter \\(\\theta\\) should depend on \\(X\\) only through \\(T\\). Definition 7.2 (Sufficient Statistic) Let \\(X \\sim f(x|\\theta)\\). We say \\(T=T(X)\\) is a sufficient statistic (SS) for \\(\\theta\\) if \\(\\mathcal{L}(\\theta|X)\\) is independent of \\(\\theta\\), i.e. \\(f(x|T;\\theta) = g(x)\\). Let’s see some examples. Example 7.1 Let \\(X = (X_1, ..., X_n), X_i\\) iid N(\\(\\theta\\), 1) and \\(T = \\bar{X}\\). Recall that \\(\\bar{X} \\sim N(\\theta, 1/n)\\). \\[ \\begin{aligned} f(x|T, \\theta) = n^{-1/2}(2\\pi)^{-(\\frac{n-1}{2})} exp\\{-\\sum_{i=1}^{n}(x_i - \\bar{x})^2\\} \\end{aligned} \\] Observe that the conditional density is independent of \\(\\theta\\). By definition, \\(T = \\bar{X}\\) is a sufficient statistic for \\(\\theta\\). Example 7.2 Let \\(X_1, X_2\\) iid Poisson(\\(\\lambda\\)). The joint distribution of \\(X_1, X_2\\) is \\(P(X_1=x_1, X_2 = x_2) = \\frac{\\lambda^{x_1+x_2}exp(-2\\lambda)}{x_1! x_2!}\\). Let \\(T(X_1, X_2) = X_1+X_2\\). Observe that \\(X_1+X_2 \\sim\\) Poisson(2\\(\\lambda\\)). Thus, \\[ \\begin{aligned} P(X_1=x_1, X_2 = x_2|X_1+X_2=t) &amp;= \\frac{P(X_1=x_1, X_2 = t - x_1)}{P(X_1 + X_2 = t)} \\\\ &amp;=\\frac{exp(-\\lambda)\\lambda^{x_1} exp(-\\lambda)\\lambda^{t -x_1}t!}{x_1!(t-x_1)! exp(-2\\lambda)(2\\lambda)^t}\\\\ &amp;= { t \\choose x_1} \\bigg(\\frac{1}{2}\\bigg)^t \\end{aligned} \\] which is independent of \\(\\lambda\\) and \\(X_1+X_2\\) is sufficient for \\(\\lambda\\). As can be seen from the example, we need a candidate statistic to prove sufficiency using the definition. Furthermore, checking sufficiency of a statistics is difficult because we need to compute the conditional distribution. Theorem 7.1 (Factorization Theorem) \\(T(X)\\) is sufficient for \\(\\theta \\Longleftrightarrow \\exists g(t|\\theta)\\) and \\(h(x)\\), such that \\[f(x|\\theta)=g(t|\\theta)h(x)\\] \\(\\forall x, \\theta\\). Note that the factorization theorem tells us that if we can manipulate \\(f(x|\\theta)\\) as above, we have a sufficient statistic. Example 7.3 Let \\(\\textbf{X}= (X_1, ..., X_n), X_i\\) iid \\(Poisson(\\lambda)\\). \\[ \\begin{aligned} P(X_1=x_1,..., X_n = x_n) &amp;= \\frac{exp(-n\\lambda)\\lambda^{\\sum x_i}}{\\prod x_i!}\\\\ &amp;= h(x)g(\\sum x_i|\\lambda) \\end{aligned} \\] \\(\\Rightarrow T(X) = \\sum X_i\\) sufficient for \\(\\lambda\\). Example 7.4 Let \\(\\textbf{X}= (X_1, ..., X_n), X_i\\) iid \\(Uniform(0,\\theta)\\). \\[ \\begin{aligned} P_{\\theta}(x_1, ..., x_n) &amp;= \\frac{1}{\\theta^n} \\prod_{i=1}^{n}I(0&lt;x_i&lt;\\theta)\\\\ &amp; = \\frac{1}{\\theta^n} I(x_{(1)}&gt;0) I(x_{(n)}&lt;\\theta) \\end{aligned} \\] \\(\\Rightarrow T(X) = X_{(n)}\\) sufficient for \\(\\theta\\). 7.1.0.1 Important facts about SS Sufficient statistics may or may not reduce the data. Original data are always sufficient. In iid sample, the order statistics are sufficient. Sufficient statistics are never unique. Suppose \\(X \\sim N(0,\\sigma^2)\\). Then by the factorization theorem \\(T(X)=X^2, |X|, X^4, exp(X^2)\\) are all sufficient. Any 1-1 function \\(g\\) of a sufficient statistic is also sufficient. Proof. Let \\(T^*(X)= g(T(X))\\). By assumption \\(g^{-1}\\) exists since g is 1-1. \\[ \\begin{aligned} f(x|\\theta) &amp;= g(T(X)|\\theta)\\\\ &amp;=g(r^{-1}(T^*(x))|\\theta)h(x)\\\\ \\end{aligned} \\] By the factorization theorem, \\(T^*(X)\\) is sufficient for \\(\\theta\\). 7.1.1 Important exponential family result Consider a sample \\(X=(X_1,...,X_n)\\) from \\(f_X(x|\\theta)= h(x)c(\\theta)\\exp\\Big(\\sum_{i=1}^k w_i(\\theta)t_i(x)\\Big)\\). Then \\(T(X) = \\Big(\\sum_{i=1}^n t_1(x), ..., \\sum_{i=1}^n t_k(x)\\Big)\\) is a sufficient statistic. Note that this result follows directly from the factorization theorem. 7.1.2 A Note on Distributions of Sufficient Statistics Recall a convenient property of exponential families: that their maximum likelihood estimate is a function of their sufficient statistic. Because of this, in order to prove Finite Sample Properties of an estimator or construct Hypothesis Tests, it is often useful to understand their distributions. This is why the distributions of each of the sufficient statistics are included in the fourth column of the table below These distributions are mostly derived from additive, location-scale, and other properties in Chapter 4 - Known Distributions 7.1.3 Moments of the Sufficient Statistic As stated by Casella and Berger (1990), if \\(X\\) is an exponential family, the moments of its exponential family can be easily computed using certain properties. Theorem 7.2 (I Don't Know What To Call This) If \\(X\\) is an exponential family, then \\[E\\Big(\\sum_{i=1}^k \\frac{\\partial w_i(\\theta)}{\\partial \\theta_j}t_i(X)\\Big) = -\\frac{\\partial}{\\partial\\theta_j} \\log c(\\theta)\\] and \\[Var\\Big(\\sum_{i=1}^k \\frac{\\partial w_i(\\theta)}{\\partial \\theta_j}t_i(X)\\Big) = -\\frac{\\partial}{\\partial\\theta_j} \\log c(\\theta) - E\\Big(\\sum_{i=1}^k \\frac{\\partial^2w_i(\\theta)}{\\partial \\theta_j^2}t_i(X)\\Big)\\] If \\(X\\) is a natural exponential family, then these identities simplify even further. Theorem 7.3 (I Don't Know What To Call This 2) If \\(X\\) is a natural exponential family, then$ \\[E(t_j(X)) = -\\frac{\\partial}{\\partial\\eta_j}\\log(c^*(\\eta))\\] and \\[Var(t_j(X)) = -\\frac{\\partial^2}{\\partial\\eta_j^2}\\log(c^*(\\eta))\\] Using these theorems, we can calculate the moments of the sufficient statistics of exponential families directly. This is because if \\(T(X)\\) is a sufficient statistic, then \\(T(X) = \\Big(\\sum_{i=1}^nt_1(X), ..., \\sum_{i=1}^nt_k(X)\\Big)\\). Suppose, for simplicity, that \\(T(X)\\) is one-dimensional and \\(X_i\\) are iid. Then, if \\(X_i\\) is a natural exponential family, \\[E(T(X)) = E\\Big(\\sum_{i=1}^nt(X_i)\\Big) = nE(t(X_i)) = n\\cdot-\\frac{\\partial}{\\partial \\eta}\\log(c^*(\\eta))\\] In fact, letting \\(A(\\eta) = -\\log(c^*(\\eta))\\), we can obtain all of the moments of \\(T(X)\\) by simply differentiating \\(A(\\eta)\\) 7.1.4 Table of Sufficient Statistics Translated from https://en.wikipedia.org/wiki/Exponential_family Distribution Parameter Sufficient Statistic S.S. Distribution Bernoulli p \\(\\sum_{i=1}^n x_i\\) \\(Binomial(n, p)\\) Binomial p \\(\\sum_{i=1}^n x_i\\) \\(Binomial(nm, p)\\) Poisson \\(\\lambda\\) \\(\\sum_{i=1}^n x_i\\) \\(Poisson(n\\lambda)\\) Negative Binomial p \\(\\sum_{i=1}^n x_i\\) Exponential \\(\\lambda\\) \\(\\sum_{i=1}^n x_i\\) \\(Gamma(n, \\lambda)\\) Normal (known \\(\\sigma^2\\)) \\(\\mu\\) \\(\\frac{1}{n}\\sum_{i=1}^n x_i\\) \\(Normal(\\mu, \\sigma^2/n)\\) Normal \\(\\mu\\), \\(\\sigma^2\\) \\((\\sum_{i=1}^n x_i, \\sum_{i=1}^n x_i^2)\\) Chi-Squared \\(\\nu\\) \\(\\sum_{i=1}^n \\log(x_i)\\) Pareto (known min \\(x_m\\)) \\(\\alpha\\) \\(\\sum_{i=1}^n \\log(x_i)\\) Gamma \\(\\alpha, \\beta\\) \\((\\sum_{i=1}^n\\log(x_i), \\sum_{i=1}^n x_i)\\) Beta \\(\\alpha, \\beta\\) \\((\\sum_{i=1}^n\\log(x_i), \\sum_{i=1}^n\\log(1 - x_i))\\) Weibull (known shape \\(k\\)) \\(\\lambda\\) \\(\\sum_{i=1}^n x^k\\) 7.2 Minimal Sufficiency In any setting there are many sufficient statistics. However, we should aim at dealing with the statistic that summarizes the data as concisely as possible. Let \\(S\\) be any a sufficient statistic for \\(\\theta\\). In principle, \\(W=(S,T)\\) is also a sufficient statistic, but we rather deal with \\(S\\) reduces the data to one-dimension. When no further reduction from a sufficient statistic is possible, then that statistic is minimal sufficient. Definition 7.3 (Minimal Sufficient Statistic) If \\(T\\) is sufficient for \\(\\theta\\), then it is a minimal sufficient statistic (MSS) if for any other sufficient statistic \\(T^*, T\\) is a function of \\(T^*\\). Remark: Of all sufficient statistics, a minimal sufficient statistic offers the maximal reduction of the data. Some “intuition” behind this definition goes as follows. A minimal sufficient statistic \\(T(X)\\) creates a partition of the sample space, \\(\\Omega\\) into sets \\(A_t\\), where \\(t \\in \\mathcal(T) = \\{t : t = T(x) \\text{ for some } x\\in \\Omega\\}\\). Now consider another sufficient statistic \\(T^*(X)\\) that creates another partition of the sample space such that \\(A^*_s = \\{x: T^*(x)=s\\}\\). Then for ever \\(s\\) there is a \\(t\\) such that \\(A^*_s \\subset A_t\\). Thus the partition associated with the minimal sufficient statistic is the coarsest possible partition for a sufficient statistic. As before, the definition for MSS is conceptually useful but it does not help us find a MSS, or how to prove a statistic is a MSS. For this task, we invoke Casella-Berger’s Theorem 6.2.13. Theorem 7.4 (Casella-Berger's Theorem 6.2.13) Let \\(X \\sim f(x|\\theta)\\). Suppose the exists a statistic \\(T=T(X)\\) such that for every \\(x,y\\) in the support of \\(X\\) \\[(*) \\frac{f(x|\\theta)}{f(y|\\theta)} = g(x,y) \\Longleftrightarrow T(x)=T(y)\\] then \\(T\\) is a MSS. In practice this theorem is used both to find a MSS and to prove that a given statistic is a MSS. Let us see some examples of applications of this theorem. Example 7.5 Let \\(\\textbf{X} = (X_1,..., X_n), X_i\\) iid \\(N(\\mu, \\sigma^2), \\theta = (\\mu, \\sigma^2)\\). Consider \\(T=(\\bar{X}, S^2)\\) where \\(S^2\\) is the sample variance. Let \\(\\textbf{x} = (x_1, ..., x_n), \\textbf{y} = (y_1, ..., y_n)\\). Then, \\[\\frac{f(\\textbf{x}|\\theta)}{f(\\textbf{y}|\\theta)} = ... = exp\\{-\\frac{1}{2\\sigma^2}[n(\\bar{x}-\\bar{y})^2 + 2 n\\mu(\\bar{x}-\\bar{y}) -(n-1)(s^2_x - s^2_y)]\\}.\\] \\(\\Rightarrow\\) Suppose the ratio is independent of \\(\\theta = (\\mu, \\sigma^2)\\). Then we must have that \\(\\bar{x} = \\bar{y}, s^2_x = s^2_y\\). \\(\\Leftarrow\\) Suppose that \\(T(x)=T(y)\\). Then \\(\\bar{x} = \\bar{y}, s^2_x = s^2_y\\) and the ratio is 1. By the previous theorem we have that \\(T\\) is a MSS. Example 7.6 Let \\(\\textbf{X} = (X_1,..., X_n), X_i\\) iid \\(U(\\theta -1, \\theta + 1)\\). \\[f(\\textbf{x}|\\theta) = \\frac{1}{2^n} I(X_{(1)} &gt; \\theta -1) I(X_{(n)} &lt; \\theta + 1)\\] Then, \\[\\frac{f(\\textbf{x}|\\theta)}{f(\\textbf{y}|\\theta)} = \\frac{I(x_{(1)} &gt; \\theta -1) I(x_{(n)} &lt; \\theta + 1)}{I(y_{(1)} &gt; \\theta -1) I(y_{(n)} &lt; \\theta + 1)}\\] and the ratio is independent of \\(\\theta\\) if and only if \\((x_{(1)}, x_{(n)}) = (y_{(1)}, y_{(n)})\\). Thus \\(T = (X_{(1)}, X_{(n)})\\) is MSS. Note that in the previous example, \\(dim(T(X))&gt;dim(\\theta)\\). This means that there is no estimator of \\(\\theta\\) that is sufficient. Theorem 7.5 If \\(X_1,..., X_n (n\\ge 1)\\) are iid with \\(X_i \\sim k\\)-parameter exponential family, then \\(T(X) = \\Big(\\sum_{i=1}^n t_1(x), ..., \\sum_{i=1}^n t_k(x)\\Big)\\) is a MSS. Fact: Like sufficient statistics, MSSs are not unique. Any 1-1 function of a MSS is also a MSS. 7.3 Ancillary Statistics Sufficiency describes where all the information in the data is contained. Ancillarity is the dual of sufficiency, describing where there is no information. Definition 7.4 (Ancillary Statistic) Let \\(X \\sim f(x|\\theta)\\). The statistic \\(S(X)\\) is ancillary for \\(\\theta\\) if \\(\\mathcal{L}(S), g(s|\\theta)\\) are independent of \\(\\theta\\), i.e. for any \\(\\theta_1, \\theta_2 \\in \\Theta\\), \\[g(s|\\theta_1)=g(s|\\theta_2) \\text{ for all } s.\\] Example 7.7 Let \\(\\textbf{X} = (X_1,..., X_n), X_i\\) iid \\(U(\\theta, \\theta + 1)\\). Consider the order statistics of the sample \\(\\textbf{X}\\). The range statistic \\(R = X_{(n)} - X_{(1)}\\) is ancillary for \\(\\theta\\) by showing that the pdf of \\(R\\) is independent of \\(\\theta\\). Intuitive, the range does not tell anything about the location of \\(\\theta\\) in the real line. In this case the ancillarity of \\(R\\) does not depend on the uniformity of the observations, but on the parameter of the distribution being a location parameter. Theorem 7.6 (Location family ancillary statistic) Let \\(\\textbf{X} = (X_1,..., X_n), X_i\\) iid \\(f_{\\mathcal{L}}(x|\\theta)\\), a location family, then \\(R = X_{(n)} - X_{(1)}\\) is ancillary for \\(\\theta\\). Theorem 7.7 (Scale family ancillary statistic) Let \\(\\textbf{X} = (X_1,..., X_n), X_i\\) iid \\(f_{\\mathcal{S}}(x|\\sigma)\\), a scale family, and let \\(S_j = X_j/X_n\\). Then \\(S=(S_1, S_2,..., S_n)\\) is ancillary for \\(\\sigma\\). Note: Since \\(S\\) is ancillary, any function of \\(S=(S_1, S_2,..., S_n)\\) is also ancillary. For example, \\(S_1 + ... + S_n\\) is also ancillary. Theorem 7.8 (Location-Scale family ancillary statistic) Let \\(\\textbf{X} = (X_1,..., X_n), X_i\\) iid \\(f_{\\mathcal{L,S}}(x|\\mu,\\sigma)\\), a location-scale family where \\(\\theta = (\\mu, \\sigma)\\). Let \\(T_1= T_1(\\textbf{X})\\) and \\(T_2= T_2(\\textbf{X})\\) be any two statistics such that \\[ (*) \\quad T_j(aX_1 + b, ..., aX_n+b) = aT_j(X_1,..., X_n) \\quad j = 1,2.\\] Then, \\(T_1/T_2\\) is ancillary for \\(\\theta\\) Example 7.8 Consider the range \\(R = X_{(n)} - X_{(1)}\\) and \\(S = \\sqrt{\\frac{1}{n-1}\\sum(X_1 -\\bar{X})^2}\\) satisfy (*) so \\(R/S\\) is ancillary for \\(\\theta = (\\mu, \\sigma)\\). 7.3.1 Why are we interested in ancillary statistics? There is some relationship between ancillarity and minimal sufficiency. Suppose there is a statistic \\(c=C(\\textbf{X})\\) and an ancillary statistic \\(S=S(\\textbf{X})\\) such that \\(T=(S,C)\\) is minimal sufficient. The ancillarity principle states that inference on the parameter should be based on the conditional distribution of \\(C\\) given the ancillary statistic \\(S\\). Furthermore, we will see later that ancillary statistics are independent of complete and sufficient statistics. Remark: Recall the \\(U(\\theta, \\theta + 1)\\) example in which we found that the MSS, T, is two dimensional, and therefore no sufficient estimator for \\(\\theta\\) exists. We can write \\(T = (S,C)\\) where \\(C\\) has a marginal distribution that is ancillary (independent of the parameter), and then \\(S\\) is conditionally sufficient, i.e. sufficient conditional on \\(C\\). Example 7.9 Let \\(N\\) be a random variable with known distribution, \\(P(N=n) = p_n\\), and let \\(X_1, ..., X_N\\) be iid with exponential family density. Then the likelihood of the data, \\((N, X_1,..., X_N)\\) is \\[p_n (\\prod_{i=1}^{n}h(x_i))c(\\theta)^{n} exp\\bigg\\{\\sum_{j=1}^{k} w_k(\\theta) \\sum_{i=1}^{n}t_j(x_i)\\bigg\\} \\] and thus \\(\\bigg\\{N, \\{\\sum_{i=1}^{N}t_j(x_i)\\}_{j=1}^{k}\\bigg\\}\\) is sufficient for \\(\\theta\\). Observe that \\(N\\) is ancillary (independent of \\(\\theta\\)) and \\(\\{\\sum_{i=1}^{N}t_j(x_i)\\}_{j=1}^{k}\\) is sufficient for \\(\\theta\\) conditional on \\(N\\). References "],["point-estimators-finite-samples.html", "Chapter 8 Point Estimators: Finite Samples 8.1 Method of Moments 8.2 MLE Theory 8.3 Unbiasedness 8.4 Minimum Variance (Cramer-Rao Lower Bound) 8.5 Uniform Minimum Variance Unbiased Estimators (UMVUEs) 8.6 Inferential Properties of Exponential Families Distributions", " Chapter 8 Point Estimators: Finite Samples Finding point estimators and evaluating their finite sample properties. 8.1 Method of Moments 8.2 MLE Theory Definition 8.1 (Likelihood) Let \\(X\\) be a random variable or random vector, and \\(\\theta\\) a parameter or vector of parameters describing the distribution of \\(X\\). Then, the likelihood is \\[\\mathcal{L}(\\theta | X) = f_X(x|\\theta)\\] Definition 8.2 (Log-Likelihood) Let \\(X\\) be a random variable or random vector, and \\(\\theta\\) a parameter or vector of parameters describing the distribution of \\(X\\). Then, the log-likelihood is \\[\\ell(\\theta|X) = \\log\\mathcal{L}(\\theta | X) = \\log f_X(x|\\theta)\\] The log-likelihood is often easier to use. Furthermore, When \\(X\\) is an iid sample \\(X_1, X_2, ... X_n\\), by the properties of logarithms, \\(\\ell(\\theta|X) = \\log\\Big(\\prod_{i=1}^nf_{X_i}(x|\\theta)\\Big) = \\sum_{i=1}^n \\log f_{X_i}(x|\\theta)\\). Working with a sum is much easier than with a product. Of course, the whole reason this is permissible in finding a maximum likelihood estimate is that the \\(\\log\\) function is monotonic - finding the maximum of a function is equivalent to finding the maximum of its logarithm. Maximizing this likelihood often requires calculus - specifically, the First Derivative Test. In statistics, the gradient of the log-likelihood has a special name: the score. Definition 8.3 (Score Equations) If \\(\\theta\\) is a vector of parameters, the score equations are defined as the gradient of the log-likelihood (as described above). That is, \\[U(\\theta | X) = \\begin{bmatrix}\\frac{\\partial}{\\partial\\theta_1}\\ell(\\theta_1|X) &amp; ... &amp; \\frac{\\partial}{\\partial\\theta_k}\\ell(\\theta_k|X) \\end{bmatrix}\\] Of course, if \\(\\theta\\) is a single parameter, then \\(U(\\theta|X) = \\frac{d}{d\\theta}\\ell(\\theta|X)\\) Therefore, the first step in finding an MLE is to set the score to 0, and solve for the desired parameter. Let’s see an example. Example 8.1 (MLE of _______) example The score has two useful properties in MLE theory. First, under regularity conditions, \\(E(U(\\theta|X)) = 0\\) - its mean is 0. This is because the regularity conditions imply Leibniz’s rule; that is, \\[\\begin{align} E(U(\\theta|X)) = \\int_{\\mathcal{X}}\\frac{\\partial}{\\partial\\theta}\\log\\mathcal{L}(\\theta|X)f(x|\\theta)dx \\\\ = \\int_{\\mathcal{X}}\\frac{1}{f(x|\\theta)}\\frac{\\partial}{\\partial\\theta}f(x|\\theta)f(x|\\theta)dx \\\\ = \\frac{\\partial}{\\partial\\theta}\\frac{f(x|\\theta)}{f(x|\\theta)}f(x|\\theta)dx = \\frac{\\partial}{\\partial\\theta}1 = 0 \\end{align}\\] The second useful property is that, under the same regularity conditions, the variance of the score is \\[\\begin{align} Var(U(\\theta|X)) = E(U(\\theta|X)U(\\theta|X)^\\top) \\\\ = -E\\Big(\\frac{\\partial^2}{\\partial\\partial^\\top}\\ell(\\theta|X)\\Big) \\end{align}\\] If \\(\\theta\\) is one-dimensional, \\(Var(U(\\theta|X)) = E(U(\\theta|X)^2) = -E\\Big(\\frac{\\partial^2}{\\partial\\theta^2}\\ell(\\theta|X)\\Big)\\). The rather insightful derivation of this is available here. This variance has a special name, the information (or sometimes, “Fisher Information”) Definition 8.4 (Information) In statistics, information refers to the amount of information that a random variable \\(X\\) contains about a parameter \\(\\theta\\). It is defined mathematically as the variance of the score, which is \\[I(\\theta) = E\\Big((\\frac{\\partial}{\\partial\\theta}\\log f(X|\\theta))^2\\Big|\\theta\\Big)\\] Under MLE regularity conditions, \\[I(\\theta) = -E\\Big(\\frac{\\partial^2}{\\partial\\theta^2}\\log f(X|\\theta) \\Big|\\theta\\Big)\\] As the mean value of a second derivative, the information measures the curve of \\(\\ell(\\theta|X)\\) (expand?) The negative second derivative of the log-likelihood also has a special name itself: the observed information. Definition 8.5 (Observed Information) The observed information is defined as \\[\\mathcal{J}(\\theta|X) = -\\frac{\\partial^2}{\\partial\\theta^2}\\ell(\\theta|X)\\] When \\(\\theta\\) is a \\(k\\)-dimensional vector, the observed information is the Hessian of the log-likelihood: \\[\\mathcal{J}(\\theta|X) = -\\nabla\\nabla^\\top\\ell(\\theta|X) = \\begin{bmatrix}\\frac{\\partial^2}{\\partial\\theta_1^2} &amp; ... &amp; \\frac{\\partial^2}{\\partial\\theta_1\\partial\\theta_k}\\\\ \\vdots &amp; &amp; \\vdots\\\\ \\frac{\\partial^2}{\\partial\\theta_k\\partial\\theta_1} &amp; ... &amp; \\frac{\\partial^2}{\\partial\\theta_k^2}\\end{bmatrix}\\] The observed information can be used to estimate the Fisher information for a given sample \\(X\\). 8.3 Unbiasedness 8.4 Minimum Variance (Cramer-Rao Lower Bound) 8.5 Uniform Minimum Variance Unbiased Estimators (UMVUEs) Definition 8.6 (Lehmann-Scheffe Theorem) This theorem 8.6 Inferential Properties of Exponential Families Distributions Suppose we draw a sample of \\(n\\) iid random variables \\(X_1,...,X_n\\) following one of the distributions below. The proceeding tables list the inferential properties of this sample. 8.6.1 Bernoulli Log-likelihood \\(\\ell(\\theta|X) = n\\log(1-p) + \\log(\\frac{p}{1-p})\\sum_{i=1}^n x_i\\) Score Equations \\(U_n(\\theta|X) = -\\frac{n}{1-p} + \\frac{1}{p(1-p)}\\sum_{i=1}^n x_i\\) Observed Information \\(-\\frac{\\partial^2}{\\partial\\theta^2}\\ell(\\theta|X) = \\frac{n}{(1-p)^2} + \\frac{1-2p}{p^2(1-p)^2}\\sum_{i=1}^n x_i\\) Fisher Information \\(I(\\theta)= \\frac{n}{p(1-p)}\\) MLE \\(\\frac{1}{n}\\sum_{i=1}^n x_i\\) 8.6.2 Binomial Since the Binomial has \\(n\\) as a parameter, notation in problems that involve a sample of \\(n\\) iid Binomial random variables can be tricky. To clarify, in the following table let \\(X_i \\sim \\text{Binomial}(m, p)\\), and let \\(n\\) represent the number of samples. Log-likelihood \\(\\ell(\\theta|X) = nm\\log(1-p) + \\log(\\frac{p}{1-p})\\sum_{i=1}^n x_i + \\log({m\\choose x_i})\\) Score Equations \\(U_n(\\theta|X) = -\\frac{nm}{1-p} + \\frac{1}{p(1-p)}\\sum_{i=1}^n x_i\\) Observed Information \\(-\\frac{\\partial^2}{\\partial\\theta^2}\\ell(\\theta|X) = \\frac{nm}{(1-p)^2} + \\frac{1-2p}{p^2(1-p)^2}\\sum_{i=1}^n x_i\\) Fisher Information \\(I(\\theta)= \\frac{nm}{p(1-p)}\\) MLE \\(\\frac{1}{nm}\\sum_{i=1}^n x_i\\) 8.6.3 Geometric Log-likelihood Score Equations Fisher Information MLE \\(n\\log(p) + \\log(1-p)\\sum_{i=1}^n x_i\\) \\(\\frac{n}{p} - \\frac{1}{1-p}\\sum_{i=1}^n x_i\\) \\(\\frac{n}{\\sum_{i=1}^n x_i}\\) 8.6.4 Negative Binomial Log-likelihood Score Equations Fisher Information MLE \\(nr\\log(\\frac{p}{1-p}) + \\sum_{i=1}^n x_i\\log(1-p) + \\log{x_i + r - 1\\choose x_i}\\) \\(-\\frac{nr}{p} - \\frac{1}{1-p}\\sum_{i=1}^n x_i\\) \\(\\frac{r}{(1-p)^2p}\\) \\(\\frac{1}{1 - \\frac{1}{nr}\\sum_{i=1}^nx_i}\\) 8.6.5 Poisson Log-likelihood Score Equations Fisher Information MLE \\(n\\lambda + \\sum_{i=1}^n x_i\\log(\\lambda) - \\log(x_i!)\\) \\(-n + \\frac{1}{\\lambda}x_i\\) \\(\\frac{1}{\\lambda}\\) \\(\\frac{1}{n}\\sum_{i=1}^nx_i\\) 8.6.6 Normal Log-likelihood Score Equations Fisher Information MLE \\(-\\frac{n}{2}\\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2}\\sum_{i=1}^n (x_i - \\mu)^2\\) \\(U(\\mu | x, \\sigma^2) = -n\\mu -\\frac{1}{\\sigma^2}\\sum_{i=1}^n x_i \\\\ U(\\sigma^2 | x, \\mu) = -\\frac{n}{2\\sigma^2} + \\frac{1}{2(\\sigma^2)^2}\\sum_{i=1}^n(x_i-\\mu)^2\\) \\(\\begin{bmatrix}\\frac{1}{\\sigma^2} &amp; 0 \\\\ 0 &amp; \\frac{1}{2\\sigma^4}\\end{bmatrix}\\) \\(\\frac{1}{n}\\sum_{i=1}^nx_i\\) 8.6.7 Exponential Log-likelihood Score Equations Fisher Information MLE \\(-n\\log(\\lambda) - \\frac{1}{\\lambda}\\sum_{i=1}^n x_i\\) \\(-\\frac{n}{\\lambda} + \\frac{1}{\\lambda^2}\\sum_{i=1}^n x_i\\) \\(\\lambda^2\\) \\(\\frac{1}{n}\\sum_{i=1}^nx_i\\) 8.6.8 Gamma Log-likelihood Score Equations Fisher Information MLE \\(-n\\log(\\Gamma(k)) - nk\\log(\\lambda) + (k - 1 - \\frac{1}{\\lambda})\\sum_{i=1}^n x_i\\) \\(-\\frac{nk}{\\lambda} + \\frac{1}{\\lambda^2}\\sum_{i=1}^n x_i\\) with \\(k\\) known (otherwise, requires differentiating \\(\\Gamma(k)\\)) \\(\\begin{bmatrix}\\psi^{(1)}(k) &amp; \\frac{1}{\\lambda}\\\\ \\frac{1}{\\lambda} &amp; \\frac{k}{\\lambda^2}\\end{bmatrix}\\) 8.6.9 Pareto Log-likelihood Score Equations Fisher Information MLE \\(n\\log{\\alpha} + n\\alpha\\log(x_m) - (\\alpha + 1)\\sum_{i=1}^n\\log(x_i)\\) \\(U(\\alpha | x_i) = \\frac{n}{\\alpha} + n\\log(x_m) - \\sum_{i=1}^n \\log(x_i)\\) \\(\\frac{n}{\\alpha^2}\\) \\(\\frac{n}{\\sum_{i=1}^n\\log(x_i)}\\) "],["point-estimators-asymptotics.html", "Chapter 9 Point Estimators: Asymptotics 9.1 Consistency 9.2 Asymptotic Efficiency 9.3 Asymptotic Properties of MLEs 9.4 Variance Stabilizing Transformations 9.5 Asymptotic Confidence Intervals", " Chapter 9 Point Estimators: Asymptotics Evaluating the asymptotic properties of point estimators. 9.1 Consistency An important asymptotic property of an estimator is that it converges in probability to the true value being estimated as \\(n \\rightarrow \\infty\\). This is called consistency. 9.1.1 Weak Law of Large Numbers The most common strategy for proving consistency is to use the Weak Law of Large Numbers. Theorem 9.1 (Weak Law of Large Numbers) iid version: If \\(Z_i\\) are iid with finite mean, then \\[\\bar{X} = \\frac{1}{n}\\sum_{i=1}^nX_i\\overset{p}{\\rightarrow}E(X_i)\\] Non-iid version: Alternatively, we can drop the iid assumption; if \\(X_i\\) have finite mean and variance, \\(Cov(X_i, X_j) = 0\\), and \\(\\lim_{n\\rightarrow\\infty}\\sum_{i=1}^n\\frac{\\sigma_i^2}{n^2} = 0\\), then \\[\\frac{1}{n}\\sum_{i=1}^nX_i - \\frac{1}{n}\\sum_{i=1}^nE(X_i) \\overset{\\mathcal{p}}{\\rightarrow} 0\\] If a statistic can be rewritten in the form of a sample mean, the WLLN can be combined with the Continuous Mapping Theorem to show a variety of estimators are consistent. One hint that the WLLN (or later, the Central Limit Theorem) may be employed is that the expression to be proven contains a sum. 9.1.2 Direct Proof via Convergence in Probability Sometimes, a statistic may not consistent of a sum of random variables. This often arises in the case of order statistics. In this case, the definition of convergence in probability must be used directly. To do this, a variety of arguments may be employed. Example 9.1 (How To Prove Convergence of a Sample Maximum) Let \\(X_i \\overset{iid}{\\sim} U(a, b)\\). We can prove that \\(X_{(n)}\\overset{p}{\\rightarrow}b\\) via a direct probability argument using disjointification. By the definition of the order statistic cdf, \\[ P(|X_{(n)} - b| &gt; \\varepsilon) = P(X_{(n)} &gt; \\varepsilon + b) + P(X_{(n)} &lt; b - \\varepsilon) \\\\ = 0 + (F_{X_i}(x))^n = (\\frac{b - \\varepsilon - a}{b - a})^n = (1 -\\frac{\\varepsilon}{b - a})^n \\] The 0 arises because \\(X_{(n)} &lt; b\\) by the definition of the Uniform. As \\(\\varepsilon\\) is taken to be small, \\(\\lim_{n\\rightarrow \\infty}(1 -\\frac{\\varepsilon}{b - a})^n = 0\\). Therefore, since \\(P(|X_{(n)} - b| &gt; \\varepsilon) = 0\\), we have proven \\(X_{(n)}\\overset{p}{\\rightarrow}b\\) 9.2 Asymptotic Efficiency In addition to checking that an estimator converges to the correct value, we are also often concerned with the estimator’s variance as \\(n \\rightarrow \\infty\\). Often, estimators converge asymptotically to a normal distribution. If an estimator \\(T_n(X)\\) has the property \\(k_n(T_n(X) - \\tau(\\theta)) \\overset{\\mathcal{D}}{\\rightarrow} N(0,\\sigma^2)\\), then \\(\\sigma^2\\) is called the asymptotic variance (Casella and Berger 1990). Furthermore, \\(T_n(X)\\) is asymptotically efficient* if \\(\\sigma^2\\) achieves the Cramer-Rao Lower Bound. If \\(T_n(X)\\) is one-dimensional, then the asymptotic efficiency** of \\(T_n(X)\\) can be computed as the ratio \\[AE(\\theta, T_n) = \\frac{(\\tau&#39;(\\theta))^2}{\\mathcal{I}(\\theta)\\sigma^2}\\] If \\(T_n(X)\\) is \\(k\\)-dimensional, let \\(d = \\begin{bmatrix}\\frac{\\partial}{\\partial\\theta_1}\\tau(\\theta),...,\\frac{\\partial}{\\partial\\theta_k}\\tau(\\theta)\\end{bmatrix}\\). Then, the asymptotic efficiency is \\[AE(\\theta, T_n) = \\frac{d&#39;\\mathcal{I}(\\theta)^{-1}d}{\\sigma^2}\\] We can also compare estimators via their asymptotic relative efficiency (ARE), which for estimators \\(S_n\\) and \\(T_n\\) is \\[ARE(\\theta, S_n, T_n) = \\frac{\\sigma_T^2}{\\sigma_S^2}\\] Let’s explore first how we might prove that an estimator converges asymptotically to a normal distribution to show this directly. 9.2.1 Central Limit Theorems These theorems are used to show asymptotic normality. There are many different types; let us focus on the three most common. Theorem 9.2 (Central Limit Theorem (iid)) If \\(X_i\\) are iid with finite first and second moments (\\(E(X_i), Var(X_i) &lt; \\infty\\)) then \\[\\sqrt{n}\\Big(\\frac{1}{n}\\sum_{i=1}^nX_i - E(X_i)\\Big) \\overset{\\mathcal{D}}{\\rightarrow} N(0,Var(X_i))\\] Theorem 9.2 (Lyapunov's Central Limit Theorem (non-iid)) Consider \\(X_i\\) that are independent with finite first and second moments (\\(E(X_i), Var(X_i) &lt; \\infty\\)). Let \\(S_n^2 = \\sum_{i=1}^n Var(X_i)\\). Then, for some \\(\\delta &gt; 0\\), if the condition \\[\\lim_{n\\rightarrow \\infty} \\frac{1}{S_n^{2 + \\delta}}\\sum_{i=1}^n E(|X_i - E(X_i)|^{2 + \\delta}) = 0\\] holds, then we know \\[\\frac{1}{S_n}\\sum_{i=1}^n (X_i - E(X_i)) \\overset{\\mathcal{D}}{\\rightarrow} N(0,1)\\] In practice, to prove that Lyapunov’s CLT holds true, we typically take \\(\\delta = 1\\) and compute the third moments contained in the Lyapunov condition. Alternatively, we can use Lindeberg’s CLT for non-iid data: Theorem 9.2 (Lindeberg's Central Limit Theorem (non-iid)) Like the Lyapunov CLT, consider \\(X_i\\) that are independent with finite first and second moments (\\(E(X_i), Var(X_i) &lt; \\infty\\)). Let \\(S_n^2 = \\sum_{i=1}^n Var(X_i)\\). Then if \\[\\lim_{n\\rightarrow \\infty} \\frac{1}{S_n^{2}}\\sum_{i=1}^n E((X_i - E(X_i))^{2})\\cdot I(|X_i - E(X_i) &gt; \\varepsilon S_n) = 0\\] holds, then we know \\[\\frac{1}{S_n}\\sum_{i=1}^n (X_i - E(X_i)) \\overset{\\mathcal{D}}{\\rightarrow} N(0,1)\\] 9.2.2 The Delta Method What if our estimator is not a sample mean? In this case, if it is a function of a sample mean, we can still use the Delta Method to prove that it converges asymptotically to either a Normal or a \\(\\chi^2\\) distribution. Theorem 9.3 (Delta Method) If \\(\\sqrt{n}(\\hat{\\theta}_n - \\theta) \\overset{\\mathcal{D}}{\\rightarrow} N(0,\\sigma^2)\\), then for a continuous \\(g\\) with continuous nonzero derivative in an interval containing \\(\\theta\\)… \\(\\sqrt{n}\\Big(g(\\hat{\\theta}_n) - g(\\theta)\\Big) \\overset{\\mathcal{D}}{\\rightarrow} N\\Big(0, \\sigma^2\\cdot (\\frac{d}{d\\theta}g(\\theta))^2\\Big)\\) (First-Order Delta Method) \\(n\\Big(g(\\hat{\\theta}_n) - g(\\theta)\\Big) \\overset{\\mathcal{D}}{\\rightarrow} \\frac{1}{2}\\sigma^2\\cdot g&#39;&#39;(\\theta)\\cdot\\chi^2(1)\\) (Second-Order Delta Method) Generally, the Second-Order Delta Method is required if \\(g&#39;(\\theta) = 0\\). Theorem 9.4 (Multivariate Delta Method) If \\(\\sqrt{n}(\\hat{\\theta}_n - \\theta) \\overset{\\mathcal{D}}{\\rightarrow} N(0,V)\\), where \\(V\\) is a \\(k \\times k\\) matrix, then for a continuous real-valued function \\(g(x)\\) of \\(k\\) variables with continuous nonzero first partial derivatives, \\[\\sqrt{n}\\Big(g(\\hat{\\theta}_n) - g(\\theta)\\Big) \\overset{\\mathcal{D}}{\\rightarrow} MVN_k\\Big(0, u&#39;Vu\\Big)\\] where \\(u = \\begin{bmatrix}\\frac{\\partial}{\\partial \\theta_1}g(\\theta),...,\\frac{\\partial}{\\partial \\theta_k}g(\\theta)\\end{bmatrix}\\) 9.2.3 Cramer-Wold Device One final technique for proving convergence in distribution is to use the Cramer-Wold Device. Theorem 9.5 (Cramer Wold Device) For a sequence of random vectors \\(X_n\\), a random vector \\(X\\), and a vector \\(a\\) of constants, \\[X_n \\overset{\\mathcal{D}}{\\rightarrow} \\iff a&#39;X_n \\overset{\\mathcal{D}}{\\rightarrow}a&#39;X, \\forall a\\] In other words, we can show convergence in distribution of a random vector by showing that every linear combination of that random vector converges in distribution to the same linear combination of \\(X\\). The Cramer-Wold Device allows us to convert a problem involving convergence of random vectors into a problem involving convergence of a single random variable. 9.2.4 Asymptotic Distribution in Practice. Statistical problems are often more complicated than simply applying the CLT or Delta Method. Often, we may need to consider the convergences of other random variables. To do this, we can combine the CLT/Delta Method with Slutsky’s Theorem or the Continuous Mapping Theorem to prove desired results. 9.3 Asymptotic Properties of MLEs While the CLT and Delta Method are extremely useful, if you are working with MLEs, it can often be faster to rely on their known properties. For \\(X_i\\) satisfying a certain set of regularity conditions (where? better link), the MLE $_n has the two import properties discussed above: Consistency: \\(\\hat{\\theta}_n \\overset{p}{\\rightarrow} \\theta\\) Asymptotic Efficiency: \\(\\sqrt{n}(\\hat{\\theta}_n - \\theta) \\overset{\\mathcal{D}}{\\rightarrow} N(0, \\mathcal{I}(\\theta_0)^{-1})\\), the Cramer-Rao Lower Bound This holds true even for multi-parameter MLEs. Note that the regularity conditions are met by the MLE of all exponential families for which \\(\\nu \\in \\Theta \\subset \\mathbb{R}\\) is an open set, which can be useful in problem-solving. Estimating \\(\\mathcal{I}(\\theta_0)\\) can be performed in two ways: (is this correct?) \\(-\\frac{1}{n} \\frac{\\partial^2}{\\partial\\theta^2}\\ell(\\hat{\\theta}_n | X_n) \\overset{p}{\\rightarrow} \\mathcal{I}(\\theta_0)\\) \\(\\frac{1}{n} I_n(\\hat{\\theta}_n) \\overset{p}{\\rightarrow} \\mathcal{I}(\\theta_0)\\) 9.4 Variance Stabilizing Transformations Sometimes, the asymptotic variance of a distribution may depend on the value of a particular parameter. This may be undesirable because the parameter might be unknown. In this case, we can perform a variance-stabilizing transformation to remove the variance. This is done by setting \\(f&#39;(\\theta) = \\frac{c}{\\tau(\\theta)}\\). Then, for \\(T_n(X)\\) such that \\(\\sqrt{n}(T_n - \\theta) \\overset{\\mathcal{D}}{\\rightarrow} N(0, \\tau^2(\\theta))\\), by the Delta Method we get \\[\\sqrt{n}(f(T_n) - f(\\theta)) \\overset{\\mathcal{D}}{\\rightarrow} N(0, f&#39;(\\theta)^2 \\cdot\\tau^2(\\theta)) = N(0, c)\\] 9.5 Asymptotic Confidence Intervals References "],["hypothesis-tests-finite-samples.html", "Chapter 10 Hypothesis Tests: Finite Samples", " Chapter 10 Hypothesis Tests: Finite Samples Constructing finite-sample hypothesis tests and evaluating their properties. "],["hypothesis-tests-asymptotics.html", "Chapter 11 Hypothesis Tests: Asymptotics 11.1 Wald Test 11.2 Score Test 11.3 Likelihood Ratio Test 11.4 Composite Null Hypotheses", " Chapter 11 Hypothesis Tests: Asymptotics Constructing a finite-sample hypothesis test requires deriving the full distribution of the test statistic, which may be difficult. Oftentimes, however, we can use the Central Limit Theorem and other asymptotic tools to prove that, as \\(n \\rightarrow \\infty\\), a test statistic converges to either a standard normal or chi-squared distribution. This permits the construction of asymptotic hypothesis tests. This section will focus on constructing and evaluating tests based on the MLE \\(\\hat{\\theta}_n\\) when regularity conditions are met under both the null hypothesis \\(H_0\\) and the alternative \\(H_1\\).. In this scenario, there are three possible tests that may be constructed: Wald Test Score Test Likelihood Ratio Test All of these tests are asymptotically equivalent. One-sided tests are generally based on a Normal approximation, while two-sided are based on a \\(\\chi^2\\) approximation. Let’s discuss each in detail, including their strengths and weaknesses. 11.1 Wald Test Definition 11.1 (Wald Test) The one-sided, one-dimensional Wald test is constructed based on \\[W_n = (\\hat{\\theta}_n - \\theta_0)(I_n(\\theta_0))^\\frac{1}{2} \\approx N(0,1)\\] The two-sided one-dimensional Wald test is constructed based on the square of this; that is, \\[W_n = (\\hat{\\theta}_n - \\theta_0)^2(I_n(\\theta_0)) \\approx \\chi^2(1)\\] It can be extended to the multidimensional setting by representing \\(\\hat{\\theta}_n\\) and \\(\\theta\\) as \\(k\\)-dimensional vectors and \\(I_n(\\hat{\\theta}_0)\\) a \\(k \\times k\\) matrix. \\[W_n = (\\hat{\\theta}_n - \\theta_0)^\\top(I_n(\\theta_0))(\\hat{\\theta}_n - \\theta_0) \\approx \\chi^2_k\\] which follows from the fact that \\(\\hat{\\theta}_n \\approx MVN_k(\\theta_0, (I_n(\\theta_0))^{-1})\\) To obtain \\(I_n(\\theta_0)\\), we can estimate it using either… \\(I_n(\\hat{\\theta}_n)\\); simply plug the MLE into the expected information. \\(-\\frac{\\partial^2}{\\partial\\theta^2}\\ell(\\hat{\\theta}_n | X_n)\\); plug the MLE into the observed information However, \\(I_n(\\hat{\\theta}_0)\\) is generally preferred, because it is more efficient. Advantages: - Simple to compute if you know the form of the MLE - Constructing confidence intervals is easy Disadvantages: - Requires knowing the form of the MLE - Not invariant if you transform the MLE - Approximation may not be as accurate as the other two tests 11.2 Score Test Definition 11.2 (Score Test) The one-sided, one-dimensional Score Test is constructed by \\[S_n = \\Big(\\frac{\\partial}{\\partial \\theta}\\ell(\\theta_0|X_n)\\Big)(I_n(\\theta_0))^{-\\frac{1}{2}}\\] The two-sided, one-dimensional Score test is constructed based on its square: \\[S_n = \\Big(\\frac{\\partial}{\\partial\\theta}\\ell(\\theta_0 | X_n)\\Big)^2(I_n(\\theta_0))^{-1}\\] It can be extended to the multivariate setting by representing \\(U_n(\\theta_0) = \\begin{bmatrix}\\frac{\\partial}{\\partial\\theta_1}\\ell(\\theta|X_n) &amp; ... &amp; \\frac{\\partial}{\\partial\\theta_k}\\ell(\\theta|X_n) \\end{bmatrix}\\) and \\(I_n(\\hat{\\theta}_0)\\) a \\(k \\times k\\) matrix: \\[(U_n(\\theta_0))^\\top(I_n(\\theta_0))^{-1}(U_n(\\theta_0)) \\approx \\chi^2_k\\] which follows from the fact that \\(U_n(\\theta_0) \\approx MVN_k(0, I_n(\\theta_0))\\) Advantages: - Not actually necessary to know the form of the MLE to construct the test - all that is needed is the score equation and information under \\(H_0\\) - More computationally efficient as a result Disadvantages: - Constructing confidence intervals is more complicated, since inverting the Score test statistic may be challenging - Not invariant to under reparametrization 11.3 Likelihood Ratio Test Definition 11.3 (Likelihood Ratio Test) The one-dimensional asymptotic Likelihood Ratio Test (LRT) is constructed based on \\[Q_n = 2\\log\\Big(\\frac{\\mathcal{L}(\\hat{\\theta}_n)}{\\mathcal{L}(\\theta_0)}\\Big) = 2(\\ell(\\hat{\\theta}_n|X_n) - \\ell(\\theta_0|X_n)) \\approx \\chi^2(1)\\] This can be extended to multiple dimensions by letting \\(\\ell(\\theta|X_n)\\) by a \\(k\\)-dimensional vector. Then, \\[Q_n = 2(\\ell(\\hat{\\theta}_n|X_n) - \\ell(\\theta_0|X_n)) \\approx\\chi^2(k)\\] Advantages: - No derivatives need to be calculated - Invariant to transformations of the MLE - Provides the most accurate approximation, especially if we decide to reparametrize the model, in which case the derivative in the Wald/Score tests would need to be recomputed. Disadvantages: - Inverting the test to construct a confidence interval is difficult - Requires knowledge of the log-likelihood under both the null hypothesis \\(H_0\\) and the alternative \\(H_1\\). 11.4 Composite Null Hypotheses A composite null hypothesis involves multiple parameters, some of which are nuisance parameters. Suppose we have a null hypothesis \\(H_0: \\alpha = \\alpha_0\\), where \\(\\alpha\\) is a vector of length \\(k\\), and we have a vector \\(\\beta\\) of nuisance parameters. In a composite null, we can partition \\(\\theta^\\top = (\\alpha^\\top, \\beta^\\top)\\). ::: {#adjusted-information .definition name=“Adjusted Information”} To use a normal asymptotic approximation in this situation, it is necessary to compute the “adjusted information” \\(I_{n,\\alpha\\alpha|\\beta}(\\alpha, \\beta)\\) - the information for \\(\\alpha\\), conditional on our \\(\\beta\\) estimate: \\[I_{n,\\alpha\\alpha|\\beta}(\\alpha, \\beta) = I_{n,\\alpha\\alpha}(\\alpha, \\beta) - I_{n,\\alpha\\beta}(\\alpha, \\beta)(I_{n,\\beta\\beta}(\\alpha, \\beta))^{-1}I_{n,\\beta\\alpha}(\\alpha, \\beta)\\] :: The adjusted information is derived from the block-matrix partition formula for the partition \\[I_n(\\alpha, \\beta) = \\begin{bmatrix} I_{n,\\alpha\\alpha}(\\alpha, \\beta) &amp; I_{n,\\alpha\\beta}(\\alpha, \\beta) \\\\ I_{n,\\beta\\alpha}(\\alpha, \\beta) &amp; I_{n,\\beta\\beta}(\\alpha, \\beta) \\\\\\end{bmatrix}\\] With the adjusted information computed, the previously-discussed asymptotic tests become the following: Wald: \\(W_n = (\\hat{\\alpha}_n - \\alpha_0)^\\top I_{n,\\alpha\\alpha|\\beta}(\\hat{\\alpha}_n, \\hat{\\beta}_n)(\\hat{\\alpha}_n - \\alpha_0)\\) Score: \\(S_n = U_{n,\\alpha}(\\alpha_0, \\hat{\\beta}_n)^\\top I_{n,\\alpha\\alpha|\\beta}(\\alpha_0, \\hat{\\beta}_n)U_{n,\\alpha}(\\alpha_0, \\hat{\\beta}_n)\\) Likelihood Ratio: \\(Q_n = 2(\\ell(\\hat{\\alpha}_n, \\hat{\\beta}_n) - \\ell(\\alpha_0, \\hat{\\beta}_n))\\) all of which follow a \\(\\chi^2(k)\\) distribution, where \\(k\\) is the number of parameters being tested in \\(\\alpha\\). "],["generating-random-variables.html", "Chapter 12 Generating Random Variables", " Chapter 12 Generating Random Variables What the title says. "],["random-processes.html", "Chapter 13 Random Processes 13.1 Poisson Processes 13.2 Branching Processes", " Chapter 13 Random Processes This section elaborates on two types of random processes: poisson processes and branching processes. 13.1 Poisson Processes A Poisson Process is a model for a series of discrete events where the average time between events is known, but the exact timing of events is random. A poisson process has the following properties: Events are independent of each other. The average rate (events per time period) is constant. Two events cannot occur at the same time. 13.1.1 Memorylessness of the Exponential Recall that the exponential distribution is memoryless, meaning \\(P(X&gt;x+a \\mid X&gt;a)=P(X&gt;x)\\). The memoryless property of the exponential distribution applies to waiting times in a Poisson process. It means that the time between events remains independent of past events, allowing us to predict future waiting times solely based on the average rate of event occurrences (i.e., the interarrival times between events are i.i.d.). 13.1.2 Count-Time Duality \\[ \\{T_n&gt;t\\}=\\{N_t&lt;n\\} \\] In words, the following 2 statements are equivalent: \\(T_n\\) (time to the \\(n^{th}\\) event) is greater than some fixed time \\(t\\) \\(N_t\\) (number of events up to time \\(t\\)) is less than some fixed number \\(n\\) \\[ \\int_t^{\\infty} \\underbrace{\\frac{1}{\\Gamma(n)\\lambda^{-n}} x^{n-1} e^{-\\lambda x}}_{T_n \\sim \\text{Gamma}(n,\\lambda)} d x \\quad = \\quad \\sum_{x=0}^{n-1} \\underbrace{\\frac{e^{-\\lambda t} (\\lambda t)^x}{x!}}_{N_t \\sim \\text{Poisson}(\\lambda t)} \\] 13.1.3 Poisson Distribution Suppose that we are interested in the expected number of events that will occur over a particular interval. The probability of observing a particular number of events can be modeled using the (discrete) poisson distribution: \\(X=\\) Discrete number of events occurring over a finite interval Moments: \\(\\mathbb{E}[X]=\\lambda\\), \\(\\operatorname{Var}(X)=\\lambda\\) \\(\\lambda=\\) Expected number of events over interval \\(=\\underbrace{\\frac{Events}{Time}}_{Rate}\\times Time\\)   13.1.4 Exponential Distribution Suppose that we are interested in the expected time before the next event. The probability of observing a particular time before the next event can be modeled using the (continuous) exponential distribution: \\(X=\\) Continuous time between events Moments: \\(\\mathbb{E}[X]=\\frac{1}{\\lambda}\\), \\(\\operatorname{Var}(X)=\\frac{1}{\\lambda^2}\\) \\(\\lambda=\\) Rate of events \\(=\\underbrace{\\frac{Events}{Time}}_{Rate}\\) Note: There is an inverse relationship between the rate of events (\\(\\lambda\\)) and expected time before the next event (\\(x\\)). As the rate of events (\\(\\lambda\\)) increases, the time before the next event (\\(x\\)) decreases.     13.1.5 Example Consider a Poisson process \\((\\lambda)\\) with a twist: After every event there is a guaranteed period of length \\(\\nu\\) during which no event can occur. Typical Poisson process: Distribution of time between events: \\(T_n-T_{n-1} \\sim \\operatorname{Exp}(\\lambda)\\) Distribution of time to the \\(n^{th}\\) event: \\(\\sum_{i=1}^n T_i \\sim \\operatorname{Gamma}(n, \\lambda)\\) Poisson process with a twist: Distribution of time between events: \\(T_n-T_{n-1} \\sim \\operatorname{Exp}(\\lambda)+\\nu\\) \\[ f_{T_n-T_{n-1}}(x)=\\lambda e^{-\\lambda(x-\\nu)} \\] Distribution of time to the \\(n^{th}\\) event: \\(\\sum_{i=1}^n T_i \\sim \\operatorname{Gamma}(n, \\lambda)+n \\nu\\) \\[ f_{T_n}(x)=\\frac{1}{\\Gamma(n) \\lambda^{-n}}(x-n \\nu)^{n-1} e^{-\\lambda(x-n \\nu)} \\] Further, by count time duality, we can write: \\[ P(T_n&gt;t)=P(N_t&lt;n) \\] \\[ \\int_t^{\\infty} \\frac{1}{\\Gamma(n)\\lambda^{-n}} (x-n \\nu)^{n-1} e^{-\\lambda (x-n \\nu)} d x \\quad = \\quad \\sum_{x=0}^{n-1} \\frac{e^{-\\lambda (t-n \\nu)} (\\lambda (t-n \\nu))^x}{x!} \\] 13.2 Branching Processes 13.2.1 Random Variables We’ll start by defining the random variables that characterize a branching process. \\(X_n\\) = size of the population at time (or generation) \\(n\\) \\(Y_{i,n-1}\\) = number of offspring produced by the \\(i^{th}\\) individual in generation \\(n-1\\)   How are random variables \\(X_n\\) and \\(Y_{i,n-1}\\) related? Intuitively, we can think of the population size of the current generation (\\(n\\)) as the sum of the offspring produced by each individual in the previous generation (\\(n-1\\)): \\[ \\begin{aligned} X_n &amp;= \\sum_{i=1}^{X_{n-1}} Y_{i,n-1}\\\\ &amp;= Y_{1,n-1} + Y_{2,n-1} +...+Y_{X_{n-1},n-1} \\end{aligned} \\] 13.2.2 Probability Generating Function Now we’ll define the Probability Generating Function (PGF), which allows us to compute probabilities from branching processes. \\[ g_X(t)=\\mathbb{E}_X\\left(t^X\\right)=\\sum_{n=0}^{\\infty} t^n P(X=n) \\] Similar to MGFs, independent random variables can be convoluted by multiplying their PGFs. For i.i.d. \\(Y_{i,j}\\), the PGF gives the first two moments: \\[ \\begin{aligned} \\mathbb{E}\\left(X_n\\right) &amp; =[\\mathbb{E}(Y)]^n \\\\ \\operatorname{Var}\\left(X_n\\right) &amp; =\\operatorname{Var}(Y)\\left(\\sum_{i=n-1}^{2(n-1)}[\\mathbb{E}(Y)]^i\\right) \\end{aligned} \\]   Asymptotics exists for Branching Processes on i.i.d. \\(\\left\\{Y_{i,j}\\right\\}\\) with finite variance: \\[ \\begin{gathered} \\text{Population-Level} \\quad \\quad \\text{Individual-Level} \\\\ \\\\ \\mathrm{E}\\left(X_n\\right) \\rightarrow \\begin{cases}0 &amp; \\mathrm{E}(Y)&lt;1 \\\\ 1 &amp; \\mathrm{E}(Y)=1 \\\\ \\infty &amp; \\mathrm{E}(Y)&gt;1\\end{cases} \\\\ \\\\ \\operatorname{Var}\\left(X_n\\right) \\rightarrow \\begin{cases}0 &amp; \\mathrm{E}(Y)&lt;1 \\\\ \\infty &amp; \\mathrm{E}(Y) \\geq 1\\end{cases} \\end{gathered} \\] 13.2.3 Criticality Theorem Criticality Theorem states that the probability of ultimate extinction of a branching process is the smallest solution to \\(\\eta=g_Y(\\eta)\\). In general, we can solve for \\(\\eta\\) using the quadratic equation: \\[ \\begin{aligned} &amp;0=a\\eta^2+b\\eta+c\\\\ \\\\ &amp;\\eta=\\frac{-b \\pm \\sqrt{b^2-4 a c}}{2 a} \\end{aligned} \\] 13.2.4 Example Consider a Branching Process where individuals duplicate with probability \\(p\\) and die with probability \\(q\\). Describe the mean and variance over time for this branching process. For what values of \\(p\\) will the process go extinct with probability 1? Establish the probability of eventual extinction for arbitrary \\(p\\).   Step 1: Define the pmf of \\(Y\\) based on the given reproduction probabilities. Does \\(Y\\) follow a known distribution? \\(f_y(y)=\\left\\{\\begin{array}{ll}y=0 &amp; \\text { w.p. } q=1-p \\\\ y=2 &amp; \\text { w.p. } p\\end{array} \\quad \\Rightarrow \\quad y \\sim 2\\right.\\) Bernoulli\\((p)\\)   Step 2: Calculate the moments of \\(Y\\), which can be used to calculate the moments of \\(X_n\\). \\[ \\begin{aligned} \\mathbb{E}[Y]=2 p \\quad \\quad \\quad \\quad \\mathbb{E}\\left[X_n\\right] &amp; =(2 p)^n \\\\ \\operatorname{Var}(Y)=4 p q \\quad \\quad \\operatorname{Var}\\left(X_n\\right) &amp; =\\operatorname{Var}(y) \\sum_{i=n-1}^{2(n-1)} \\mathbb{E}[Y]^i \\\\ &amp; =4 p q \\sum_{i=n-1}^{2(n-1)}(2 p)^i \\end{aligned} \\] Based on our asymptotic results, we know that the process \\(X_n\\) will go extinct with probability 1 if \\(\\mathbb{E}[Y]&lt;1\\) \\[ \\mathbb{E}[Y]=2 p \\Rightarrow \\text { process will go extinct with probability 1 if } p&lt; \\frac{1}{2} \\]   Step 3: Find the probability of ultimate extinction using Criticality Theorem. First, find the PGF of \\(Y\\): \\[ \\begin{aligned} g_Y(\\eta) =\\mathbb{E}\\left[\\eta^Y\\right]&amp;=\\sum_Y \\eta^Y f_Y(y) \\\\ &amp; =\\eta^2 P(Y=2)+\\eta^0 P(Y=0) \\\\ &amp; =\\eta^2 p+q \\end{aligned} \\] Second, find the probability of ultimate extinction, which is the smallest solution to \\(\\eta=g_Y(\\eta)\\): \\[ \\begin{aligned} \\eta&amp;=\\eta^2 p+q \\\\ 0&amp;=\\eta^2 p-\\eta+q\\\\ \\\\ a&amp;=p, b=-1, c=q\\\\ \\\\ \\eta&amp;= \\frac{1 \\pm \\sqrt{1-4pq}}{2p} \\\\&amp;= \\frac{1 \\pm \\sqrt{1-4p(1-p)}}{2p} \\\\&amp;= \\frac{1 \\pm \\sqrt{1-4p+4p^2}}{2p} \\\\&amp;= \\frac{1 \\pm \\sqrt{(2p-1)^2}}{2p} \\\\&amp;= \\frac{1 \\pm (2p-1)}{2p}\\\\ &amp;=\\text{min}\\left(1 \\quad \\text{or} \\quad \\frac{1}{p}-1\\right) \\end{aligned} \\] For \\(p \\leq \\frac{1}{2}, 1\\) is the minimum. Thus, \\(\\eta=P(\\text{Extinction})=1.\\) "],["references.html", "References", " References "]]
