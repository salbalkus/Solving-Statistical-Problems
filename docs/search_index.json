[["index.html", "Solving Statistical Problems Chapter 1 Introduction", " Solving Statistical Problems Salvador Balkus, Kimberly Greco, and Mónica Robles Fontán 2023-07-17 Chapter 1 Introduction Welcome to Solving Statistical Problems, a compilation of problem-solving tools and tricks for graduate-level Probability and Statistical Inference courses. Many of our examples originate from or are inspired by Casella and Bergers Statistical Inference, The Book of Statistical Proofs, lectures slides and notes from BIOSTAT 230: Probability I and BIOSTAT 231: Statistical Inference I at the Harvard Department of Biostatistics, and other online resources. The goal of this site is to provide a supplement to the aforementioned works providing a better explanation of how to actually solve problems using the material presented. "],["math-tricks.html", "Chapter 2 Math Tricks 2.1 Combinatorics 2.2 Binomial and Multinomial Theorems 2.3 Geometric Series 2.4 Taylor Series for Exponential Function 2.5 Taylors Formula 2.6 Exponential Limit 2.7 Integration by Parts 2.8 Leibnizs Rule 2.9 Fubinis Theorem 2.10 Gamma Function 2.11 Triangle Inequality", " Chapter 2 Math Tricks This chapter will cover highly specific mathematical techniques used to solve stats problems but themselves require no statistical knowledge, i.e Exponential function as limits/series Inductive integration by parts substitution tricks for integration binomial theorem Gamma function properties Matrices (determinants, jacobians, etc.) Taylor Series (univariate and multivariate) Lagrange multipliers? (Maybe save for MLEs) Open versus closed intervals 2.1 Combinatorics 2.2 Binomial and Multinomial Theorems The binomial theorem states that \\[(x + y)^n = \\sum_{k=0}^n {n\\choose k}x^ky^{n-k}\\] It can be used to prove that the pmf of a Binomial Random Variable sums to 1. Analogously, the multinomial theorem states \\[(x_1 + x_2 + ... + x_m)^n = \\sum_{k_1, k_2,...,k_m \\geq 0}\\frac{n!}{k_1!k_2!...k_m!}\\prod_{i=1}^nx_i^{k_i}\\] provided \\(\\sum_{i=1}^m k_i = n\\). It can be used to prove that the pmf of a Multinomial Random Variable sums to 1. 2.3 Geometric Series The geometric series is a useful series convergence, defined as follows: \\[\\sum_{x=0}^\\infty ar^x = \\frac{a}{1-r} \\text{ for } |r| &lt; 1\\] One place it arises is computing the moments of the Geometric Distribution. 2.4 Taylor Series for Exponential Function Some distributions such as the Poisson rely on infinite sums. Often, we can simplify these infinite sums to an exponential by rewriting them using the following series convergence: \\[\\sum_{x=0}^\\infty \\frac{\\lambda^x}{x!} = \\exp(\\lambda)\\] This is the Taylor series for the exponential function, sometimes called the exponential series. 2.5 Taylors Formula If the \\((n+1)\\)th derivative of a function \\(f\\) exists for all \\(\\xi \\in (c, x)\\), then $$ f(x) = f(c) + f(c)(x-c) + (x-c)^2 + \\ + (x - c)^n + R_n(x) $$ where \\[R_n(x) = \\frac{f^{(n+1)}(\\xi)}{(n+1)!}(x - c)^{n+1}\\] This can be found on page 550 of Keisler (2013). Taylors formula is key to proofs of several important statistical theorems, including the Delta Method and the asymptotic properties of the MLE. Do note the following points of confusion: - Although it is based on Taylors Formula, this is not identical to the infinite Taylor Series used to define functions like the exponential as mentioned previously. Taylors Formula terminates with \\(R_n(x)\\), a function defining the remaining error when approximating a function by a finite number of derivatives. This property is key, because showing that \\(R_n(x) \\rightarrow 0\\) allows asymptotic proofs to be developed. - The aforementioned asymptotic proofs typically only rely on \\(n = 1\\) or \\(n = 2\\), with the remainder appearing only after the second or third term. 2.6 Exponential Limit Another way to express the exponential function is by the limit \\[\\exp(x) = \\lim_{n\\rightarrow \\infty}(1 + \\frac{x}{n})^n\\] 2.7 Integration by Parts Another technique, especially for computing moments, is integration by parts, defined by: \\[\\int udv = uv - \\int vdu\\] Integration by parts is typically used for products of functions. By setting \\(u\\) equal to a function which has a finite number of nonzero derivatives (for example, \\(x^k\\)), and \\(v\\) equal to a function which does not (such as \\(e^x\\)), this technique can be repeatedly applied to compute integration. Of course, take caution: If both functions can be repeatedly differentiated infinitely, this may not work! However, it may be possible to use inductive integration by parts to prove that if we apply integration by parts infinitely, the result will yield a series which converges to some value (where?) 2.8 Leibnizs Rule Theorem 2.1 (Leibniz's Rule: Simple Version) If \\(a, b\\) are constant and \\(f(x, \\theta)\\) is differentiable w.r.t to \\(\\theta\\), then \\[\\frac{d}{d\\theta} \\int_{a}^{b} f(x,\\theta)dx = \\int_a^b \\frac{\\partial}{\\partial\\theta}f(x,\\theta)dx\\] Theorem 2.2 (Leibniz's Rule: Complicated Version) If \\(a(\\theta)\\), \\(b(\\theta)\\), and \\(f(x, \\theta)\\) are differentiable w.r.t to \\(\\theta\\), then \\[\\frac{d}{d\\theta}\\int_{a(\\theta)}^{b(\\theta)}f(x,\\theta)dx = f(b(\\theta), \\theta)\\frac{d}{d\\theta}b(\\theta) - f(a(\\theta), \\theta)\\frac{d}{d\\theta}a(\\theta) + \\int_{a(\\theta)}^{b(\\theta)} \\frac{\\partial}{\\partial\\theta}f(x,\\theta)dx\\] 2.9 Fubinis Theorem One useful problem-solving technique is exchanging the order of integrals or infinite series to more easily solve an equation. But when can we do this? Theorem 2.3 (Fubini's Theorem: Integrals) \\(\\int_\\mathcal{X}\\int_\\mathcal{Y}f(x,y)dydx = \\int_\\mathcal{Y}\\int_\\mathcal{X}f(x,y)dxdy\\) if \\(\\int_\\mathcal{X}\\int_\\mathcal{Y}|f(x,y)|dydx &lt; \\infty\\) that is, if the integral of its absolute value is finite. Theorem 2.4 (Fubini's Theorem: Infinite Series) \\(\\sum_{m=1}^\\infty\\sum_{n=1}^\\infty a_{m,n} = \\sum_{n=1}^\\infty\\sum_{m=1}^\\infty a_{m,n}\\) if \\(\\sum_{m=1}^\\infty\\sum_{n=1}^\\infty |a_{m,n}| &lt; \\infty\\) that is, if the original series is absolutely convergent 2.10 Gamma Function Defined as \\[\\Gamma(z) = \\int_{0}^\\infty t^{z-1}e^{-t}dt\\] the Gamma function commonly appears in the probability density function of several random variables, including the Gamma (duh!) and the Beta. The most important property of \\(\\Gamma(z)\\) is that \\[\\Gamma(z + 1) = z\\Gamma(z)\\] This is because we can substitute \\(\\Gamma(z) = \\frac{\\Gamma(z + 1)}{z}\\) to perform the Kernel Technique on distributions involving the Gamma function in their pdf As a corollary, when \\(n\\) is an integer, \\(\\Gamma(n) = (n-1)!\\). 2.11 Triangle Inequality The triangle inequality is defined as \\[|x + y| \\leq |x| + |y|\\] with \\(|x + y| \\leq |x| + |y|\\) unless \\(x, y \\geq 0\\). This inequality is useful for proving moment bounds as well as asymptotic convergence in Chapter 9 and Chapter 11. A general trick for proving the triangular inequality that may be useful in other problems involving absolute value can be termed the square trick - noting \\(x^2 = |x|^2\\), we can first show the equality of a square, then take the square root to obtain \\(|x|\\). Here is a proof of the triangle inequality using this trick: Note \\((a + b)^2 = |a + b|^2 \\leq 0\\) Rearrange \\((a + b)^2\\) and \\[ (a + b)^2 = a^2 + 2ab + b^2 = |a|^2 + 2ab + |b|^2\\\\ \\leq |a|^2 + 2|a||b| + |b|^2 = (|a| + |b|)^2\\\\ \\implies |a + b|^2 \\leq (|a| + |b|)^2\\\\ \\] 3. Take the square root of the final implication to get \\(|a + b| \\leq |a| + |b|\\) References "],["probability.html", "Chapter 3 Probability 3.1 Basic Axioms 3.2 Basic Probability Solving Techniques 3.3 Conditional Probability 3.4 Independence 3.5 Order Statistics 3.6 Convergence", " Chapter 3 Probability 3.1 Basic Axioms How do we reason about events that may or may not occur? The field of probability provides the mathematical foundations for reasoning under uncertainty. The foundation of probability is set theory. When you think about something that could happen, you can conceptualize that something as an experiment. We can define all of the details of that experiment in terms of sets: Any given outcome of the experiment we represent as a set - call it \\(A\\). We represent the set of all possible that could have occurred - the sampling space - as \\(\\Omega\\). Hence, \\(A \\subseteq \\Omega\\) Now suppose we could repeat this experiment infinitely. The probability of an event A is defined as the proportion of experiments in which that event will occurs as the number of experiments increases towards infinity. Mathematically, we represent probability as a function: \\(P(A)\\). Then, \\[P(A) = \\lim_{n\\rightarrow\\infty}\\frac{\\text{# of times A is drawn}}{n}\\] In the simplest case, we can compute this proportion directly as \\(P(A) = \\frac{\\text{# of outcomes in }A}{\\text{# of outcomes in }\\Omega}\\) - a typical grade-school exercise. Furthermore, from this definition, three properties (called Kolmogorovs axioms) arise: Definition 3.1 (Kolmogorov's Axioms of Probability) For all probability functions \\(P(\\cdot)\\) defined on a sampling space \\(\\Omega\\): \\(0 \\leq P(A) \\leq 1\\) for all \\(A\\subseteq \\Omega\\) \\(P(\\Omega) = 1\\). If $A_1, A_2, , $ are pairwise disjoint, then \\[P(\\cup_{i=1}^\\infty A_i) = \\sum_{i=1}^\\infty P(A_i)\\] Two sets \\(A\\) and \\(B\\) are pairwise disjoint if \\(A\\) does not share any elements in common with \\(B\\). This means \\(A \\cup B = \\emptyset\\). Consequently, to find a probability involving multiple events, you can use any knowledge of set theory you might have in conjunction with these axioms. Heres one example: ::: {.example name=Probability of a Set Complement} \\(P(A^c) = 1 - P(A)\\). Why? - Since \\(A^c\\) and \\(A\\) are disjoint, and naturally \\(A^c \\cup A = \\Omega\\) we first note, by axiom 3, that \\(P(\\Omega) = P(A^c) + P(A)\\). - Then, by axiom 2, \\(P(\\Omega) = 1\\), so \\(P(A^c) + P(A) = 1\\). Rearrange to complete the proof. ::: These axioms are the foundation of all probability. Learn to recognize them when working directly with probability functions \\(P(\\cdot)\\). For instance, any time you see a sum, you should think Aha! Kolmogorov Axiom 3! 3.2 Basic Probability Solving Techniques 3.2.1 Disjointification Kolmogorovs Axiom 3 is by far the most useful, but to use it requires a trick called disjointification, or partitioning into disjoint subsets. In this technique, we rewrite a set into a union of disjoint subsets: \\[A = (A \\cap B) \\cup (A \\cap B^c)\\] Then, if we know the probability of the intersections, we apply Axiom 3 to solve: \\(P(A) = P(A \\cap B) + P(A \\cap B^c)\\). Lets prove three classic equalities by disjointification: Example 3.1 (Probability of Set Intersection with Complement) To compute the probability of set intersection, use the following: \\[P(B \\cap A^c) = P(B) - P(A \\cap B)\\] Proof: Disjointify: Note \\(B = (B \\cap A) \\cup (B \\cap A^c)\\). Therefore, \\(P(B) = P(B \\cap A^c) + P(A \\cap B)\\) by Probability Axiom 3 Rearrange: \\(P(B \\cap A^c) = P(B) - P(A \\cap B)\\) Example 3.2 (Subset Inequality) Probability inequalities can be constructed using subsets: \\[A \\subset B \\implies P(A) \\leq P(B)\\] Proof: Disjointify: If \\(A \\subset B\\), then \\(B = A \\cup (A^c \\cap B)\\), with \\(A\\) and \\((A^c \\cap B)\\) disjoint (since \\((A^c \\cap B)\\) is the part of \\(B\\) not contained by \\(A\\)). Consequently, \\(P(B) = P(A) + P(A^c \\cap B)\\). By Kolmogorov Axiom 1, \\(P(A^c \\cap B) \\geq 0\\) which implies that \\(P(A) \\leq P(B)\\), completing the proof. Of course, these examples are famous properties that you can simply memorize and apply directly on an example. But, in case you forget, now you know how to derive it! 3.2.2 DeMorgans Laws If you ever need to turn a union into an intersection or vice versa, first take the complement \\(P(A) = 1 - P(A^c)\\). Then, apply DeMorgans Laws from set theory: \\((A \\cup B)^c = A^c \\cap B^c\\) \\((A \\cap B)^c = A^c \\cup B^c\\) Heres an example: Example 3.3 (Probability of a Set Union) \\[P(A \\cup B) = P(A) + P(B) - P(A \\cap B)\\] Start by taking the complement: \\(P(A \\cup B) = 1 - P((A\\cup B)^c\\) Apply DeMorgans Laws: \\(1 - P((A\\cup B)^c = 1 - P(A^c \\cap B^c)\\). Now, we could disjointify \\(A^c\\), but a faster way is to recall the previous set intersection example: \\(1 - P(A^c \\cap B^c) = 1 - (P(A^c) - P(A^c \\cap B))\\) Revert the complement: \\(1 - (P(A^c) - P(A^c \\cap B)) = P(A) + P(A^c \\cap B)\\) Repeat Step 3 on the other set intersection: \\(P(A) + P(A^c \\cap B) = P(A) + P(B) - P(A \\cap B)\\), completing the proof. 3.2.3 Proving Inequalities: Subsetting The Subset Inequality proven above - \\(A \\subset B \\implies P(A) \\leq P(B)\\) can be applied to prove other, more famous inequalities in probability as well. Well look at two. Theorem 3.1 (Boole's Inequality) Booles Inequality is a general inequality for unions of events. For a set of events \\(A_1,...,A_n\\), \\[P(\\cup_{i=1}^n A_i) \\leq \\sum_{i=1}^n P(A_i)\\] Proof: We want to use the subset inequality, so we need to construct a set of subsets \\(\\{B_i\\}\\). This subset must have the properties: \\(\\cup_{i=1}^n A_i = \\cup_{i=1}^n B_i\\), so we can replace this on the LHS of Booles inequality above \\(B_i \\subset A_i \\subset B_i\\), so we can use the subset inequality. \\(\\{B_i\\}\\) is disjoint, so we can construct the sum on the RHS using Kolmogorovs Axiom 3. We can accomplish this by disjointifying. Let \\[\\begin{align} B_1 = A_1 \\\\ B_2 = A_2 \\cap A_1^c \\\\ B_3 = A_3 \\cap A_2^c \\cap A_1^c \\\\ ... \\\\ B_n = A_n \\cap A_{n-1}^c \\cap ... \\cap A_1^c \\end{align}\\] Since \\(\\{B_i\\}\\) are all disjoint, by Kolmogorov Axiom 3, \\(P(\\cup_{i=1}^n A_i) = \\sum_{i=1}^nP(B_i)\\). Finally, use the subset inequality to note that \\(B_i \\subseteq A_i \\implies P(B_i) \\leq P(A_i)\\) and therefore \\(P(\\cup_{i=1}^n A_i) \\leq \\sum_{i=1}^n P(B_i) = \\sum_{i=1}^n P(A_i)\\) Theorem 3.2 (Bonferroni's Inequality) Bonferronis Inequality is the set intersection counterpart of Booles Inequality. In general, it states \\[P(\\cap_{i=1}^n A_i) \\geq \\sum_{i=1}^n P(A_i) - (n-1)\\] Proof: 1. We need the LHS. Start by taking the complement \\(P(\\cap_{i=1}^n A_i) = 1 - P((\\cap_{i=1}^n)^c)\\). 2. Apply DeMorgans Laws: \\(1 - P((\\cap_{i=1}^n)^c) = 1 - P(\\cup_{i=1}^nA_i^c)\\). 3. This is Booles Inequality which we already proved. \\(1 - P(\\cup_{i=1}^nA_i^c) \\geq 1 - \\sum_{i=1}^n P(A_i^c)\\) 4. Undo the complement: \\[ 1 - \\sum_{i=1}^n P(A_i^c) = 1 - \\sum_{i=1}^n (1 - P(A_i)) \\\\ = \\sum_{i=1}^n P(A_i) - (n-1) \\] Next, well discuss conditional probability, where computing set intersections and their bounds is critical for problem-solving. 3.3 Conditional Probability Sometimes in a problem, we are given additional information about an event. For example, we might wish to compute the probability of \\(A\\) given that we know another event \\(B\\) has already occurred. In this case, we must compute conditional probabilities. Definition 3.2 (Conditional Probability) The probability of event \\(A\\) occurring given that we know event \\(B\\) has occurred is given by \\[P(A|B) = \\frac{P(A\\cap B)}{P(B)}\\] When solving problems involving conditional probabilities, we often convert them via the above definition, and then manipulate the intersection \\(A\\cap B\\) using set theory to rewrite into the given form. Definition 3.3 (Bayes' Theorem) If we need to invert the order of \\(A\\) and \\(B\\) in a conditional probability express, we can use the property \\[P(A|B) = \\frac{P(B|A)P(A)}{P(B)}\\] More generally, let \\(A_1, A_2, ...\\) be a partition of the sample space and let \\(B\\) be any set \\[P(A_i|B) = \\frac{P(B|A_i)P(A_i)}{\\sum_{j=1}^{\\infty} P(B|A_j)P(A_j)}\\] Bayes Theorem often arises in word problems involving given probabilities Definition 3.4 (Law of Total Probability) Let \\(A_1, A_2, ..., A_n\\) be a partition of the sample space and let \\(B\\) be any set. \\[P(B) = \\sum_{i=1}^{n} P(B \\cap A_i) = \\sum_{i=1}^{n} P(B|A_i) P(A_i)\\] 3.3.1 Conditional Probability in Practice Public health research often relies on the task of comparison. There are several standard measures of association that are widely used in public health research. Here we present two of them. Definition 3.5 (Relative Risk, RR) Let \\(D\\) be an outcome event and \\(S\\) be an exposure event. The relative risk (or risk ratio), denoted by \\(RR\\) is given by \\[RR = \\frac{P(D|S)}{P(D|\\bar{S})}\\] where \\(\\bar{S}\\) is the complement of \\(S\\). Definition 3.6 (Odds Ratio, OR) Let \\(D\\) be an outcome event and \\(S\\) be an exposure event. The odds ratio, denoted by \\(OR\\), is given by \\[OR = \\frac{P(D|S)/P(\\bar{D}|S)}{P(D|\\bar{S})/P(\\bar{D}|\\bar{S})}\\] Note that \\(P(\\bar{D}|S) = 1 - P(D|S)\\). The odds ratio is invariant to switching \\(D\\) and \\(S\\), ie. \\[\\frac{P(D|S)/P(\\bar{D}|S)}{P(D|\\bar{S})/P(\\bar{D}|\\bar{S})} = \\frac{P(S|D)/P(\\bar{S}|D)}{P(S|\\bar{D})/P(\\bar{S}|\\bar{D})}.\\] Proof. \\[ \\begin{aligned} \\frac{P(D|S)/P(\\bar{D}|S)}{P(D|\\bar{S})/P(\\bar{D}|\\bar{S})} &amp;= P(D|S) \\cdot \\frac{1}{P(\\bar{D}|S)} \\cdot \\frac{1}{P(D|\\bar{S})} \\cdot P(\\bar{D}|\\bar{S})\\\\ &amp;= \\frac{P(S|D) P(D)}{P(S)} \\cdot \\frac{P(S)}{P(S|\\bar{D})P(\\bar{D})} \\cdot \\frac{P(\\bar{S})}{P(\\bar{S}|D)P(D)} \\cdot \\frac{P(\\bar{S}|\\bar{D})P(\\bar{D})}{P(\\bar{S})} \\\\ &amp;= \\frac{P(S|D)/P(\\bar{S}|D)}{P(S|\\bar{D})/P(\\bar{S}|\\bar{D})} \\end{aligned} \\] The second step plugs in the following equalities which follow from Bayes Rule \\[P(D|S) = \\frac{P(S|D) P(D)}{P(S)}, P(\\bar{D}|S) = \\frac{P(S|\\bar{D})P(\\bar{D})}{P(S)},\\] \\[P(D|\\bar{S}) = \\frac{P(\\bar{S}|D)P(D)}{P(\\bar{S})}, \\text{and } P(\\bar{D}|\\bar{S}) = \\frac{P(\\bar{S}|\\bar{D})P(\\bar{D})}{P(\\bar{S})}\\] ## Random Variables 3.3.2 Definition Statisticians study data, which typically comes not in the form of events, but rather, random variables. A random variable is a real-valued function which maps an event \\(A\\) to some numeric quantity - that is, for an event \\(A\\), the random variable \\(X(A) \\mapsto x \\in \\mathbb{R}\\). Even though \\(X\\) is a function, the \\(A\\) is usually omitted, and probability texts usually refer to it as one would a number (i.e. saying it takes on a particular value). Suppose \\(A\\) represents the event where a person contracts heart disease. Then, we can turn this into a number by defining a random variable: \\(X = \\begin{cases}1 \\text{ if }A\\text{ occurs}\\\\0 \\text{ if }A\\text{ does not occur}\\end{cases}\\). Since the output of a random variable is real, we can describe the probability of a given numerical output from a random variable in many ways. Here are the most common: 3.3.3 Functions Describing the Distribution Definition 3.7 (Probability Mass Function) If \\(X\\) is countable, then the probability mass function (pmf) is \\[f_X(x) = P(X = x)\\]. Properties arising from Kolmogorovs Axioms: - \\(0 \\leq f_X(x) \\leq 1\\) (by Axiom 1) - \\(\\sum_{x} f_X(x) = 1\\) (by Axiom 3) Definition 3.8 (Cumulative Distribution Function) The cumulative distribution function (cdf) of \\(X\\) is \\(F_X(x) = P(X \\leq x)\\). For discrete RVs with \\(x \\geq 0\\), \\(F_X(x) = \\sum_{k = 0}^xP(X = x)\\) For continuous RVs, \\(F_X(x) = \\int_{-\\infty}^xf(t)dt\\) A function \\(F(x)\\) is a cdf iff the following all hold: \\(\\lim_{x\\rightarrow -\\infty}F(x) = 0\\) and \\(\\lim_{x\\rightarrow \\infty}F(x) = 1\\). \\(F(x)\\) is nondecreasing. \\(F(x)\\) is right continuous: \\(\\lim_{x\\rightarrow c^+}F(x) = F(c)\\). We can prove a particular function is a cdf by proving each necessary property. Prove - (1) by taking the limit directly - (2) by taking the derivative of \\(F_X(x)\\) and showing it is positive - (3) by arguing that \\(F_X(x)\\) is continuous, showing it has no undefined points (continuity implies right-continuity). In general, we can show two random variables have the same distribution by showing they have the same cdf (example?) Definition 3.9 (Probability Density Function) The probability density function (pdf) is the continuous analogue of a pmf. One way to define the pdf of \\(X\\) is as the derivative of the cdf: \\[f_X(x) = \\frac{d}{dx}F_X(x)\\] Therefore, \\(F_X(x) = \\int_{-\\infty}^xf_X(t)dt\\) (It is not P(X = x); \\(X\\) is uncountable, so this is technically 0). It satisfies two properties from Kolmogorovs Axioms: - \\(0 \\leq f_X(x) \\leq 1\\) (by Axiom 1) - \\(\\sum_{x} f_X(x) = 1\\) (by Axiom 3) 3.3.4 Technique: Direct Probability Manipulation Functions of random variables are also random variables. Suppose \\(Y\\) and \\(X\\) are both random variables, and \\(Y = g(X)\\). We often want to find \\(P(Y\\in A)\\) where \\(A\\) is some set such as an interval or even a single value. If we know the distribution of \\(X\\), we can find the distribution of \\(Y\\) like so: \\[ P(Y \\in A) = P(g(X) \\in A) = P(X \\in g^{-1}(A)) \\] where \\(g^{-1}(A) = \\{x \\in \\mathcal{X}: g(X) \\in A\\}\\) (note \\(\\mathcal{X}\\) is the sample space of \\(X\\)) Since this is such a fundamental technique arising throughout statistics, in lieu of an example, we will point out where this technique is used in future chapters. We can, however, note that if \\(X\\) and \\(Y\\) are both discrete, then \\[f_Y(y) = P(Y = y) = \\sum_{x \\in g^{-1}(y)}P(X=x) = \\sum_{x \\in g^{-1}(y)}f_X(x)\\] meaning the pmf of \\(Y\\) is just the sum of the probabilities of \\(X\\) over all \\(y \\in \\mathcal{Y}\\) (where \\(\\mathcal{Y}\\) is the sample space of \\(Y\\)). For more information, see Chapter 2 of Casella and Berger (1990). 3.4 Independence Definition 3.10 (Independence) Two events or random variables are statistically independent if the occurrence of one has no impact on the other. Mathematically, \\[P(A | B) = P(A)\\] By Bayes Theorem, \\[P(A \\cap B) = P(A)P(B)\\] Often, we prefer to use the second definition because it is independent and easier to generalize to multiple events or random variables. For example, if we know that the random variables \\(X\\) and \\(Y\\) are independent, then \\[f_{X,Y}(x, y) = f_X(x)f_Y(y)\\] This property is the foundation of statistical inference for iid random variables discussed in Chapters 8 and 10. 3.5 Order Statistics Definition 3.11 (Order statistic) The \\(k\\)th order statistic of a random sample from a random variable \\(X\\) is the \\(k\\)th smallest value. We denote the \\(k\\)th order statistic as \\(X_{(k)}\\). Let \\(X_1, X_2, ..., X_n\\) be a random sample from \\(X \\sim f_X(x)\\) with cumulative distribution \\(F_X(x)\\). The order statistics are random variables themselves that satisfy \\(X_{(1)} \\leq X_{(2)}\\leq ... \\leq X_{(n)}\\). In particular, \\(X_{(1)} = min(X_1, X_2, ..., X_n)\\) and \\(X_{(n)} = max(X_1, X_2, ..., X_n)\\). The sample range, \\(R = X_{(n)} - X_{(1)}\\), is the distance between the smallest and the largest order statistics of the sample. The sample median is a number \\(M\\) such that approximately half of the observations in the sample are less than \\(M\\) and the other half are greater. \\[M = \\begin{cases} X_{((n+1)/2)}&amp; \\text{if } n \\text{ is odd} \\\\ (X_{(n/2)} + X_{(n/2 + 1)})/2 &amp; \\text{if } n \\text{ is even.} \\end{cases}\\] Theorem 3.3 Let \\(X_1, X_2, ..., X_n\\) be a random sample from a discrete distribution with pmf \\(f_X(x_i) = p_i\\), where \\(x_1&lt;x_2&lt; \\cdots\\) are the possible values of \\(X\\) in ascending order. Define \\(P_i = p_1 + p_2 + \\cdots p_i\\). Let \\(X_{(1)}, X_{(2)}, ... ,X_{(n)}\\) denote the order statistics from the sample. Then \\[P(X_{(j)}\\leq x_i) = \\sum_{k=j}^{n} {n \\choose k} P_i^k(1-P_i)^{n-k}\\] and \\[P(X_{(j)}= x_i) = \\sum_{k=j}^{n} {n \\choose k} [P_i^k(1-P_i)^{n-k} - P_{i-1}^k(1-P_{i-1})^{n-k}].\\] Theorem 3.4 Let \\(X_1, X_2, ..., X_n\\) be a random sample from a continuous distribution with pdf \\(f_X(x_i)\\) and cdf \\(F_X(x)\\). Then the pdf of \\(X_{(j)}\\) is \\[f_{X_{(j)}}(x) = \\frac{n!}{(j-1)!(n-j)!} f_X(x)[F_X(x)]^{j-1}[1-F_X(x)]^{n-j}.\\] Theorem 3.5 Let \\(X_1, X_2, ..., X_n\\) be a random sample from a continuous distribution with pdf \\(f_X(x_i)\\) and cdf \\(F_X(x)\\). Then the joint pdf of \\(X_{(i)}\\) and \\(X_{(j)}, 1\\leq i \\leq j \\leq n\\), is \\[f_{X_{(i)}, X_{(j)}} (u,v) = \\frac{n!}{(i-1)!(j-1-i)!(n-j)!}f_X(u)f_X(v)[F_X(u)]^{i-1} [F_X(v)-F_X(u)]^{j-1-i}[1-F_X(v)]^{n-j}.\\] The joint pdf of all order statistics is \\[f_{X_{(1)}, X_{(2)}, ..., X_{(n)}} (x_1,x_2, ..., x_n) = \\begin{cases} n!f_X(x_1)f_X(x_2) \\cdots f_X(x_n) &amp; -\\infty&lt; x_1&lt;x_2&lt;\\cdots&lt;x_n&lt;\\infty\\\\ 0 &amp; otherwise \\end{cases}\\] Example 3.4 (Joint Density of Order Statistic Differences) A useful trick for finding the joint of \\(D_i = X_{(i)} - X_{(i-1)}\\) and proving that they are independent for a given distribution is by rewriting the random variable as $$ X_{(1)} = D_1 + 0\\ X_{(2)} = D_2 + X_{(1)} = D_2 + D_1\\ X_{(3)} = D_3 + X_{(2)} = D3 + D2 + D1\\ $$ and so on. By induction, \\(X_{(i)} = \\sum_{j=1}^{i-1}X_{(j)}\\). Therefore, we can rewrite the joint density of all order statistics as \\(n!f(d_1)f(d_2 + d_1)\\cdot...\\cdot f(d_1 + ... + d_n)I(d_i \\geq 0, \\forall i)\\), which we can then compute and factorize to show independence. 3.6 Convergence What happens when, instead of a set of events or random variables, we observe a sequence of \\(n\\) random variables? There exist two major types of convergence of random variables with which we are typically concerned: convergence in probability and convergence in distribution 3.6.1 Convergence in Probability Definition 3.12 (Convergence in Probability) If \\(Z\\) is a random variable and \\(Z_n\\) is a sequence of random variables, then \\[Z_n \\overset{p}{\\rightarrow} Z \\iff\\lim_{n\\rightarrow\\infty}P(|Z_n - Z| &gt; \\epsilon) = 0\\] If \\(Z\\) is degenerate (meaning it equals a constant value, or \\(Z = c\\)), we can show convergence in probability by disjointifying the absolute value into two disjoint events, and using Kolmogorov Axiom 3. For example, $$ P(|Z_n - c| &gt; ) = P(( Z_n - c &gt; ) (Z_n - c &lt;-)) ( Z_n - c &gt; ), (Z_n - c &lt;-) \\ = P(Z_n &gt; c + ) + P(Z_n &lt; c - ) = 1 - P(Z_n &lt; c + ) + P(Z_n &lt; c - ) $$ From here, we can use the CDF of \\(Z_n\\) to compute the necessary probability. But be careful - this method may not work if the convergence of the CDF cannot be evaluated easily. Convergence in probability has several properties - if we know \\(Z_n\\) convergence in probability, we can use these for solving other problems, especially in Chapter 9 - Point Estimator Asymptotic If \\(A_n \\overset{p}{\\rightarrow} a\\) and \\(B_n\\overset{p}{\\rightarrow}b\\), then \\(A_n + B_n \\overset{p}{\\rightarrow} a + b\\) \\(A_n - B_n \\overset{p}{\\rightarrow} a - b\\) \\(A_n \\cdot B_n \\overset{p}{\\rightarrow} a \\cdot b\\) \\(A_n / B_n \\overset{p}{\\rightarrow} a / b\\) Convergence in probability can also be extended to the multivariate setting, where it takes on a slightly different meaning. Definition 3.13 (Multivariate Convergence in Probability) If \\(X\\) is random vector and \\(X_n\\) is a sequence of random vectors, then \\[\\begin{align} X_n \\overset{p}{\\rightarrow} X \\iff\\lim_{n\\rightarrow\\infty}P(||X_n - X|| &gt; \\epsilon) = 0 \\\\ \\iff X_{jn} \\overset{p}{\\rightarrow}X_j, \\forall j \\in 1,...,k \\end{align}\\] 3.6.2 Convergence in Distribution Definition 3.14 (Convergence in Probability) \\[Z_n \\overset{\\mathcal{D}}{\\rightarrow} Z \\iff\\lim_{n\\rightarrow\\infty}F_n(Z) = F(z), \\forall\\text{ continuity points of }F\\] Note that convergence in probability implies convergence in distribution, but not the converse. That is, \\[Z_n \\overset{p}{\\rightarrow} Z \\implies Z_n \\overset{\\mathcal{D}}{\\rightarrow} Z\\] Unless, that is, the random variable converges in distribution to a constant - then, convergence in distribution does imply convergence in probability! That is, \\[Z_n \\overset{\\mathcal{D}}{\\rightarrow} c \\implies Z_n \\overset{p}{\\rightarrow} c \\] :::{.definition name=Multivariate Convergence in Distribution} If \\(X\\) is random vector and \\(X_n\\) is a sequence of random vectors, then \\[X_n \\overset{\\mathcal{D}}{\\rightarrow} X \\iff \\lim_{n\\rightarrow\\infty}F_n(X_1, ..., X_k) = F(X_1,...,X_k), \\forall\\text{ continuity points of }F\\] How do we prove \\(X_n\\) converges in distribution to \\(X\\)? There are three chief strategies: Derive the limiting CDF of \\(X_n\\) and show its limit as \\(n\\rightarrow \\infty\\) equals the \\(F_X(x)\\). Derive the MGF or CF of \\(X_n\\) and show its limit as \\(n\\rightarrow \\infty\\) equals the \\(\\mathcal{M}_X(t)\\). Use the CLT with Slutskys Theorem or the CMT (will be discussed later in Chapter 9 - Point Estimator Asymptotics) 3.6.3 Important Theorems Theorem 3.6 (Slutsky's Theorem) If \\(Z_n \\overset{\\mathcal{D}}{\\rightarrow} Z\\) and \\(Y_n \\overset{p}{\\rightarrow} c\\), then \\(Z_n + Y_n \\overset{\\mathcal{D}}{\\rightarrow} Z + c\\) \\(Z_nY_n \\overset{\\mathcal{D}}{\\rightarrow} cZ\\) \\(\\frac{Z_n}{Y_n}\\overset{\\mathcal{D}}{\\rightarrow}\\frac{Z}{c}\\). For problem-solving, Slutskys theorem is generally applied whenever we deal with both convergence in probability and convergence in distribution together. Theorem 3.7 (Continuous Mapping Theorem) Suppose \\(Y_n\\) is a sequence of random variables (possibly vectors), \\(Y\\) is a random variable (or vector the same length as \\(Y_n\\)), \\(c\\) is a constant, and \\(g\\) is a function. Then, If \\(Y_n \\overset{p}{\\rightarrow} c\\), and \\(g\\) is continuous at \\(c\\), then \\(g(Y_n) \\overset{p}{\\rightarrow} g(c)\\) If \\(Y_n \\overset{\\mathcal{D}}{\\rightarrow} Y\\), and \\(g\\) is continuous (with \\(g: \\mathbb{R}^k \\mapsto \\mathbb{R}^m\\) for vectors), then \\(g(Y_n) \\overset{\\mathcal{D}}{\\rightarrow} g(Y)\\) (in \\(\\mathbb{R}^m\\) for vectors) References "],["known-distributions.html", "Chapter 4 Known Distributions 4.1 Families of Distributions 4.2 Location and Scale Families 4.3 Exponential Families 4.4 Known Univariate Exponential Families 4.5 Non-exponential families 4.6 Multivariate Distributions 4.7 Medians and Other Functionals of a Distribution", " Chapter 4 Known Distributions This will cover proofs and useful properties of commonly used distributions, as well as location-scale and exponential families. 4.1 Families of Distributions Many so-called distributions are actually families of distributions, meaning that their pdf involves one or more parameters. That is, their pdfs represent a family of curves, a set of pdfs with variable parameters. For example, the \\(\\text{Normal}(\\mu, \\sigma^2)\\) distribution contains two parameters, \\(\\mu\\) (the mean), and \\(\\sigma^2\\) (the variance). These are also examples of two types of parameters with special properties - called location and scale parameters, respectively - that can be used to simply calculations. 4.2 Location and Scale Families 4.2.1 Location Families Definition 4.1 (Location Family) Let \\(Z \\sim f_Z(z)\\). Given a constant location parameter \\(b\\), \\(X\\) is a location family if \\(X \\sim f_Z(z - b)\\) or if \\(X = Z + b\\). The two above definitions are equivalent because if \\(X = Z + b\\), then \\(P(X &lt; z) = P(Z + b &lt; z) = P(Z &lt; z - b)\\), so the cdf of \\(Z\\) is \\(F_Z(z - b)\\) and therefore \\(X \\sim f_Z(z - b)\\) (note this makes use of a direct probability argument) 4.2.2 Scale Families Definition 4.2 (Scale Family) Let \\(Z \\sim f_Z(Z)\\). Given a constant scale parameter \\(a\\), \\(X\\) is a scale family if \\(X \\sim \\frac{1}{a}f_Z(\\frac{z}{a})\\) \\(X \\sim F_Z(\\frac{z}{a})\\) or \\(X = aZ\\) All of the above definitions are equivalent because if \\(X = aZ\\), then \\(P(X &lt; z)\\) = \\(P(aZ &lt; z) = P(Z &lt; \\frac{z}{a} = F_Z(z)\\). Also, \\(f_X(x) = \\frac{d}{dx}F_X(x) = \\frac{d}{dx}F_Z(\\frac{z}{a}) = \\frac{1}{a}f_Z(\\frac{z}{a})\\) 4.2.3 Properties of Location-Scale Families We can compute moments by using general properties of expectation (see Moments) \\(E(X) = aE(Z) + b\\), by linearity of expectation. If the support of \\(Z\\) includes \\(0\\), then we typically define \\(Z\\) such that \\(E(Z) = 0\\) so that \\(E(X) = b\\). \\(Var(X) = a^2Var(Z)\\), since \\(Var(Z + b) = Var(Z)\\). We typically define \\(Z\\) such that \\(Var(Z) = 1\\), so that \\(Var(X) = a^2Var(Z) = a^2\\). An example of this is the standard normal. \\(\\mathcal{M}_X(t) = e^{tb} \\mathcal{M}_Z(at)\\) It may seem like the sum of a scale family should also follow the same family - indeed, this is true for a number of distributions include the Normal, Poisson, and Gamma. However, it is not true always. For instance, \\(X_i \\sim \\text{Uniform}(0,a)\\) is a scale family, but \\(X_1 + X_2\\) does not follow a uniform distribution: X1 = runif(1000, 0, 1) X2 = runif(1000, 0, 1) hist(X1 + X2) 4.3 Exponential Families Definition 4.3 (Exponential Family) \\(X \\sim f_X(x|\\theta)\\) is an exponential family if its pdf can be written in the form \\[f_X(x|\\theta) = h(x)c(\\theta)\\exp\\Big(\\sum_{i=1}^k w_i(\\theta)t_i(x)\\Big)\\] How do we prove that a pdf can be written in the above form? Often, the easiest way is to use a simple trick to get the necessary \\(\\exp\\) function: \\(f(x) = \\exp(\\log(f(x)))\\). Then, we algebraically manipulate to obtain this form. Heres an example for a distribution which will be presented imminently. Example 4.1 (Simple Technique: Proving Exponential Families) From Casella and Berger (1990), \\(X \\sim \\text{Binomial}(n, p)\\) is an exponential family given fixed \\(n\\) and \\(p \\in (0,1)\\). Proof: By the properties of the logarithm, \\[ f_X(x|p) = {n\\choose x}p^x(1-p)^{n-x} = {n\\choose x}\\exp(x\\log(p) + (n-x)\\log(1-p))\\\\ = {n\\choose x}\\cdot\\exp(n\\log(1-p))\\cdot\\exp(x\\log(\\frac{p}{1-p})) \\] Letting \\(h(x) = {n\\choose x}\\), \\(c(\\theta) = \\exp(n\\log(1-p))\\), \\(w(\\theta) = \\log(\\frac{p}{1-p})\\) and \\(t(x) = x\\), the Binomial is an exponential family. We require \\(p \\in (0,1)\\) to guarantee \\(w(\\theta)\\) is not undefined. How do we prove that a pdf cannot be written in the above form? Generally, we want to prove that the pdf cannot be factorized into separate functions of \\(x\\) and \\(\\theta\\). One example of this is when the support depends on the parameter. Consider \\(f_X(x|\\theta) = \\frac{1}{\\theta}\\cdot I(x &lt; \\theta)\\), the pdf of a \\(\\text{Uniform}(0, \\theta)\\). Here the indicator function \\(I(x &lt; \\theta)\\) cannot be decomposed in any way. Since it is mathematically impossible to factorize \\(I(x &lt; \\theta)\\) into \\(x\\) and \\(\\theta\\), we know that any function containing an indicator involving \\(X\\) and \\(\\theta\\) cannot be an exponential family. Also note that, by this definition of the exponential family, in order to guarantee that \\(f_x(x|\\theta)\\) integrates to 1, it must be true that \\[\\frac{1}{c(\\theta)} = \\int_{-\\infty}^{\\infty}h(x)\\exp\\Big(\\sum_{i=1}^k w_i(\\theta)t_i(x)\\Big)\\] a fact which can be used to prove certain properties of exponential families. For example, FORTHCOMING 4.3.1 Properties Exponential families have a number of incredibly useful properties: Leibnizs rule holds, meaning that the Cramer-Rao Lower Bound provides a lower bound on the variance of estimators. Among most common families, only exponential families admit sufficient statistics with dimension bounded in \\(n\\). This is proven by the Pitman-Koopman-Darmois theorem for families with smooth, nowhere-vanishing pdfs whose domain does not depend on the parameter being estimated. If \\(X\\) is an exponential family, \\[T(X) = \\Big(\\sum_{i=1}^n t_1(x), ..., \\sum_{i=1}^n t_k(x)\\Big)\\] is a minimal sufficient statistic. Furthermore, if \\(\\{w_1(\\theta),...,w_k(\\theta)\\}\\) contains an open set, then \\(T(X)\\) is a complete sufficient statistic, which we can use to compute an UMVUE. The Method of Moments (MOM) estimator is equal to the Maximum Likelihood Estimator (MLE) The regularity conditions required for consistency and asymptotic normality of the MLE are guaranteed to hold. The family must have a Monotone Likelihood Ratio, meaning that the Karlin-Rubin Theorem may be employed to construct an UMP test. 4.3.2 Natural Exponential Families Definition 4.4 (Natural Exponential Family) \\(X \\sim f_X(x|\\theta)\\) is a natural exponential family if its pdf can be written in the form \\[f_X(x|\\theta) = h(x)c^*(\\boldsymbol{\\eta})\\exp\\Big(\\sum_{i=1}^k \\eta_i t_i(x)\\Big)\\] This is also sometimes called the canonical parametrization. 4.4 Known Univariate Exponential Families Many common distributions follow exponential families. As you will come to find, virtually all of them arise in order to model variations on a common idea: the Bernoulli trial. Lets discuss this distribution, and the situations in which it arises. 4.4.1 Bernoulli The Bernoulli distribution, represented mathematically as \\(\\text{Bernoulli}(p)\\), describes the outcome of a random variable \\(X\\) that takes only two possible values, 0 and 1. Such an event is often termed a Bernoulli trial. Description Parameters Support pmf Any random variables whose value can be either 0 or 1 \\(0 \\leq p \\leq 1\\) \\(x \\in \\{0, 1\\}\\) \\(p^x(1-p)^{1-x}\\) The Bernoulli distribution occurs very commonly because many situations can be described in terms of 0 or 1 outcomes. For example, all indicator functions \\(I(A)\\) of random variables, where \\(A\\) is a statement about the random variable (for instance, \\(A = \\{x: x &gt; 1\\}\\)) are Bernoulli random variables with \\(p = P(A)\\). The Bernoulli is a special case of the Binomial distribution: \\(\\text{Bernoulli}(p) = \\text{Binomial}(1, p)\\) As we discuss regarding the Binomial distribution, the Bernoulli has an additive property: \\(\\sum_{i=1}^n \\sim \\text{Binomial}(n, p)\\). This can be proven by the additivity technique discussed imminently. If \\(X\\sim \\text{Bernoulli}\\) then \\(E(X) = P(Y = 1)\\), a fact which is often useful for computing moments as well as finding UMVUEs. The above also holds true for multiple Bernoulli random variables. For example, if \\(Z_1, Z_2 \\sim \\text{Bernoulli}\\), then \\(E(Z_1Z_2) = P(Z_1 = 1, Z_2 = 1)\\). 4.4.2 Binomial What happens when we repeat a Bernoulli trial many times and count how many 1s occur? The Binomial distribution is represented mathematically as \\(\\text{Bernoulli}(n, p)\\). It describes the number of successes in a series of Bernoulli trials. Description Parameters Support pmf The number of times an event was successful out of \\(n\\) attempts \\(0 \\leq p \\leq 1\\), \\(n \\in \\mathbb{N}\\) \\(x \\in \\mathbb{N}\\) \\({n\\choose x}p^x(1-p)^{n-x}\\) Example 4.2 (Proving Additive Properties of Distributions) The Binomial distribution is additive: if \\(X \\sim \\text{Binomial}(n,p)\\) and \\(X \\sim \\text{Binomial}(m,p)\\), then \\(X + Y \\sim \\text{Binomial}(m + n, p)\\). One can prove the additivity of any distribution, not just the Binomial, by relying on the convolution property of mgfs: \\(\\mathcal{M}_{X + Y}(t) = \\mathcal{M}_X(t)\\cdot\\mathcal{M}_Y(t)\\). Heres an example with the Binomial: if \\(X \\sim \\text{Binomial}(n, p)\\) and \\(Y \\sim \\text{Binomial}(m, p)\\), then \\[\\mathcal{M}_{X + Y}(t) = ((1-p) + pe^t)^n\\cdot ((1-p) + pe^t)^m = ((1-p) + pe^t)^{mn}\\] which we can recognize as the mgf of a \\(\\text{Binomial}(n + m, p)\\) distribution. Since, like the cdf and the pdf, the mgf fully characterizes a probability distribution, weve proven the additive property mentioned above. Heres another example with the Binomial, this time generalizing it to an arbitrary summation: If \\(X_i \\overset{iid}{\\sim} \\text{Binomial}(m, p)\\), and \\(Y = \\sum_{i=1}^nX_i\\), then \\[ \\mathcal{M}_Y(t) = \\prod_{i=1}^n\\mathcal{M}_{X_i}(t) = (\\mathcal{M}_{X_i}(t))^n \\\\ = (((1-p) + pe^t)^m)^n = ((1-p) + pe^t)^{mn} \\] proving the generalized additive property that if \\(X_i \\sim \\text{Binomial}(m, p)\\), then \\(\\sum_{i=1}^n X_i \\sim \\text{Binomial}(nm, p)\\). 4.4.3 Geometric Suppose instead of counting the number of successes, we wish to count the number of attempts until a single success occurs? The Geometric distribution is represented mathematically as \\(\\text{Geo}(p)\\). It describes the number of trials before a success occurs in a series of Bernoulli trials. Note that the parametrization below does not include the final success in the number of trials, but alternatives exist in which it may. Description Parameters Support pmf The number of Bernoulli trials attempted before a success occurs \\(0 &lt; p \\leq 1\\) \\(x \\in \\mathbb{N}\\) \\(p(1-p)^{x}\\) The Geometric is a special case of the Negative Binomial: \\(\\text{Geo}(p) = \\text{NegBin}(1, p)\\) Just like the Bernoulli, the Geometric has an additive property: \\(\\sum_{i=1}^n X_i \\sim \\text{NegBin}(n, p)\\), proven via the same addivity technique discussed previously. The Geometric is the only discrete memoryless distribution; that is, for \\(k &gt; i\\), \\(P(X \\geq k | X &gt; i) = P(X \\geq k - i)\\). 4.4.4 Negative Binomial The Negative Binomial distribution generalizes the Geometric distribution to instead represent the number of Bernoulli trials until \\(r\\) successes have occurred. It is represented mathematically as \\(\\text{NegBin}(r, p)\\). Description Parameters Support pmf The number of Bernoulli trials attempted before a success occurs \\(0 &lt; p \\leq 1\\), \\(r \\in \\mathbb{N}\\) \\(x \\in \\mathbb{N}\\) \\({x+r-1\\choose x}p^r(1-p)^{x}\\) The Negative Binomial is additive: If \\(X_i \\sim \\text{NegBin(r, p)}\\), then \\(\\sum_{i=1}^n X_i \\sim \\text{NegBin(nr, p)}\\) 4.4.5 Poisson The Poisson distribution describes one possible behavior of a count random variable. It describes the probability that a certain number of events occur within a fixed interval, such as a time period, distance or area. It is mathematically represented as \\(\\text{Poisson}(\\lambda)\\). Description Parameters Support pmf The number of events occurring in a fixed interval \\(\\lambda \\in (0, \\infty)\\) \\(x \\in \\mathbb{N}_0\\) \\(\\frac{1}{x!}\\lambda^xe^{-\\lambda}\\) Like the Binomial, the Poisson is formulated by counting the number of successes within a set of Bernoulli trials. The distribution describes the asymptotic behavior of the Binomial distribution as \\(n \\rightarrow \\infty\\) and \\(np \\rightarrow \\lambda\\), a fixed rate parameter. The Poisson is additive. If \\(X_i \\sim \\text{Poisson}(\\lambda)\\), then \\(\\sum_{i=1}^n X_i \\sim \\text{Poisson}(n\\lambda)\\) 4.4.6 Normal Often denoted \\(N(\\mu, \\sigma^2)\\), the Normal distribution is especially common in asymptotics - by the Central Limit Theorem, the sample mean converges in distribution to a normal. Many other variables also converge to a normal. For \\(X \\sim N(\\mu, \\sigma^2)\\), the mean is \\(\\mu\\) and the variance is \\(\\sigma^2\\). This means that the Normal is a location-scale family. Description Parameters Support pdf Describes the asymptotic behavior of sample means and many distributions \\(\\mu \\in \\mathbb{R}\\), \\(\\sigma^2 \\in (0, \\infty)\\) \\(x \\in \\mathbb{R}\\) \\(\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\Big(-\\frac{1}{2}\\frac{(x-\\mu)^2}{\\sigma^2}\\Big)\\) The following distributions converge to a Normal: \\(\\text{Binomial}(n, p) \\approx N(np, np(1-p))\\) for large \\(n\\) and \\(p\\) bounded away from 0 or 1 \\(\\text{Pois(\\lambda)} \\approx N(\\lambda, \\lambda)\\) for large \\(\\lambda\\). \\(\\chi^2(\\nu)\\approx N(\\nu, 2\\nu)\\) for large \\(\\nu\\). \\(t(\\nu) \\approx N(0, 1)\\) for large \\(\\nu\\). The Normal is additive. If \\(X_1 \\sim N(\\mu_1, \\sigma_1^2)\\) and \\(X_2 \\sim N(\\mu_2, \\sigma_2^2)\\), and \\(X_1, X_2\\) are iid, then \\(X + Y \\sim N(\\mu_1 + \\mu_2, \\sigma_1^2 + \\sigma_2^2)\\). If \\(X_1\\) and \\(X_2\\) are normally distributed as above, but also correlated with \\(Corr(X_1, X_2) = \\rho\\), then \\(X + Y \\sim N(\\mu_1 + \\mu_2, \\sigma_1^2 + \\sigma_2^2 + 2\\rho\\sigma_x\\sigma_y)\\) 4.4.7 Exponential The Poisson process that models the number of events occuring in an interval also gives rise to another distribution: the Exponential. This distribution models the size of the interval (time, distance, etc.) between events. The Exponential is described mathematically as \\(\\text{Exp}(\\lambda)\\). It can be parametrized in two ways: with \\(\\lambda\\) as a rate parameter, describing how often events occur; or with \\(\\lambda\\) as a scale parameter (yes, the same scale parameter discussed in Location Scale Families) that is the inverse of the rate. Hence, the Exponential is a scale family. Note that the parametrization below describes the distribution in terms of the scale parameter. The scale parameter version simply replaces \\(\\lambda\\) with \\(\\frac{1}{\\lambda}\\). Description Parameters Support pdf Models the size of the interval between events in a Poisson process \\(\\lambda \\in (0, \\infty)\\) \\(x \\in (0, \\infty)\\) \\(\\lambda e^{-\\lambda x}\\) The Exponential is a special case of the Gamma distribution: \\(\\text{Exp}(\\lambda) = \\text{Gamma}(1, \\lambda)\\) By extension, the exponential has an additive property: If $X_iExp() $, then \\(\\sum_{i=1}^n X_i \\sim \\text{Gamma}(n, \\lambda)\\). The Exponential is the only continuous memoryless distribution; that is, for \\(k &gt; i\\), \\(P(X \\geq k | X &gt; i) = P(X \\geq k - i)\\). We can prove the memorylessness of the Exponential using conditional probability: \\[ P(X &gt; s | X &gt; t) = \\frac{P(X &gt; s, X &gt; t)}{P(X &gt; t)} = \\frac{P(X &gt; s)}{P(X &gt; t)} \\] since \\(s &gt; t\\). This equals \\(\\frac{e^{-\\lambda s + 1}}{e^{-\\lambda t + 1}} = e^{\\lambda(s - t)} = P(X &gt; s - t)\\) 4.4.8 Gamma Similar to how the Negative Binomial generalizes the Geometric to multiple successes, the Gamma generalizes the Exponential to multiple events. The Gamma is useful for modeling random variables that are known to be greater than 0. It is represented mathematically as \\(\\text{Gamma}(k, \\lambda)\\), where \\(k\\) is a shape parameter and \\(\\lambda\\) is a scale parameter. This means the Gamam is a scale family. It is called Gamma because it involves the gamma function. Description Parameters Support pdf Generalization of the exponential distribution \\(k \\in (0, \\infty)\\), \\(\\lambda \\in (0, \\infty)\\) \\(x \\in (0, \\infty)\\) \\(\\frac{1}{\\Gamma(k)\\lambda^k}x^{k-1}\\exp(-\\frac{x}{\\theta})\\) The Gamma is additive: If $X_i(k, ) $, then \\(\\sum_{i=1}^n X_i \\sim \\text{Gamma}(nk, \\lambda)\\). 4.4.9 Beta The Beta distribution models proportions. It is mathematically denoted \\(\\text{Beta}(\\alpha, \\beta)\\) where \\(\\alpha\\) and \\(\\beta\\) are two shape parameters. It is known as Beta because its pdf contains the beta function: \\(B(\\alpha, \\beta) = \\frac{\\Gamma(\\alpha)\\Gamma(\\beta)}{\\Gamma(\\alpha + \\beta)}\\) Description Parameters Support pdf Model of a proportion \\(\\lambda \\in (0, \\infty)\\) \\(x \\in (0, 1)\\) \\(\\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)}x^{\\alpha-1}(1-x)^{\\beta-1}\\) 4.4.10 Chi-squared The Chi-squared distribution describes the distribution of the sum of squared standard Normal (\\(N(0,1)\\)) random variables. As a result, it is useful in asymptotics, especially asymptotic hypothesis testing, since if an estimator is asymptotically normal, its square is asymptotically \\(\\chi^2\\). It is represented mathematically as \\(\\chi^2(\\nu)\\) or \\(\\chi^2_\\nu\\) where \\(\\nu\\) is the degrees of freedom - the number of squared normal random variables in the sum. Description Parameters Support pdf Squared standard normals \\(\\nu \\in \\mathbb{N}\\) \\(x \\in (0, \\infty)\\) \\(\\frac{1}{\\Gamma(\\nu/2)2^{\\nu / 2}}x^{\\nu/2 - 1}\\exp(-\\nu/2)\\) If \\(X_i \\sim N(0,1)\\), then \\(\\sum_{i=1}^nX_i^2\\sim \\chi^2(n)\\) The Chi-squared distribution is a special case of the Gamma. That is, if \\(X \\sim \\chi^2(\\nu)\\), then \\(X \\sim \\text{Gamma}(\\frac{\\nu}{2}, \\frac{1}{2})\\). This can be observed directly from the pdf. 4.4.11 Weibull The Weibull is a generalization of the exponential distribution. The extra parameter \\(k\\) describes how the failure rate changes over time. It is an exponential family when \\(k\\) is fixed. Like the exponential, \\(\\lambda\\) is a scale parameter, meaning that the Weibull is a scale family. Description Parameters Support pdf Models time-to-event variables \\(\\lambda \\in (0, \\infty)\\), \\(k \\in (0, \\infty)\\) \\(x \\in [0, \\infty)\\) \\(\\frac{k}{\\lambda}\\Big(\\frac{x}{\\lambda}\\Big)^{k-1}\\exp(-(x/\\lambda)^k)\\) When \\(k = 1\\), the Weibull is equal to an \\(\\text{Exponential}(\\frac{1}{\\lambda})\\), which can be observed directly from its pdf. 4.4.12 Pareto The Pareto distribution, written \\(\\text{Pareto}(x_m, \\alpha)\\), models variables involving power-law relationships. \\(\\alpha \\in (0, \\infty)\\) is a shape parameter, while \\(x_m\\) is a scale parameter. This means that the Pareto is a scale family. It is an exponential family when \\(x_m\\) is fixed. Description Parameters Support pdf Power-law models \\(\\alpha \\in (0, \\infty)\\), \\(x_m \\in (0, \\infty)\\) \\(x \\in [0, \\infty)\\) \\(\\frac{\\alpha x_m^\\alpha}{x^{\\alpha+1}}\\) The Pareto is related to the Exponential. If \\(X\\sim \\text{Pareto}(x_m, \\alpha)\\), then \\(Y = \\log(\\frac{X}{x_m}) \\sim \\text{Exp}(\\alpha)\\) 4.5 Non-exponential families The following families are not exponential, but still commonly arise. 4.5.1 Uniform The Uniform distribution is parametrized by its minimum \\(a\\) and maximum \\(b\\). Denoted \\(U(a,b)\\), it describes a scenario where every possible value of \\(x\\) has the same probability. Description Parameters Support pdf Every \\(x\\) has same probability \\(-\\infty &lt; a &lt; b &lt; \\infty\\) \\(x \\in [a, b]\\) \\(\\frac{1}{b-a}\\) If \\(X \\sim \\text{Beta}(1,1)\\), then \\(X \\sim U(0,1)\\) If \\(X \\sim U(0,1)\\), then \\(-\\lambda \\log(X) \\sim \\text{Exp}(\\lambda)\\). If \\(X_i \\overset{iid}{\\sim} U(0,1)\\), then \\(X_{(k)} - X_{(j)} \\sim \\text{Beta}(k - j, n - (k - j) + 1)\\) By the Probability Integral Transform, inverse cdfs always follow a standard uniform distribution. That is, if \\(X = F_X^{-1}(Y)\\), then \\(Y \\sim U(0,1)\\). This can be used to generate any random variable with a known cdf. 4.5.2 Cauchy The Cauchy arises in situations involving ratios of standard normal variables, as well as rotations. It is \\(\\text{Cauchy}(x_0, \\gamma)\\), where \\(x_0\\) is a location parameter and \\(\\gamma\\) is a scale parameter, making it a location-scale family. Description Parameters Support pdf Rotations and ratios of normals \\(x_0 \\in \\mathbb{R}\\), \\(\\gamma \\in (0, \\infty)\\) \\(x \\in \\mathbb{R}\\) \\(\\frac{1}{\\pi \\gamma\\Big(1 + \\Big(\\frac{x - x_0}{\\gamma}\\Big)^2\\Big)}\\) If \\(U, V \\sim N(0,1)\\) independently, then \\(U/V \\sim Cauchy(0,1)\\) The Cauchy is often used as a pathological example in statistical problems, since it famously has no mean or variance (\\(E(X) = Var(X) = \\infty\\)) If \\(X\\sim t(1)\\), then \\(X\\sim \\text{Cauchy}(0,1)\\) 4.5.3 t-distribution The t-distribution describes the distribution of the t-statistic for \\(X_1, ... X_n \\overset{iid}{\\sim}N(\\mu, \\sigma^2)\\) \\[t = \\frac{\\bar{X} - \\mu}{\\sqrt{S^2/n}}\\] As a result, it is commonly used in hypothesis testing. Denoted \\(t(\\nu)\\), the parameter \\(\\nu\\) represents the degrees of freedom - the number of \\(X_i\\) in the sample that are being summed in the computation of \\(\\bar{X}\\) and \\(S^2\\). Description Parameters Support pdf Distribution of the t-statistic \\(\\nu \\in \\mathbb{N}\\) \\(x \\in \\mathbb{R}\\) \\(\\frac{\\Gamma((\\nu + 1) / 2)}{\\sqrt{\\nu\\pi}\\Gamma(\\nu/2)}\\Big(1 + \\frac{x^2}{\\nu}\\Big)^{-(\\nu+1)/2}\\) As \\(\\nu \\rightarrow \\infty\\), the t-distribution converges to a \\(N(0,1)\\). If \\(X \\sim t(1)\\) then \\(X \\sim \\text{Cauchy}(0,1)\\). 4.5.4 F-distribution Also useful for [hypothesis testing]](#hypothesis-tests-finite-samples), the F-distribution, denoted \\(F(n, m)\\), describes the distribution of the F-statistic: \\[X = \\frac{S_1 / n}{S_2 / m}\\] where \\(S_1\\) and \\(S_2\\) are the sums of independent standard normal random variables with degrees of freedom \\(n\\) and \\(m\\), respectively - that is, \\(S_1 \\sim \\chi^2(n)\\) and \\(S_2 \\sim \\chi^2(m)\\). Description Parameters Support pdf Distribution of the \\(F\\)-statistic \\(n \\in \\mathbb{N}\\), \\(m \\in \\mathbb{N}\\) \\(x \\in (0, \\infty)\\) \\(\\sqrt{\\frac{(nx)^nm^m}{(nx + m)^{n + m}}}\\frac{\\Gamma(n + m)}{x\\Gamma(n)\\Gamma(m)}\\) If \\(X \\ F(n, m)\\), then \\(\\frac{1}{X} \\sim F(m, n)\\) If \\(X \\sim t(n)\\), then \\(X^2 \\sim F(1, n)\\) If \\(X \\sim \\chi^2(n)\\) and \\(Y \\sim \\chi^2(m)\\), then \\(\\frac{X / n}{Y / m} \\sim F(n, m)\\) If \\(X_i \\overset{iid}{\\sim} \\text{Gamma}(\\alpha_i, \\beta_i)\\), then \\(\\frac{\\alpha_2\\beta1X_1}{\\alpha_1\\beta_2X_2} \\sim F(2\\alpha_1, 2\\alpha_2)\\) If \\(X \\sim \\text{Beta}(n/2, m/2)\\), then \\(\\frac{mX}{n(1-X)}\\sim F(n, m)\\) 4.5.5 Hypergeometric Imagine drawing \\(n\\) samples without replacement from a finite population of size \\(N\\). Suppose \\(K\\) of the units in the population are considered successes if drawn. The Hypergeometric distribution, denoted \\(\\text{HGeo}(N, K, n)\\), describes the probability that you will draw \\(x\\) successes under these circumstances. Description Parameters Support pmf Sampling without replacement \\(N \\in \\mathbb{N}_0\\), \\(K \\in \\{0, 1, ..., N\\}\\), \\(n \\in \\{0,1,...,N\\}\\) \\(x \\in \\{\\max(0, n+K-N),..., \\min(n,K)\\}\\) \\(\\frac{{K\\choose x}{N - K \\choose n - x -1}}{N \\choose n}\\) Fishers Exact Test is based on the Hypergeometric distribution. If \\(X \\sim \\text{HGeo}(N, K, n)\\), and \\(N\\) and \\(K\\) are sufficiently large compared to \\(n\\), then \\(X \\approx \\text{Binom}(n, p)\\) 4.6 Multivariate Distributions 4.6.1 Bivariate Normal The bivariate normal describes a situation where two random variables \\(X\\) and \\(Y\\) are normally distributed, and their sum is also normally distributed. Description Parameters Support pmf Two-dimensional normal \\(\\mu_x, \\mu_y \\in \\mathbb{R}\\), \\(\\sigma_x, \\sigma_y \\in \\mathbb{R} &gt; 0\\), \\(\\rho \\in [-1, 1]\\) \\(x\\in\\mathbb{R}^2\\) \\(\\frac{1}{2\\pi\\sigma_x\\sigma_y\\sqrt{1-\\rho^2}}\\exp\\Big(-\\frac{1}{2(1-\\rho^2)}\\Big((\\frac{x-\\mu_x}{\\sigma_x})^2 - 2\\rho(\\frac{x - \\mu_x}{\\sigma_x})(\\frac{y - \\mu_y}{\\sigma_y}) + (\\frac{y-\\mu_y}{\\sigma_y})^2\\Big)\\Big)\\) Suppose that \\((X, Y)\\) follows the bivariate normal distribution above. Then, The marginal distributions are \\(X \\sim N(\\mu_x, \\sigma_x^2)\\) and \\(Y \\sim N(\\mu_y, \\sigma_y^2)\\) \\(Corr(X, Y) = \\rho\\) Any linear combination of \\(X\\) and \\(Y\\) is univariate normal. That is, \\(aX + bY \\sim N(a\\mu_x + b\\mu_y, a^2\\sigma_x^2 + b^2\\sigma_y^2 +2ab\\rho\\sigma_x\\sigma_y\\). The conditional distribution \\(Y|X = x \\sim N\\Big(\\mu_Y + \\frac{\\sigma_Y}{\\sigma_X}\\rho(x - \\mu_X), \\sigma_Y^2(1-\\rho^2)\\Big)\\) 4.6.2 Multivariate Normal The Multivariate Normal, often denoted \\(MVN(\\mu, \\Sigma)\\), generalizes the normal to a random vector \\(X\\) where all linear combinations of its components have a univariate normal distribution. Description Parameters Support pmf \\(k\\)-dimension normal \\(\\mu \\in \\mathbb{R}^k\\), \\(\\Sigma \\in \\mathbb{R}^{k \\times k}\\) \\(x\\in\\mathbb{R}^k\\) \\(\\frac{1}{\\sqrt{(2\\pi)^{k}\\det(\\Sigma)}}\\exp\\Big(-\\frac{1}{2}(x - \\mu)^\\top\\Sigma^{-1}(x-\\mu)\\Big)\\) If a vector \\(X\\sim MVN(\\mu, \\Sigma)\\), then All marginal distributions of \\(X\\) follow a multivariate normal with the marginalized means and rows/columns in the covariance matrix dropped. For example, \\(X_1 \\sim N(\\mu_1, \\sigma_1)\\) If \\(Y = c + BX\\), then \\(Y \\sim MVN(c + B\\mu, B\\Sigma B^\\top)\\) Note that two normally distributed random variables may not be jointly bivariate normal! 4.6.3 Multinomial Consider an event in which one of \\(k\\) discrete outcomes is guaranteed to occur - for instance, rolling a 6-sided die, where there are \\(k = 6\\) possible outcomes. Repeat this event \\(n\\) times. If outcome \\(i\\) occurs with probability \\(p_i\\) (not necessarily equal), then the number of times \\(X_i\\) that each outcome \\(i = 1,...,k\\) occurs after \\(n\\) trials is modeled by the Multinomial distribution. \\(X\\) is a vector representing the number of successes of each event. Description Parameters Support pmf \\(k\\) joint binomials \\(n \\in \\mathbb{N}\\), \\(k \\in \\mathbb{N}\\), \\(p_1,...,p_k\\in (0,1)\\), \\(\\sum_{i=1}^kp_i = 1\\) \\(x_i\\in \\mathbb{N}\\), \\(\\sum_{i=1}^kx_i = n\\) \\(\\frac{n!}{\\prod_{i=1}^kx_i!}\\prod_{i=1}^kp_i^{x_i}\\) If the vector \\(X\\) is multinomial, then All marginal distributions of \\(X\\) are multinomial All conditional distributions are multinomial \\(X_i \\sim \\text{Binomial}(n, p_i)\\) \\(Cov(X_i, X_j) = E((X_i - mp_i)(X_j - mp_j)) = -mp_ip_j, \\forall i\\neq j\\) 4.7 Medians and Other Functionals of a Distribution In statistics, a functional is a function that maps a distribution (CDF) to a real number. For example, expected value is a type of functional treated in-depth in Chapter 6: Moments. Another type of functional that arises in first-year statistical inference courses is the median, which is defined as follows: Definition 4.5 (Median) The median \\(m\\) of a random variable \\(X\\) is a value satisfying \\[P(X \\leq m) = P(X \\geq m) = \\frac{1}{2}\\] Intuitively, it is a value that splits the distribution such that half of the probability mass lies on one side and half lies on the other. For continuous random variables, \\(m\\) satisfies \\[\\int_{-\\infty}^m f(x)dx = \\int_{m}^\\infty f(x)dx = \\frac{1}{2}\\] If \\(X\\) has a symmetric distribution - that is, there exists some \\(a\\) such that \\[\\int_{-\\infty}^a f_X(x)dx = \\int_{a}^\\infty f_X(x)dx\\] and its mean \\(E(X)\\) exists, then \\(a = m = E(X)\\). We define \\(E(X)\\) and techniques for computing it in Chapter 6). Therefore, the median is equal to the mean for the following distributions: Normal Uniform Students t as well as any other symmetric distribution. Heres how to prove such a fact. Example 4.3 (Proving a median is a mean) Suppose \\(X\\) is a symmetric location family with location parameter \\(\\mu = E(X)\\). We can show \\(\\mu\\) must be the median as follows. Since \\(X\\) is symmetric, we can expand directly from the definition of a pdf, using the property that it must integrate to 1: \\[ \\int_{-\\infty}^\\infty f_X(x)dx = \\int_{-\\infty}^\\mu f_X(x)dx + \\int_{\\mu}^\\infty f_X(x)dx = 1 \\\\ \\iff 2\\int_{-\\infty}^\\mu f_X(x) = 1 \\iff \\int_{-\\infty}^\\mu f_X(x) = \\frac{1}{2} \\] by definition of symmetry, showing that \\(\\mu\\) is a median by definition. If \\(X\\) is not symmetric, we can calculate a median using the cumulative distribution function (CDF) of the given distribution. Here are two examples: Example 4.4 (Calculating a Median: Exponential) This proof comes from Soch et al. (2022). Suppose we want to find the median \\(m\\) of a \\(X\\sim \\text{Exp}(\\lambda)\\) distribution. By the definition of a median, \\(F_X(m) = \\frac{1}{2}\\) Since the CDF of an exponential distribution is \\[\\int_0^x\\lambda\\exp(-\\lambda t)dt = 1 - \\exp(-\\lambda x)\\] So, \\(1 - \\exp(-\\lambda m) = \\frac{1}{2} \\implies \\log(\\frac{1}{2}) = -\\lambda m \\implies m = \\frac{\\log(2)}{\\lambda}\\) Hence, weve calculated the median of the exponential distribution. Example 4.5 (Calculating a Median: Cauchy) Suppose \\(X \\sim \\text{Cauchy}(x_0, \\gamma)\\). Again, we use the CDF: $$ F_X(x) = () + = \\ = (0)\\ m = x_0 $$ proving the median of the Cauchy is its scale parameter. Often, we may be interested in estimating a median as well. Techniques for this are discussed in Chapters 8 and 9. What about other functionals? Quantiles are functionals that define points on a distribution such that a certain probability mass lies among those values less than the quantile. An example is the 75th percentile - values less than this percentile make up 75 percent of the probability mass. We also just discussed the median, which is the 50th percentile. Mathematically, we can write a quantile \\(q_p\\) as \\[F(q_p) = p\\] So for example, the 75th percentile would be a value \\(q\\) satisfying $F(q) = 0.75. We can solve for quantiles the same way we did the median - heres an example: Example 4.6 (Percentiles of the Cauchy) For \\(X\\sim \\text{Cauchy}(0, \\gamma)\\), the parameter \\(\\gamma\\) characterizes the 25th and 75th percentiles. Lets prove it using the same technique as the median - using the CDF of the Cauchy. $$ F(q) = () + = \\ = () = 1\\ q = $$ The same process can be repeated for the 25th percentile, since \\(\\tan(\\frac{3\\pi}{4}) = -1\\), so the 25th percentile is \\(-\\gamma\\). References "],["new-distributions.html", "Chapter 5 New Distributions 5.1 Transformations 5.2 Computing Joint Probabilities 5.3 Probability Integral Transform", " Chapter 5 New Distributions This chapter will demonstrate how to derive the PDF or CDF of a random variable that is a function of other random variables, including hierarchical models. 5.1 Transformations 5.1.1 Theorems Well start by introducing two useful theorems for finding the PDF of a transformed random variable \\(Y=g(X)\\) where \\(g(X)\\) is a monotone, one-to-one function over the domain of interest.   Definition 5.1 (Theorem 2.1.5) Let \\(X\\) have pdf \\(f_X(x)\\) and let \\(Y=g(X)\\), where \\(g\\) is a monotone function. Let \\(\\mathcal{X}\\) and \\(\\mathcal{Y}\\) be defined by (2.1.7). Suppose that \\(f_X(x)\\) is continuous on \\(\\mathcal{X}\\) and that \\(g^{-1}(y)\\) has a continuous derivative on \\(\\mathcal{Y}\\). Then the PDF of \\(Y\\) is given by \\[ f_Y(y)= \\begin{cases}f_X\\left(g^{-1}(y)\\right)\\left|\\frac{d}{d y} g^{-1}(y)\\right| &amp; y \\in \\mathcal{Y} \\\\ 0 &amp; \\text { otherwise}\\end{cases} \\] A non-rigorous sketch of the proof of this follows from the Direction Probability Manipulation of the CDF described in Chapter 2. Note \\[ F(Y &lt; y) = P(Y &lt; y) = P(g(X) &lt; y) \\\\ = P(X &lt; g^{-1}(y)) = F_x(g^{-1}(y)) \\] Then, \\(f(y) = \\frac{d}{dy}F_Y(y) = f_X(g^{-1}(y)\\cdot|\\frac{d}{dy}g^{-1}(y)|\\) by the chain rule (the absolute value is necessary to handle the case where \\(g\\) is decreasing) In some cases, \\(g(X)\\) will only be a monotone, one-to-one function over subsets of the domain of interest. Below, Theorem 2.1.8 provides a generalization of Theorem 2.1.5 that can be applied in these situations. Definition 5.2 (Theorem 2.1.8) Let \\(X\\) have PDF \\(f_X(x)\\), let \\(Y=g(X)\\), and define the sample space \\(\\mathcal{X}\\) as in (2.1.7). Suppose there exists a partition, \\(A_0, A_1, \\ldots, A_k\\), of \\(\\mathcal{X}\\) such that \\(P\\left(X \\in A_0\\right)=0\\) and \\(f_X(x)\\) is continuous on each \\(A_i\\). Further, suppose there exist functions \\(g_1(x), \\ldots, g_k(x)\\), defined on \\(A_1, \\ldots, A_k\\), respectively, satisfying \\(g(x)=g_i(x)\\), for \\(x \\in A_i\\), \\(g_i(x)\\) is monotone on \\(A_i\\), the set \\(\\mathcal{Y}=\\left\\{y: y=g_i(x)\\right.\\) for some \\(\\left.x \\in A_i\\right\\}\\) is the same for each \\(i=1, \\ldots, k\\), and \\(g_i^{-1}(y)\\) has a continuous derivative on \\(\\mathcal{Y}\\), for each \\(i=1, \\ldots, k\\). Then \\[ f_Y(y)= \\begin{cases}\\sum_{i=1}^k f_X\\left(g_i^{-1}(y)\\right)\\left|\\frac{d}{d y} g_i^{-1}(y)\\right| &amp; y \\in \\mathcal{Y} \\\\ 0 &amp; \\text { otherwise }\\end{cases} \\] 5.1.2 Practical Strategy Now, we will outline a practical strategy for solving problems that involve transformations. Given a transformation \\(y=g(x)\\) (univariate) or \\(u,v=g(x,y)\\) (multivariate), perform a transformation by following these steps: Step 1: Define PMF/PDF of the untransformed random variable(s). Univariate: \\(f(x)\\) (given) Multivariate: calculate joint density \\(f(x,y)\\) (not always given) Step 2: Find the inverse. Univariate: \\(y=g(x) \\quad \\Rightarrow \\quad x=g^{-1}(y)\\) Multivariate: \\(u=g(x,y), v=g(x,y) \\quad \\Rightarrow \\quad x=g^{-1}(u,v),y=g^{-1}(u,v)\\) (Note: for multivariate case, solve system of equations) Step 3: Calculate jacobian of the transformation. For transformations of multiple random variables, recall bivariate Jacobian: \\[ |J|=\\left|\\begin{array}{ll} \\frac{\\partial x}{\\partial u} &amp; \\frac{\\partial x}{\\partial v} \\\\ \\frac{\\partial y}{\\partial u} &amp; \\frac{\\partial y}{\\partial v} \\end{array}\\right|=\\left| \\frac{\\partial x}{\\partial u} \\frac{\\partial y}{\\partial v}-\\frac{\\partial y}{\\partial u} \\frac{\\partial x}{\\partial v} \\right| \\] Step 4: Calculate PMF/PDF of the transformed random variable. \\[ f_Y(y)= f_X\\left(g^{-1}(y)\\right)\\left|\\frac{d}{d y} g^{-1}(y)\\right| \\] \\[ f_{U,V}(u,v)= f_{X,Y}\\left(g^{-1}(u,v)\\right) \\left|\\begin{array}{ll} \\frac{\\partial x}{\\partial u} &amp; \\frac{\\partial x}{\\partial v} \\\\ \\frac{\\partial y}{\\partial u} &amp; \\frac{\\partial y}{\\partial v} \\end{array}\\right| \\] Example 5.1 (Chi-squared density) Suppose \\(Z \\sim N(0,1)\\). What is the distribution of \\(Y = Z^2\\)? Here, \\(g\\) is not monotone. If we want to proceed using the Transformation Theorem, we need split up the domain into areas where \\(g\\) is monotone. This, trivially, is \\(z &lt; 0\\) and \\(z &gt; 0\\). Then, \\[g(z) = z^2 \\implies z = g^{-1}(y) = \\begin{cases}-\\sqrt{y} &amp; z &lt; 0\\\\\\sqrt{y} &amp; z&gt;0\\end{cases}\\]. An intuition for why we need to sum the pdfs for non-monotone functions is that \\(Y = Z^2\\) maps multiple values of \\(Z\\) to the same value of \\(Y\\). For example, \\(g(z)\\) maps both \\(Z = -1\\) and \\(Z = 1\\) to \\(Y = 1\\). Therefore, \\(f_Y(1)\\) needs to include both the probability of \\(Z = -1\\) and \\(Z = 1\\). Hence, we apply the Non-Monotone Transformation Theorem to obtain \\[ f_Y(y) = f_Z(\\sqrt{y})\\Big|\\frac{d}{dy}\\sqrt{y}\\Big| + f_Z(-\\sqrt{y})\\Big|\\frac{d}{dy}-\\sqrt{y}\\Big|\\\\ = \\frac{1}{\\sqrt{2\\pi}}\\cdot \\exp(-\\frac{1}{2}(\\sqrt{y})^2\\cdot\\frac{1}{2\\sqrt{y}} + \\frac{1}{\\sqrt{2\\pi}}\\cdot \\exp(-\\frac{1}{2}(-\\sqrt{y})^2\\cdot\\frac{1}{2\\sqrt{y}}\\\\ = \\frac{1}{\\sqrt{2\\pi}y^\\frac{1}{2}}\\exp(-\\frac{y}{2}) \\] which is a \\(\\chi^2(1)\\) distribution. Alternatively, we could have proceeded directly using Direct Probability Manipulation like so: \\[ F(y) = P(Y \\leq y) = P(Z^2 \\leq y)\\\\ = P(-\\sqrt{y}&lt; Z &lt; \\sqrt{y}) = \\Phi(\\sqrt{y}) - \\Phi(-\\sqrt{y}) \\] where \\(\\Phi(z)\\) is the normal cdf. Then, \\[ f_Y(y) = \\frac{d}{dy}F_Y(y) = f_y(\\sqrt{y})\\cdot\\frac{d}{dy}{\\sqrt{y}} - f_y(\\sqrt{y})\\cdot(\\frac{d}{dy}{-\\sqrt{y}})\\\\ = \\frac{1}{\\sqrt{2\\pi}y^\\frac{1}{2}}\\exp\\Big(-\\frac{y}{2}\\Big) \\] the same result as the theorem. 5.1.3 Proving Independence From a Joint Transformation Some problems may involve proving that two transformations \\(U = g_1(x,y)\\) and \\(V = g_2(x,y)\\) are independent. To do this, we can first compute the joint distribution using the Transformation Theorem, and then factorize the joint distribution into components containing only \\(u\\) and \\(v\\) respectively. Example 5.2 (Independence From a Transformation) test 5.2 Computing Joint Probabilities Besides finding the joint pdf of a function, one may also be asked to compute a probability involving two random variables, such as \\(P(X &gt; Y)\\), given a joint distribution \\(f_{X, Y}(x,y)\\). Solving these is generally a two-step process: Define the region of interest. This will be a set \\(\\{(x, y)\\}\\) with restriction on \\(x\\) and \\(y\\) given in the problem. For example, to compute \\(P(X &gt; Y)\\) where \\(x, y \\in [-1, 1]\\), it might take the form \\(\\{(x,y) : -1 &lt; y &lt; x &lt; 1\\}\\). Set up and solve a double integral over the region of interest. Following the above, you might write \\[P(X &gt; Y) = \\int_{-1}^1\\int_{-1}^x f(x,y)dydx\\] Setting up the bounds of these integrals can be very tricky, so make sure they are computed carefully! Some tips: It is generally always advisable to graph the region to be integrated, in order to ensure that is has been specified correctly. Unlike when computing a CDF, when computing the value of an expression with a greater than sign, such as \\(P(X + Y &gt; 1)\\), you can proceed directly. You do not need to flip the sign - and in fact, it may result in error if the new expression with \\(&lt;\\) is not strictly the complement of the old one containing \\(&gt;\\). However, if you have the joint CDF rather than the joint PDF, it may be advantageous to do the above if it allows you to evaluate the CDF directly, without requiring any integration. 5.3 Probability Integral Transform The probability integral transform is a powerful technique that allows for the simulation of random variables from any distribution, given that you have access to a source of uniformly distributed random numbers. This method is particularly useful because it provides a straightforward way to transform uniform random variables into random variables following a desired distribution. The general idea is as follows: When you plug any continuous random variable \\(X\\) into its own CDF, you get \\(U \\sim\\) Uniform(0,1): \\[ F_X(X) \\sim \\text{Uniform} (0,1) \\] When you plug \\(U \\sim\\) Uniform(0,1) into an inverse CDF, you get a continuous random variable \\(X\\) with that CDF: \\[ F_X^{-1}(U) \\text{ has CDF } F_X(x) \\] Procedure: Construct a random variable \\(X\\) with a particular CDF \\(F_X(x)\\) Generate \\(U \\sim\\) Uniform(0,1) Plug \\(U\\) into the inverse CDF: \\(X=F^{-1}_X(U)\\) \\(X\\) is distributed according to the CDF \\(F_X(x)\\)   Click Here: Visual Explanation of Universality of the Uniform 5.3.1 Hiearchical Models (Iterated Moments) Definition 5.3 (Theorem 4.4.3 and Theorem 4.4.7) If \\(X\\) and \\(Y\\) are any two random variables and the relevant expectations exist, then \\[ \\begin{aligned} E_X(X) &amp; =E_Y\\left(E_{X \\mid Y}(X \\mid Y)\\right) \\\\ \\operatorname{Var}_X(X) &amp; =E_Y\\left(\\operatorname{Var}_{X \\mid Y}(X \\mid Y)\\right)+\\operatorname{Var}_Y\\left(E_{X \\mid Y}(X \\mid Y)\\right) \\end{aligned} \\] 5.3.2 Convolutions Convolutions arise when we calculate the probability distribution of the sum or linear combination of independent random variables. In the following table, we provide a compilation of convolutions involving random variables from well-established probability distributions. Acquainting yourself with these convolutions can significantly improve the efficiency of computing moment generating functions and other statistical expressions of interest. \\[ \\begin{array}{l|l} f_{X_i} &amp; f_{\\Sigma X_i}\\left(X_i \\text { independent }\\right) \\\\ \\hline \\operatorname{Ber}(p) &amp; \\operatorname{Bin}(n, p) \\\\ \\operatorname{Bin}\\left(n_i, p\\right) &amp; \\operatorname{Bin}\\left(\\sum_i n_i, p\\right) \\\\ \\operatorname{Geo}(p) &amp; \\operatorname{NBin}(n, p) \\\\ \\operatorname{Exp}(\\beta) &amp; \\operatorname{Gam}(n, \\beta) \\quad \\quad \\text { Note: } E\\left(X_i\\right)=\\frac{1}{\\beta}, E\\left(\\sum X_i\\right)=\\frac{n}{\\beta} \\\\ \\operatorname{Gam}\\left(n_i, \\beta\\right) &amp; \\operatorname{Gam}\\left(\\sum_i n_i, \\beta\\right) \\\\ \\operatorname{Pois}\\left(\\lambda_i\\right) &amp; \\operatorname{Pois}\\left(\\sum_i \\lambda_i\\right) \\\\ \\chi^2(1) &amp; \\chi^2(n) \\\\ \\chi^2\\left(n_i\\right) &amp; \\chi^2\\left(\\sum_i n_i\\right) \\\\ \\operatorname{Norm}\\left(\\mu_i, \\sigma_i^2\\right) &amp; \\operatorname{Norm}\\left(\\sum_i \\mu_i, \\sum_i \\sigma_i^2\\right) \\end{array} \\]   Convolutions can also be computed directly as follows: Definition 5.4 (Theorem) Let \\(X\\) and \\(Y\\) be two independent continuous random variables with possible values \\(\\mathcal{X}\\) and \\(\\mathcal{Y}\\) and let \\(Z=X+Y\\). Then, the probability density function of \\(Z\\) is given by \\[ \\begin{aligned} f_Z(z) &amp; =\\int_{-\\infty}^{+\\infty} f_X(z-y) f_Y(y) \\mathrm{d} y \\\\ \\text { or } \\quad f_Z(z) &amp; =\\int_{-\\infty}^{+\\infty} f_Y(z-x) f_X(x) \\mathrm{d} x \\end{aligned} \\] where \\(f_X(x), f_Y(y)\\) and \\(f_Z(z)\\) are the probability density functions of \\(X, Y\\) and \\(Z\\). Tip: For convolutions, this theorem provides an alternative to the multivariate Jacobian above.   Lastly, we can apply the below theorem to easily compute the moment generating function and characteristic function of a convolution: Definition 5.5 (Theorem 4.2.12) Let \\(X\\) and \\(Y\\) be independent random variables with moment generating \\(M_X(t)\\) and \\(M_Y(t)\\). Then the moment generating and characteristic functions of the random variable \\(Z=X+Y\\) are given by \\[ M_Z(t)=M_X(t) \\cdot M_Y(t) \\] \\[ \\varphi_Z(t) =\\varphi_X(t) \\cdot \\varphi_Y(t) \\] For the random variable \\(Z = X-Y\\), the moment generating and characteristic functions are given by: \\[ M_Z(t)=M_X(t) \\cdot M_Y(-t) \\] \\[ \\varphi_Z(t) =\\varphi_X(t) \\cdot \\varphi_Y(-t) \\] "],["moments.html", "Chapter 6 Moments 6.1 Basic Definitions 6.2 \\(E(X)\\) Properties 6.3 \\(Var(X)\\) Properties 6.4 Covariance and Correlation 6.5 Conditional Expectation 6.6 Moment Generating Functions 6.7 Moment Inequalities 6.8 Techniques for Deriving Moments 6.9 Other Moments (for reference)", " Chapter 6 Moments Finding the moments of a random variable is a chief problem in statistics. This is because moments characterize important properties about a distribution - for example, the mean measures the central tendency of a random variable, while the variance measures its dispersion. This chapter will define expected value and moments, summarize their useful properties, and discuss strategies for finding moments, especially for common distributions. 6.1 Basic Definitions Definition 6.1 (Expected Value) The expected value \\(E(g(X))\\) is the average value of a random variable \\(g(X)\\) across its support \\(\\mathcal{X}\\), weighted by their probability. If \\(X\\) is discrete, then this is defined \\[E(g(X)) = \\sum_{x \\in \\mathcal{X}}g(x)P(X = x)\\] If \\(X\\) is continuous, then the expected value is \\[E(g(x)) = \\int_{\\mathcal{X}}g(x)f_X(x)dx\\] Definition 6.2 (Multivariate Expected Value) If \\(X_1, X_2,...X_n\\) are discrete, the expected value over a function of multiple random variables is defined as \\[E(g(X_1, X_2,...X_n)) = \\sum_{x_1}\\sum_{x_2}...\\sum_{x_n}g(x_1, x_2, ..., x_n)P(X_1 = x_1, X_2 = x_2, ...X_n = x_n)\\] If \\(X_1, X_2,...X_n\\) are continuous, the expected value is instead \\[E(g(X_1, X_2,...X_n)) = \\int_{x_1\\in \\mathcal{X}_1}\\int_{x_2\\in \\mathcal{X}_2}...\\int_{x_n\\in \\mathcal{X}_n}g(x_1, x_2, ..., x_n)f_{X_1, X_2,...,X_n}(x_1, x_2, ..., x_n)\\] Definition 6.3 (Moments) The \\(n\\)th moment of a random variable \\(X\\) is defined as \\(E(X^n)\\). Similarly, the \\(n\\)th central moment is defined as \\(E((X - E(X))^n)\\). The first moment \\(E(X)\\) is also known as the mean. The second central moment is the variance, denoted \\(Var(X) = E((X - E(X))^2)\\) 6.2 \\(E(X)\\) Properties The linearity of expectation is defined as \\(E(aX + b) = aE(X) + b\\). If multiple random variables \\(X\\) and \\(Y\\) are involved, then \\[E(ag_1(X) + bg_2(Y) + c) = aE(g_1(X)) + bE(g_2(Y)) + c\\] This follows from the linearity of the integral operator. Since sums of random variables are so common, this property is incredibly useful, especially for proving the unbiasedness of estimators (see Chapter 8). For example, we can use the linearity of expectation to prove that the sample mean \\(\\bar{X} = \\frac{1}{n}\\sum_{i=1}^nX_i\\) is unbiased for a set of iid \\(X_i\\) by noting, by linearity of expectation, \\[E(\\bar{X}) = E\\Big(\\frac{1}{n}\\sum_{i=1}^nX_i\\Big) = \\frac{1}{n}\\sum_{i=1}^nE(X_i) = \\frac{n}{n}E(X_i) = E(X_i)\\] When \\(X\\) and \\(Y\\) are independent, \\[E(XY) = E(X)E(Y)\\] This property can also be useful for computing the expectation of iid random variables in statistical inference problems. 6.3 \\(Var(X)\\) Properties The most important variance property is its alternative definition: \\[Var(X) = E(X^2) - E(X)^2\\] Often, we are interested in both the mean and the variance. By simplifying \\(Var(X)\\) into a function of the first and second moments, we can compute \\(E(X^2)\\) (which is often much easier) and use what we know about \\(E(X)\\) to more easily compute \\(Var(X)\\). While variance is not exactly linear, the variance of a linear transformation of a random variable is \\[Var(aX + b) = a^2Var(X)\\] When multiple random variables are involved in a linear expression, then \\[Var(aX + bY) = a^2Var(X) + b^2Var(Y) + 2ab\\cdot Cov(X, Y)\\] where \\(Cov(X,Y)\\) is described in the next subsection. If \\(X\\) and \\(Y\\) are independent then \\(Cov(X,Y) = 0\\) and \\(Var(aX + bY) = a^2Var(X) + b^2Var(X)\\) 6.4 Covariance and Correlation From the expected value function and moments, we can also define the covariance and correlation Definition 6.4 (Covariance) Covariance measures the joint dispersion of two random variables. Mathematically, \\[Cov(X,Y) = E((X - E(X))(Y - E(Y)))\\] Equivalently, \\[Cov(X,Y) = E(XY) - E(X)E(Y)\\] which is usually the more convenient definition. Note that if \\(X\\) and \\(Y\\) are independent, then \\(Cov(X,Y) = 0\\), which simplifies calculations. However, make no mistake, this implication is not bidirectional: \\(Cov(X,Y) = 0\\) does NOT imply X, Y$ are independent! Covariance of Linear Combinations: Similar to variance, covariance also has a special property for linear combinations of random variables. \\[Cov(aX + bY, cW + dV) = ac\\cdot Cov(X,W) + ad\\cdot Cov(X,V) + bc\\cdot Cov(Y,W) + bd\\cdot Cov(Y, V)\\] Stemming from this, when covariance is calculated for a sum of random variables, independent components can be ignored. For example, if \\(Y_1 = X_1 + Z_1\\) and \\(Y_2 = X_2 + Z_2\\), and \\(X_1\\) and \\(X_2\\) are independent, then \\[Cov(Y_1, Y_2) = Cov(X_1 + Z_1, X_2 + Z_2) = Cov(Z_1, Z_2)\\] Relationship Between Covariance and Variance. \\[Cov(X, X) = Var(X)\\] The Covariance Matrix. For vectors of random variables, we can define a covariance matrix as follows. Let \\(X = \\begin{bmatrix} X_1, ..., X_n \\end{bmatrix}\\). Then, \\[ Cov(X) = \\begin{bmatrix} Var(X_1) &amp; ... &amp; Cov(X_1,X_n)\\\\ ... &amp; ... &amp; ... \\\\ Cov(X_n, X_1) &amp; ... &amp; Var(X_n) \\end{bmatrix} \\] This is especially useful when working with the Multivariate Normal Occasionally, we might want to work with a measure of joint dispersion that is normalized. This is called the correlation. Definition 6.5 (Covariance) Correlation is a measure of the joint dispersion of two random variables normalized to [0,1], with \\(Corr(X,Y) = 0\\) indicating that the variables are independent and \\(Corr(X,Y) = 1\\) indicating perfect collinearity. Mathematically, \\[Cov(X,Y) = E\\Big(\\frac{X - E(X)}{\\sqrt{Var(X)}}\\cdot \\frac{Y - E(Y)}{\\sqrt{Var(Y)}}\\Big) = \\frac{Cov(X,Y)}{\\sqrt{Var(X)Var(Y)}}\\] 6.5 Conditional Expectation When an expectation is computed using a conditional probability, it is known as a conditonal expectation. For discrete random variables (or when \\(Y\\) is simply an event), it is denoted \\[E(g(X)|Y) = \\sum_{\\mathcal{X}}g(x)P(X = x | Y)\\] and for continuous, \\[E(g(X) | Y) = \\int_{\\mathcal{X}}g(x)f_{X|Y}(x|y)\\] Conditional expectation has the same linear properties as discussed previously in the non-conditional case. In addition, \\(E(g(X,Y) | X = x) = E(g(x, Y) | X = x)\\) If \\(X\\) and \\(Y\\) are independent, then \\(E(Y | X = x) = E(Y)\\) \\(E(g(X)Y | X) = g(X) \\cdot E(Y | X)\\) Two fundamental properties regarding conditional expectations exist that simply the computation of its unconditional counterpart. Definition 6.6 (Law of Iterated Expectation (Adam's Law)) \\(E(X) = E(E(X|Y))\\) Definition 6.7 (Law of Total Variance (EVVE's Law)) \\(Var(X) = E(Var(X|Y)) + Var(E(X|Y))\\) These two properties can be used to compute \\(E(X)\\) when only \\(E(X|Y)\\) is known. For example, this occurs in Chapter 8 when finding UMVUEs. Note on Regression Functions: Given covariates \\(X = X_1, X_2, ..., X_n\\), finding a model for \\(E(Y | X)\\) is also known as regression. Functions of \\(X\\) are linear if \\(d(X) = \\beta_0 + \\beta_1X_1 +...+\\beta_n X_n\\). See Gut (2009) for more information on regression problems. 6.6 Moment Generating Functions Definition 6.8 (MGF) The moment generating function (MGF) is defined as \\[\\mathcal{M}_X(t) = E(e^{tx})\\] This function has three important properties: The MGF fully characterizes a distribution. That is, if \\(\\mathcal{M}_X(t) = \\mathcal{M}_Y(t)\\), then \\(X\\) and \\(Y\\) are identically distributed. \\(E(X^n) = \\frac{d^n}{dt^n}\\mathcal{M}_X(t)\\Big|_{t=0}\\). This means the MGF can also be used to compute any given moment simply by taking a derivative! Convolution: If \\(X\\) and \\(Y\\) and independent, then the mgf of \\(X+Y\\) is \\(\\mathcal{M}_{X+Y}(t) = \\mathcal{M}_{X}(t)\\mathcal{M}_{Y}(t)\\). This is useful for finding the distribution of sums of random variables. The Convolution property can also be extended to subtraction. If \\(X = Y_1 - Y_2\\), \\[\\mathcal{M}_X(t) = \\mathcal{M}_{Y_1}(t) \\cdot \\mathcal{M}_{Y_2}(-t)\\] Do note, however, that the MGF may not exist for some distributions. In this case it may be preferable to work with the characteristic function, which follows fundamentally the same principles, except one solves for \\(\\phi_X(t) = E(e^{itx})\\), where \\(i = \\sqrt{-1}\\). 6.7 Moment Inequalities Several inequalities exist that can bound moments. A general bound is that \\(a &lt; g(X) &lt; b \\implies a &lt; E(g(X)) &lt; b\\). This, coupled with the triangle inequality can be used to prove the following inequalities. The next two inequalities bound probabilities based on moments. They are named the student-teacher pair that developed them. Definition 6.9 (Markov's Inequality) \\[P(X \\geq t) \\leq \\frac{E(X)}{t}\\] Definition 6.10 (Chebychev's Inequality) This inequality can be stated in several ways: Chebychevs inequality is instrumental in proving the Weak Law of Large Numbers. Next, we present an equality regarding functions of moments. Definition 6.11 (Jensen's Inequality) For a convex function \\(f\\), \\[E(f(X)) \\geq f(E(X))\\] If \\(f\\) is instead concave, \\[E(f(x)) \\leq f(E(X))\\] For both definitions, equality only holds if \\(f(x)\\) is a linear function of \\(x\\). Jensens inequality is useful for showing that an estimator is biased. The next three inequalities are less commonly used, but are still useful is certain situations (where?) Definition 6.12 (Cauchy-Schwarz Inequality) \\[E(XY)^2 \\leq E(X)^2E(Y)^2\\] This equality can be generalized as follows Definition 6.13 (Holder's Inequality) For \\(p, q \\geq 1\\) such that \\(\\frac{1}{p} + \\frac{1}{q} = 1\\), \\[E(|XY|) \\leq E(|X|^p)^\\frac{1}{p}E(|Y|^q)^\\frac{1}{q}\\] While the above inequalities deal with products of moments, the following handles sums: Definition 6.14 (Minkowski's Inequality) \\[E(|X + Y|^p)^\\frac{1}{p} \\leq E(|X|^p)^\\frac{1}{p} + E(|Y|^p)^\\frac{1}{p}\\] 6.8 Techniques for Deriving Moments Below, weve listed the important moments of each distribution that can be reasonably derived by hand. In this section, we discuss a myriad of techniques to derive these moments, each of which can be applied in general for their respective distributions. Distribution \\(E(Y)\\) \\(Var(Y)\\) mgf \\(\\text{Bernoulli}(p)\\) \\(p\\) \\(p(1-p)\\) \\((1-p) + pe^t\\) \\(\\text{Binom}(n, p)\\) \\(np\\) \\(np(1-p)\\) \\(((1-p) + pe^t)^n\\) \\(\\text{Geo}(p)\\) \\(\\frac{1-p}{p}\\) \\(\\frac{1-p}{p^2}\\) \\(\\frac{p}{1 - (1 - p)e^t}\\) for \\(t &lt; -\\log(1-p)\\) \\(\\text{NegBinom(r, p)}\\) \\(\\frac{r(1-p)}{p}\\) \\(\\frac{r(1-p)}{p^2}\\) \\(\\Big(\\frac{p}{1 - (1 - p)e^t}\\Big)^r\\) for \\(t &lt; -\\log(p)\\) \\(\\text{Pois}(\\lambda)\\) \\(\\lambda\\) \\(\\lambda\\) \\(\\exp(\\lambda(e^t - 1))\\) \\(\\text{Normal}(\\mu, \\sigma^2)\\) \\(\\mu\\) \\(\\sigma^2\\) \\(\\exp(\\mu t + \\sigma^2 t^2 / 2)\\) \\(\\text{Exp}(\\lambda)\\) \\(\\lambda\\) \\(\\lambda^2\\) \\(\\frac{1}{1-\\lambda t}\\) for \\(t &gt; \\lambda\\) \\(\\text{Gamma}(k, \\lambda)\\) \\(k\\lambda\\) \\(k\\lambda^2\\) \\((1 - \\lambda t)^{-k}\\) for \\(t &lt; \\frac{1}{\\lambda}\\) \\(\\text{Beta}(\\alpha, \\beta)\\) \\(\\frac{\\alpha}{\\alpha + \\beta}\\) \\(\\frac{\\alpha\\beta}{(\\alpha + \\beta)^2(\\alpha + \\beta + 1)}\\) \\(1 + \\sum_{k=1}^\\infty\\Big(\\prod_{r=0}^{k-1}\\frac{\\alpha + r}{\\alpha + \\beta + r}\\Big)\\frac{t^k}{k!}\\) \\(\\chi^2(\\nu)\\) \\(\\nu\\) \\(2\\nu\\) \\((1 - 2t)^{-\\nu/2}\\) for \\(t &lt; \\frac{1}{2}\\) \\(\\text{Uniform}(a, b)\\) \\(\\frac{1}{2}(a+b)\\) \\(\\frac{1}{12}(b-a)^2\\) \\(\\begin{cases}\\frac{e^{tb}-e^{ta}}{t(b-a)} &amp; t \\neq 0 \\\\ 1 &amp; t = 0 \\\\\\end{cases}\\) \\(F(n, m)\\) \\(\\frac{m}{m - 2}\\) for \\(m &gt; 2\\) \\(\\frac{2m^2(n + m - 2)}{n(m - 2)^2(m - 4)}\\) for \\(m &gt; 4\\) Does Not Exist \\(\\text{HyperGeo}(N, K, n)\\) \\(\\frac{nK}{N}\\) \\(\\frac{nK(N-K)(N-n)}{N^2(N-1)}\\) Too complicated to reproduce here! 6.8.1 Bernoulli: Direct Summation The moments of a Bernoulli distribution are simple to compute, because \\(x\\) is only 0 or 1. When \\(x = 0\\), \\(0 * P(X = 0) = 0\\). Hence, \\(E(X) = 1 * P(X = 1) = p^1(1 - p)^{1 - 1} = p\\). Since \\(1^k = 1\\), a convenient property follows: \\[X \\sim \\text{Bernoulli}(p) \\implies E(X^k) = p\\] This also suggests that \\(E(X^k) = P(X = 1)\\), which can be exceptionally useful in many statistical problems, including finding UMVUEs. Similarly, we can compute the variance using the property \\(Var(X) = E(X^2) - E(X^2) = p - p^2 = p(1-p)\\). Finally, the moment generating function follows directly by noting \\[E(e^{tx}) = e^{t*0}p^0(1-p)^{1-0} + e^{t*1}p^1(1-p)^{1-1} = (1-p) + pe^t\\] 6.8.2 Uniform: Direct Integration Moments of distributions with simple pdfs bounded at both ends such as the Uniform can be solved directly by integrating their pdf. The Uniform has pdf \\(f_X(x) = \\frac{1}{b-a}\\), so its moments are as follows: \\[\\begin{align} E(X) = \\int_{a}^b \\frac{x}{b-a} dx = \\frac{x^2}{2(b-a)}\\Big|_{a}^b\\\\ = \\frac{b^2 - a^2}{2(b-a)} = \\frac{(b+a)(b-a)}{2(b-a)} = \\frac{1}{2}(b+a)\\\\ \\end{align}\\] Generalizing this, \\(E(X^k) = \\frac{b^k - a^k}{2(b-a)}\\). As a result, solving the variance is easier to do directly: \\[\\begin{align} Var(X) = \\int_{a}^b \\frac{(x - \\frac{1}{2}(b-a))^2}{b-a} dx \\\\ = \\frac{(x - \\frac{1}{2}(b-a))^3}{3(b-a)}\\Big|_{a}^b \\\\ = \\frac{(b - \\frac{1}{2}b - \\frac{1}{2}a)^3 - (a - \\frac{1}{2}b - \\frac{1}{2}a)^3}{3(b-a)}\\Big|_{a}^b\\\\ = \\frac{2(b-a)^3}{24(b-a)} = \\frac{1}{12}(b-a)^2\\\\ \\end{align}\\] 6.8.3 Geometric: Series Convergence Deriving the moments of the Geometric distribution requires the use of the Geometric Series (from where we can speculate its name originates). Note \\(E(X) = \\sum_{x=0}^\\infty xp(1-p)^x\\), which does not quite match the geometric series; first, we need to take the derivative, and then interchange differentiation and summation like so: \\[\\begin{align} \\sum_{x=0}^\\infty xp(1-p)^x = p(1-p)\\sum_{x=0}^\\infty x(1-p)^{x-1} &amp;&amp; \\text{factor out for correct form}\\\\ = p(1-p)\\sum_{x=0}^\\infty \\frac{d}{dx}-(1-p)^{x} &amp;&amp; \\text{notice derivative}\\\\ = -p(1-p) \\frac{d}{dx}\\sum_{x=0}^\\infty (1-p)^{x} &amp;&amp; \\text{interchange derivative}\\\\ = -p(1-p) \\frac{d}{dx}\\frac{1}{1 - (1 - p)} &amp;&amp; \\text{geometric series}\\\\ = \\frac{p(1-p)}{p^2} = \\frac{1-p}{p} &amp;&amp; \\\\ \\end{align}\\] This can also be performed to compute \\(E(X)\\) for the variance, though the computation is relatively long to be reproduced here. A quicker way might be to employ the moment generating function from which all moments can be computed, which can easily be found by substituting the geometric series: \\[\\begin{align} E(e^{tx}) = \\sum_{x=0}^\\infty e^{tx}(p(1-p)^x) \\\\ = p\\sum_{x=0}^\\infty ((1-p)e^t)^x) \\\\ = \\frac{p}{1 - (1-p)e^t} \\end{align}\\] 6.8.4 Binomial: Kernel Technique, Series Version Moments of the Binomial are trickier since they involve an infinite sum. Since the Binomial is a discrete distribution, we can compute its moments using the discrete moment formula \\(E(X) = \\sum_{i=1}^\\infty xP(X = x)\\). This introduces one technique for moment calculations: the Kernel Technique Example 6.1 (Kernel Technique with Infinite Series) Fact: pmfs integrate to 1. That is, \\(\\sum_{x=0}^\\infty f_X(x) = 1\\). We can use this fact to compute moments by: Recognizing the kernel of the distribution within the moment formula Factoring out appropriate constants to turn the kernel into the full pmf, and simplifying the infinite series to \\(\\sum_{x=0}^\\infty f_X(x) = 1\\). The left-over components then, are the value of the moment. Binomial Distribution: We can use this technique to compute moments of the Binomial distribution like so: \\[\\begin{align} E(X) = \\sum_{x=0}^\\infty xP(X = x) = \\sum_{x=0}^\\infty x{n\\choose x}p^x(1-p)^{n-x} &amp;&amp; \\\\ = \\sum_{x=0}^\\infty \\frac{x\\cdot n!}{x!(n-x)!}p^x(1-p)^{n-x} &amp;&amp; \\text{(kernel)} \\\\ = 0 + \\sum_{x=1}^\\infty \\frac{n \\cdot (n-1)!}{(x-1)!((n-1) - (x - 1)!}\\cdot p \\cdot p^{x-1}(1-p)^{(n - 1) - (x - 1)} &amp;&amp; \\text{(form pmf)}\\\\ = np\\sum_{x=0}^\\infty \\cdot {n - 1 \\choose x}p^x(1-p)^{(n-1) - x} &amp;&amp; \\text{(sum pmf to 1)}\\\\ = np \\end{align}\\] To compute the \\(E(X^2)\\) component of the variance, this process needs to be repeated twice: \\[\\begin{align} E(X^2) = \\sum_{x=0}^\\infty x^2P(X = x) = \\sum_{x=0}^\\infty x^2{n\\choose x}p^x(1-p)^{n-x} &amp;&amp; \\\\ = np \\sum_{x=0}^\\infty(x+1) \\frac{(n-1)!}{x!(n-1-x)!}p^x (1-p)^{n-x-1} &amp;&amp; \\text{(from E(X))}\\\\ = np(0 + (n-1)p\\sum_{x=1}^\\infty \\frac{(n-2)!}{(x-1)!((n-2)-(x-1))!}p^{x-1}(1-p)^{(n-2)-(x-1)} + 1 &amp;&amp; \\\\ = np((n-1)p + 1) = (np)^2 + np(1-p) &amp;&amp; \\text{(pmf sums to 1)}&amp;&amp; \\\\ \\end{align}\\] Then, \\(Var(X) = E(X^2) - E(X)^2 = (np)^2 + np(1-p) - (np)^2 = np(1-p)\\) Alternatively, we could have computed this using the fact that the Binomial is equal to a sum of Bernoulli random variables. By the linearity of expectation, if \\(X_i \\sim \\text{Bernoulli}(p)\\), then \\(E(\\sum_{i=1}^n X_i) = n\\cdot E(X_i) = np\\). \\(Var(X)\\) follows similarly. Even the MGF can be found easily this way - the MGF of a Bernoulli is \\(1 - p + pe^t\\), so by convolution \\(\\mathcal{M}_{X} = (1 - p + pe^t)^n\\) for \\(X\\sim\\text{Binomial(n,p)}\\). Multinomial Distribution: We can use the same technique to compute \\(Cov(X_i, X_j)\\) for a multivariate distribution as well. Suppose \\((X_1, ..., X_n)\\sim \\text{Multinomial}(m, p1_,...,p_n\\). Then, \\(Cov(X_i, X_j) = E(X_iX_j) - E(X_i)E(X_j)\\). Since the marginals are binomial, \\(E(X_i)E(X_j) = (mp_i)(mp_j)\\) based on the moments computed earlier. Then, \\[ E(X_iX_j) = \\sum_{x_i=0}^m\\sum_{x_j=0}^m x_ix_jf_{X_i, X_j}(x_i, x_j)\\\\ = \\sum_{x_i=0}^m\\sum_{x_j=0}^m\\frac{m!}{(x_i - 1)!(x_j - 1)!}p_i^{x_i}p_j^{x_j} \\\\ = m(m-1)p_ip_j\\sum_{x_i=0}^m\\sum_{x_j=0}^m\\frac{(m-2)!}{(x_i - 1)!(x_j - 1)!}p_i^{x_i-1}p_j^{x_j-1}\\\\ = m^2p_ip_j - mp_ip_j \\] by recognizing the \\(\\text{Multinomial}\\) kernel in the second-to-last line. Then, \\(Cov(X_i, X_j) = m^2p_ip_j - mp_ip_j - (mp_i)(mp_j) = -mp_ip_j\\), completing the proof. 6.8.5 Negative Binomial and Hypergeometric: Computing \\(E(X(X-1))\\) The second moment of distributions with pmfs/pdfs involving factorials can often be computing more easily by finding \\(E(X(X-1))\\) instead of \\(E(X^2)\\) directly. The Negative Binomial is one such distribution. Its mean can be computed by the same Kernel Technique used for the Binomial: \\[\\begin{align} E(X) = \\sum_{x=0}^\\infty x{x + r - 1 \\choose x} \\cdot (1-p)^x p^r &amp;&amp; \\\\ = (1-p)\\sum_{x=1}^\\infty \\frac{((x-1) + r)!}{(x-1)!(r-1)!}\\cdot (1-p)^{x-1}p^{r} &amp;&amp; \\\\ = \\frac{r(1-p)}{p}\\sum_{x=1}^\\infty \\frac{((x-1) + r)!}{(x-1)!(r-1)!}\\cdot (1-p)^{x-1}p^{r} &amp;&amp; \\\\ = \\frac{r(1-p)}{p}\\sum_{x=0}^\\infty \\frac{((x-1) + r)!}{(x-1)!r!}\\cdot (1-p)^{x-1}p^{r+1} &amp;&amp; \\\\ = \\frac{r(1-p)}{p} &amp;&amp; \\text{pmf sums to 1} \\end{align}\\] With \\(E(X)\\) known, we can now take advantage of the factorial to compute \\(E(X(X-1))\\), using the same technique of simplifying the combinatorial fraction and pulling out components to reform a pdf: \\[\\begin{align} E(X(X-1)) = \\sum_{x=0}^\\infty x(x-1){x + r - 1 \\choose x}(1-p)^x p^r \\\\ = \\frac{r(r+1)}{1-p)^2}{p^2}\\sum_{x=0}^\\infty \\frac{((x-2) + (r+1))!}{(x-2)!(r+1)!}(1-p)^{x-2}p^{r+2}\\\\ = \\frac{r(r+1)}{1-p)^2}{p^2}\\Big(0 + 0 + \\sum_{x=2}^\\infty {(x-2) + r + 1\\choose x-2}(1-p)^{x-2}p^{r+2}\\\\ = \\frac{r(r + 1)(1-p)^2}{p^2} \\text{ pmf sums to 1}\\\\ \\end{align}\\] Since, by linearity of expectation, \\(E(X(X-1)) = E(X^2) - E(X)\\), we can write \\[\\begin{align} Var(X) = E(X^2) - E(X) + E(X) - E(X)^2 \\\\ = E(X(X-1)) + E(X) - E(X)^2 \\\\ = \\frac{r(r+1)(1-p)^2}{p^2} + \\frac{r(1-p)}{p} - \\frac{r^2(1-p)^2}{p^2}\\\\ = \\frac{r(1-p)^2}{p^2} + \\frac{r(1-p)}{p} \\\\ = \\frac{r(1-p)(1 - p + p)}{p^2} = \\frac{r(1-p)}{p^2} \\end{align}\\] Note: The moments of a Negative Binomial can also be computed simply by relying on its additive property in relation to the geometric, and then using the linearity expectation. That is, if \\(Y \\sim \\text{NegBin}(r, p)\\), then \\(Y = \\sum_{i=1}^r X_i\\) where \\(X_i \\sim \\text{Geo}(p)\\). Then, \\(E(Y) = E(\\sum_{i=1}^r X_i) = \\frac{r(1-p)}{p}\\). Hypergeometric Distribution: The above technique can also be used to find the variance of a \\(\\text{HGeo}(N, K, n)\\) random variable. First, let us use the Kernel Technique to compute its first moment. Writing the combinatorial functions in their full form, noting in general \\(y - x = (y - 1) - (x - 1)\\), we can rewrite this as the pmf of a \\(\\text{HGeo}(N-1, K-1, n-1)\\): \\[\\begin{align} E(X) = \\sum_{x=0}^n x \\cdot \\frac{K!}{x!(K-x)}\\cdot\\frac{(N-K)!}{(n-x)!(N-K-n+x)!} \\cdot \\frac{n!(N-n)!}{N!}\\\\ = \\sum_{x=1}^n \\frac{K(K-1)!}{(x-1)!((K-1) - (x-1))!} \\\\ \\cdot \\frac{((N-1) - (K - 1))!}{((n-1) - (x-1))!((N-1) - (k-1) - (n-1) + (x-1))!}\\cdot\\frac{n(n-1)!((N-1) - (n-1))!}{N(N-1)!}\\\\ = \\frac{NK}{n}\\sum_{x=0}^{n-1} \\frac{{K-1\\choose x}{(N-1) - (K-1)\\choose (n-1) - x}}{{N-1\\choose n-1}}\\\\ = \\frac{NK}{n} \\end{align}\\] For the variance, we first compute \\(E(X(X-1))\\) in the same fashion. For brevity, we exclude writing down \\(y - x = (x - 2) - (y-2)\\) expansion, but the calculations below do rely on this principle. \\[\\begin{align} E(X(X-1)) = \\sum_{x=0}^n x(x-1) \\cdot \\frac{K!}{x!(K-x)}\\cdot\\frac{(N-K)!}{(n-x)!(N-K-n+x)!} \\cdot \\frac{n!(N-n)!}{N!}\\\\ = \\sum_{x=2}^n \\frac{K(K-1)!}{(x-1)!(K - X)!} \\\\ \\cdot \\frac{(N - K)!}{((n-2) - x-2))!(N - k - (n-2) + (x-2))!}\\cdot\\frac{n(n-1)(n-2)!((N-2) - (n-2))!}{N(N-1)(N-2)!}\\\\ = \\frac{N(N-1)K(K-1)}{n(n-1)}\\sum_{x=0}^{n-2} \\frac{{K-2\\choose x}{(N-2) - (K-2)\\choose (n-2) - x}}{{N-2\\choose n-2}}\\\\ = \\frac{N(N-1)K(K-1)}{n(n-1)} \\end{align}\\] Then, \\[\\begin{align} Var(X) = E(X(X-1)) + E(X) - E(X)^2\\\\ = \\frac{K(K-1)N(N-1)}{n(n-1)} + \\frac{KN}{n} - \\frac{KN}{n^2}\\\\ = n\\frac{K}{N} \\cdot \\frac{N - K}{N}\\cdot \\frac{N - n}{N-1} \\end{align}\\] after lengthy algebra. 6.8.6 Poisson: Exponential Taylor Series Like the Geometric, we can also derive the moments of the Poisson by relying the convergence of an infinite series. This time, we rely on the Taylor Series for the exponential distribution, which is \\(\\sum_{x=0}^\\infty \\frac{\\lambda^x}{x!} = e^\\lambda\\). Proceed as follows: \\[\\begin{align} E(X) = \\sum_{x=0}^\\infty x\\frac{e^{-\\lambda}\\lambda^x}{x!} &amp;&amp; \\\\ = \\lambda e^{-\\lambda}\\sum_{x=1}^\\infty \\frac{\\lambda^{x-1}}{(x-1)!} &amp;&amp; \\text{when }x = 0, \\text{ the series term is 0}\\\\ = \\lambda e^{-\\lambda}e^{\\lambda} = \\lambda &amp;&amp; \\text{exponential series} \\end{align}\\] The variance follows similarly: \\[\\begin{align} E(X^2) = \\sum_{x=0}^\\infty x^2\\frac{e^{-\\lambda}\\lambda^x}{x!} \\\\ = \\lambda e^{-\\lambda}\\sum_{x=1}^\\infty x\\frac{\\lambda^{x-1}}{(x-1)!} \\\\ = \\lambda e^{-\\lambda}\\sum_{x=1}^\\infty x\\frac{\\lambda^{x-1}}{(x-1)!} \\\\ = \\lambda e^{-\\lambda}\\sum_{x=0}^\\infty (x + 1)\\frac{\\lambda^{x}}{x!} \\\\ = e^{-\\lambda} \\cdot \\lambda \\cdot \\lambda \\cdot e^{\\lambda} + e^{-\\lambda} \\cdot \\lambda \\cdot e^{\\lambda} = \\lambda^2 + \\lambda \\\\ \\implies Var(X) = E(^2) - E(X)^2 = \\lambda^2 + \\lambda - \\lambda^2 = \\lambda \\end{align}\\] The MGF can also be found using an exponential Taylor series: \\[\\begin{align} \\mathcal{M}_X(t) = \\sum_{x=0}^\\infty e^{tx} \\cdot e^{-\\lambda} \\cdot \\frac{\\lambda^x}{x!} = e^{-\\lambda}\\sum_{x=0}^\\infty \\frac{(\\lambda e^{tx})^x}{x!} \\\\ = e^{-\\lambda}\\cdot e^{\\lambda e^{t}} = e^{\\lambda(e^t - 1)} \\end{align}\\] 6.8.7 Exponential: Integration By Parts Sometimes, a pdf can be integrated directly using more advanced integration techniques. The Exponential is one such distribution - its moments can be computed using integration by parts For the mean, note \\(E(X) = \\int_0^\\infty \\lambda e^{-\\lambda x}dx\\). Let \\(u = x \\implies du = 1\\) and \\(du = \\lambda e^{-\\lambda x} \\implies v = -e^{-\\lambda x}\\). Then, \\[\\begin{align} E(X) = uv - \\int_{0}^\\infty vdu = -xe^{-\\lambda x}\\Big|_{0}^\\infty + \\int_0^\\infty e^{-\\lambda x}dx\\\\ = 0 + 0 - \\frac{1}{\\lambda}e^{-\\lambda x} \\Big|_0^\\infty = \\frac{1}{\\lambda} \\end{align}\\] noting that \\(\\lim_{x\\rightarrow\\infty}xe^{-\\lambda x} = \\lim_{x\\rightarrow\\infty}-\\lambda e^{-\\lambda x} = 0\\) by applying LHopitals rule. Hence \\(E(X) = \\frac{1}{\\lambda}\\) For the variance, start by computing \\(E(X^2)\\). Applying integration by parts with \\(u = x^2 \\implies du = 2x\\) and \\(dv = \\lambda e^{-\\lambda x} \\implies -e^{-\\lambda x}\\), \\[E(X^2) = uv - \\int_{0}^\\infty vdu = -x^2e^{-\\lambda x} \\Big|_0^\\infty + \\int_{0}^\\infty 2xe^{-\\lambda x}\\] Now, we could apply integration by parts again, but a faster way to solve this is by moment recognization: noting that the second term can be transformed to equal \\(E(X)\\) like so: \\(\\int_{0}^\\infty 2xe^{-\\lambda x} = \\frac{2}{\\lambda}\\int_{0}^\\infty x\\lambda e^{-\\lambda x} = E(X) = \\lambda\\) as weve already solved. As before, the first term is \\(\\lim_{x\\rightarrow \\infty}-x^2e^{-\\lambda x} \\Big|_0^\\infty = \\lim_{x\\rightarrow\\infty}\\lambda^2e^{-\\lambda x} = 0\\) by applying LHopitals rule twice. Plugging in \\(E(X) = \\frac{1}{\\lambda}\\), we get \\(E(X^2)\\) = \\(\\frac{2}{\\lambda^2}\\) and \\[Var(X) = E(X^2) - E(X)^2 = \\frac{2}{\\lambda^2} - \\frac{1}{\\lambda^2} = \\frac{1}{\\lambda^2}\\] 6.8.8 Gamma and Beta: Kernel Technique, Integration Version Similar to discrete distributions, we can also use the Kernel Technique to more easily integrate continuous distributions as well. For example, noting Gamma function property $(k) = , \\[\\begin{align} E(X) = \\int_{0}^\\infty \\frac{x}{\\Gamma(k)\\lambda^k}x^{k-1}e^{-\\frac{1}{\\lambda}x}dx &amp;&amp;\\\\ = \\int_{0}^\\infty \\frac{\\lambda}{(\\Gamma(k+1) / k)\\lambda^{k+1}}x^k e^{-\\frac{1}{\\lambda}x}dx &amp;&amp; \\text{recognize Gamma kernel}\\\\ = \\lambda k \\int_{0}^\\infty \\frac{1}{\\Gamma(k+1)\\lambda^{k+1}}x^k e^{-\\frac{1}{\\lambda}x}dx &amp;&amp; \\text{pull out excess terms}\\\\ = \\lambda k &amp;&amp; \\text{ integrate } Gamma(k+1, \\lambda) \\text{ to 1} \\end{align}\\] Similarly, the variance of the Gamma can be calculated like so: Solve for \\(E(X^2)\\) \\[\\begin{align} E(X^2) = \\int_{0}^\\infty \\frac{x^2}{\\Gamma(k)\\lambda^k}x^{k-1}e^{-\\frac{1}{\\lambda}x}dx = \\int_{0}^\\infty \\frac{\\lambda^2}{(\\Gamma(k+2) / (k(k+1)))\\lambda^{k+2}}x^{k+1} e^{-\\frac{1}{\\lambda}x}dx\\\\ = \\lambda^2k(k+1) \\text{ Pull out excess terms from } Gamma(k+1, \\lambda) \\text{ kernel, integrate to 1} \\end{align}\\] Compute variance using \\(Var(X) = E(X^2) - E(X)^2\\) using the components solved previously. \\[Var(X) = \\lambda^2k(k+1) - \\lambda^2k^2 = \\lambda^2k\\] Since the Beta distribution also involves Gamma function, we can compute the moments in a similar fashion. Lets start with the mean: \\[\\begin{align} E(X) = \\int_{0}^\\infty x \\cdot\\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)}x^{\\alpha-1}(1-x)^{\\beta-1}dx &amp;&amp; \\\\ = \\frac{\\alpha}{\\alpha + \\beta}\\int_{0}^{1} \\frac{\\Gamma(\\alpha + 1 + \\beta)}{\\Gamma(\\alpha + 1)\\Gamma(\\beta)}x^\\alpha (1-x)^{\\beta-1}dx &amp;&amp; \\text{form kernel of } Beta(\\alpha+1, \\beta)\\\\ = \\frac{\\alpha}{\\alpha + \\beta} &amp;&amp; \\text{ integrate Beta pdf to 1}\\\\ \\end{align}\\] Now, we apply this same principle twice to compute the variance: Compute \\(E(X^2)\\) by using \\(\\Gamma(\\alpha) = \\alpha(\\alpha+1)\\Gamma(\\alpha+2)\\) and \\(\\Gamma(\\alpha + \\beta) = (\\alpha + \\beta)(\\alpha + \\beta + 2)\\) to form the Beta kernel: \\[\\begin{align} E(X^2) = \\int_{0}^1 x^2 \\cdot \\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)}x^{\\alpha-1}(1 - x)^{\\beta-1}dx &amp;&amp; \\\\ = \\frac{\\alpha(\\alpha+1)}{(\\alpha + \\beta)(\\alpha + \\beta + 1)}\\int_{0}^1x^{\\alpha-1}(1-x)^{\\beta-1}dx &amp;&amp; \\text{form kernel of } Beta(\\alpha + 2, \\beta)\\\\ = \\frac{\\alpha(\\alpha+1)}{(\\alpha + \\beta)(\\alpha + \\beta + 1)} \\text{integrate Beta pdf to 1} \\end{align}\\] Compute the variance using \\(Var(X) = E(X^2) - E(X)^2\\) \\[\\begin{align} Var(X) = E(X^2)\\frac{\\alpha^2 + \\alpha}{(\\alpha + \\beta)(\\alpha + \\beta + 1)} - \\frac{\\alpha^2}{(\\alpha + \\beta)^2}\\\\ = \\frac{\\alpha^2(\\alpha + \\beta) + \\alpha(\\alpha + \\beta) - \\alpha^2(\\alpha + \\beta + 1)}{(\\alpha + \\beta)^2(\\alpha + \\beta + 1)}\\\\ = \\frac{\\alpha\\beta}{(\\alpha + \\beta)^2(\\alpha + \\beta + 1)} \\end{align}\\] Virtually identical steps are used to compute the moments of the Chi-squared (which is a special case of the Gamma) and F distributions as well. 6.8.9 Normal: Location-Scale Trick and Polar Integration Proving that the moments of the \\(\\text{Normal}(\\mu, \\sigma^2)\\) can be a bit tricky. However, if we use the fact that it is a location-scale family, it becomes much easier. Letting \\(X = \\sigma Z + \\mu\\), then \\[E(X) = \\sigma E(Z) + \\mu\\] by linearity of expectation. Since \\(Z \\sim N(0,1)\\), noting that the antiderivative of \\(z\\exp(-\\frac{1}{2}z^2)\\) is \\(-\\exp(-\\frac{1}{2}z^2)\\), we get \\[\\begin{align} E(Z) = \\int_{\\infty}^\\infty \\frac{z}{\\sqrt{2\\pi}}\\exp(-\\frac{1}{2}z^2)dx\\\\ = -\\exp(-\\frac{1}{2}z^2)\\Big|_{-\\infty}^\\infty = 0 \\end{align}\\] since \\(\\lim_{z^2\\rightarrow\\infty} \\exp(-\\frac{1}{2}z^2) = 0\\). Therefore, \\(E(X) = E(\\sigma Z + \\mu) = \\sigma E(Z) +\\mu = \\mu\\). Computing the variance necessitates a more advanced integration technique: Polar Coordinates. Proceed with Integration by Parts, letting \\(u = \\frac{z}{\\sqrt{2\\pi}} \\implies du = \\frac{1}{\\sqrt{2\\pi}}\\) and \\(dv = z\\exp(-\\frac{1}{2}z^2)\\implies v = -\\exp(-\\frac{1}{2}z^2)\\) as used previously. Then, \\[\\begin{align} E(Z^2) = \\int_{\\infty}^\\infty \\frac{z^2}{\\sqrt{2\\pi}}\\exp(-\\frac{1}{2}z^2)dx\\\\ = \\frac{z}{\\sqrt{2\\pi}}\\exp(-\\frac{1}{2}z^2)\\Big|_{-\\infty}^\\infty + \\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^\\infty \\exp(-\\frac{1}{2}z^2)dz \\end{align}\\] By LHopitals rule, \\[\\lim_{z\\rightarrow\\infty} \\frac{z}{\\sqrt{2\\pi}}\\exp(-\\frac{1}{2}z^2) = \\lim_{z\\rightarrow\\infty}\\frac{1}{\\sqrt{2\\pi}z\\exp(\\frac{1}{2}z^2)} = 0\\] so \\(\\frac{z}{\\sqrt{2\\pi}}\\exp(\\frac{1}{2}z^2)\\Big|_{-\\infty}^\\infty = 0\\). But how do we solve the second integral? This is where polar coordinates come into play. From Strang (2010), if \\(A = \\int_{-\\infty}^\\infty \\exp(-\\frac{1}{2}x^2)dx\\), then we can solve the integral by converting into polar like so: \\[\\begin{align} A^2 = \\int_{-\\infty}^\\infty \\exp(-\\frac{1}{2}x^2)dx \\cdot \\int_{-\\infty}^\\infty \\exp(-\\frac{1}{2}xy^2)dy\\\\ = \\int_{-\\infty}^\\infty\\int_{-\\infty}^\\infty \\exp(-\\frac{1}{2}(x^2 + y^2))dxdy \\\\ = \\int_{0}^{2\\pi}\\int_{0}^\\infty r\\exp(-\\frac{1}{2}r^2(\\cos^2(\\theta) + \\sin^2(\\theta)))drd\\theta\\\\ = \\int_{0}^{2\\pi}\\int_{0}^\\infty r\\exp(-\\frac{1}{2}r^2)drd\\theta\\\\ = 2\\pi \\implies A = \\sqrt{2\\pi} \\end{align}\\] Hence, \\(E(Z^2) = 0 + \\frac{\\sqrt{2\\pi}}{\\sqrt{2\\pi}} = 1\\) and therefore, \\[\\begin{align} Var(X) = E((\\sigma Z + \\mu - E(Z))^2) \\\\ = E((\\sigma Z + \\mu - \\mu)^2) = \\sigma^2E(Z^2) \\\\ = \\sigma^2 \\end{align}\\] Hence, we have proven that, for \\(X \\sim \\text{Normal}(\\mu, \\sigma^2)\\), that the mean is \\(\\mu\\) and variance is \\(\\sigma^2\\) - as we expected (ba-dum tss). 6.9 Other Moments (for reference) Distribution \\(E(Y)\\) \\(Var(Y)\\) mgf \\(\\text{Weibull}(k, \\lambda)\\) \\(\\lambda\\Gamma(1 + \\frac{1}{k})\\) \\(\\lambda^2\\Big(\\Gamma(1 + \\frac{2}{k}) - (\\Gamma(1 + \\frac{1}{k}))^2\\Big)\\) \\(\\sum_{n=0}^\\infty\\frac{t^n \\lambda^n}{n!}\\Gamma(1 + \\frac{n}{k})\\), \\(k \\geq 1\\) \\(\\text{Pareto}(x_m, \\alpha)\\) \\(\\begin{cases}\\infty &amp; \\alpha \\leq 1 \\\\ \\frac{\\alpha x_m}{a - 1} &amp; \\alpha &gt; 1\\end{cases}\\) \\(\\begin{cases}\\infty &amp; \\alpha \\leq 2 \\\\ \\frac{x_m^2 \\alpha}{(a-1)^2(\\alpha - 2)} &amp; \\alpha &gt; 2\\end{cases}\\) Does not exist \\(\\text{Cauchy}(x_0, \\gamma)\\) Does Not Exist Does Not Exist \\(\\exp(x_0it - \\gamma|t|\\) (cf) \\(t(\\nu)\\) 0 \\(\\begin{cases}\\frac{\\nu}{\\nu-2} &amp; \\nu &gt; 2\\\\ \\infty &amp; 1 &lt; \\nu \\leq 2 \\\\ \\text{undefined} &amp; \\text{otherwise}\\end{cases}\\) Does Not Exist References "],["statistics.html", "Chapter 7 Statistics 7.1 Sufficient Statistics 7.2 Minimal Sufficiency 7.3 Ancillary Statistics 7.4 Complete Statistics", " Chapter 7 Statistics One of the most important tasks we will be performing as statisticians is inference. Inference is the area of statistics which uses data from a random sample to estimate a population parameter. Definition 7.1 (Random Sample) \\(X = (X_1, X_2, ... X_n)\\) constitutes a random sample of size \\(n\\) provided each \\(X_i\\) is mutually independent. The term iid stands for independently and identically distributed. A random sample is iid if all \\(X_i\\) are independent and follows the same distribution. In this case, \\[f_X(x_1, ..., x_n) = f(x_1)\\cdot f(x_2)\\cdot,..., \\cdot f(x_n) = \\prod_{i=1}^nf(x_i)\\] To perform inference, it is usually necessary to reduce or summarize the recollected data into some measurement that we will refer to as statistic. Definition 7.2 (Statistic) Let \\(X \\sim f(x|\\theta)\\), where both \\(X, \\theta\\) can be vectors. A statistic is a function \\(T=T(\\textbf{X})\\) of the sample \\(\\textbf{X}\\) from \\(X\\). For example, the \\(T= T(\\textbf{X}) = \\frac{1}{n} \\sum_{i=1}^{n} X_i\\) reduces the sample \\(\\textbf{X}\\) to a single measurement. In general, we would like to choose statistics that satisfy principles that will make inference about the population parameter easier. One of these principles is sufficiency. The idea behind sufficiency is to retain information about the population parameter, say \\(\\theta\\), while reducing the data. 7.1 Sufficient Statistics Sufficiency Principle. If \\(T\\) is sufficient, then any information about the parameter \\(\\theta\\) should depend on \\(X\\) only through \\(T\\). Definition 7.3 (Sufficient Statistic) Let \\(X \\sim f(x|\\theta)\\). We say \\(T=T(X)\\) is a sufficient statistic (SS) for \\(\\theta\\) if \\(\\mathcal{L}(\\theta|X)\\) is independent of \\(\\theta\\), i.e. \\(f(x|T;\\theta) = g(x)\\). 7.1.1 Techniques for Finding Sufficient Statistics There are several ways to find a sufficient statistic - this section will outline several methods. Firstly, if you are given a statistic and you wish to check whether it is sufficient, you can do so directly. ::: {.example name=Checking Whether a Sample Mean is Sufficient} Let \\(X = (X_1, ..., X_n), X_i\\) iid N(\\(\\theta\\), 1) and \\(T = \\bar{X}\\). Recall that \\(\\bar{X} \\sim N(\\theta, 1/n)\\). \\[ \\begin{aligned} f(x|T, \\theta) = n^{-1/2}(2\\pi)^{-(\\frac{n-1}{2})} exp\\{-\\sum_{i=1}^{n}(x_i - \\bar{x})^2\\} \\end{aligned} \\] Observe that the conditional density is independent of \\(\\theta\\). By definition, \\(T = \\bar{X}\\) is a sufficient statistic for \\(\\theta\\). ::: As you can see, given a particular statistic, you can check whether it is sufficient directly. However, this requires knowing its distribution. If you are given the joint distribution of random variables (or can compute it) involved in the statistic, the definition of sufficiency can be checked. Example 7.1 (Finding a Sufficient Statistic Directly) Let \\(X_1, X_2\\) iid Poisson(\\(\\lambda\\)). The joint distribution of \\(X_1, X_2\\) is \\(P(X_1=x_1, X_2 = x_2) = \\frac{\\lambda^{x_1+x_2}exp(-2\\lambda)}{x_1! x_2!}\\). Let \\(T(X_1, X_2) = X_1+X_2\\). Observe that \\(X_1+X_2 \\sim\\) Poisson(2\\(\\lambda\\)). Thus, \\[ \\begin{aligned} P(X_1=x_1, X_2 = x_2|X_1+X_2=t) &amp;= \\frac{P(X_1=x_1, X_2 = t - x_1)}{P(X_1 + X_2 = t)} \\\\ &amp;=\\frac{exp(-\\lambda)\\lambda^{x_1} exp(-\\lambda)\\lambda^{t -x_1}t!}{x_1!(t-x_1)! exp(-2\\lambda)(2\\lambda)^t}\\\\ &amp;= { t \\choose x_1} \\bigg(\\frac{1}{2}\\bigg)^t \\end{aligned} \\] which is independent of \\(\\lambda\\) and \\(X_1+X_2\\) is sufficient for \\(\\lambda\\). As can be seen from the example, we need a candidate statistic to prove sufficiency using the definition. Furthermore, checking sufficiency of a statistics is difficult because we need to compute the conditional distribution. Even if you are not given a statistic, a sufficient statistic can still be identified using the factorization theorem: Theorem 7.1 (Factorization Theorem) \\(T(X)\\) is sufficient for \\(\\theta \\Longleftrightarrow \\exists g(t|\\theta)\\) and \\(h(x)\\), such that \\[f(x|\\theta)=g(t|\\theta)h(x)\\] \\(\\forall x, \\theta\\). Note that the factorization theorem tells us that if we can manipulate \\(f(x|\\theta)\\) as above, we have a sufficient statistic. Here is an example. ::: {.example name=Using the Factorization Theorem} Let \\(\\textbf{X}= (X_1, ..., X_n), X_i\\) iid \\(Poisson(\\lambda)\\). \\[ \\begin{aligned} P(X_1=x_1,..., X_n = x_n) &amp;= \\frac{exp(-n\\lambda)\\lambda^{\\sum x_i}}{\\prod x_i!}\\\\ &amp;= h(x)g\\Big(\\sum x_i|\\lambda\\Big) \\end{aligned} \\] \\(\\Rightarrow T(X) = \\sum X_i\\) sufficient for \\(\\lambda\\). ::: Some distributions involve indicator functions. If an indicator function involves both \\(X_i\\) and \\(\\theta\\), such as \\(I(X_i &lt; \\theta)\\), chances are that an order statistic like \\(X_{(1)}\\) (the minimum) or \\(X_{(n)}\\) (the maximum) will be involved in the sufficient statistic. Why is this? Take \\(I(X_i &lt; \\theta)\\) as an example. For iid \\(X_i\\), \\(\\prod_{i=1}^nI(X_i &lt; \\theta) = I(X_{(n)} &lt; \\theta)\\). For this product to be nonzero, every \\(X_i\\) must be less than \\(\\theta\\), which is equivalent to saying that the largest value (\\(X_{(n)}\\)) is less than \\(\\theta\\). This trick is useful for finding sufficient statistics. Heres an example involving the Uniform distribution. ::: {.example name=Indicator Functions in the Factorization Theorem} Let \\(\\textbf{X}= (X_1, ..., X_n), X_i\\) iid \\(Uniform(0,\\theta)\\). Then, \\[ \\begin{aligned} P_{\\theta}(x_1, ..., x_n) &amp;= \\frac{1}{\\theta^n} \\prod_{i=1}^{n}I(0&lt;x_i&lt;\\theta)\\\\ &amp; = \\frac{1}{\\theta^n} I(x_{(1)}&gt;0) I(x_{(n)}&lt;\\theta) \\end{aligned} \\] \\(\\Rightarrow T(X) = X_{(n)}\\) sufficient for \\(\\theta\\). ::: 7.1.1.1 Important facts about SS Sufficient statistics may or may not reduce the data. Original data are always sufficient. In iid sample, the order statistics are sufficient. Sufficient statistics are never unique. Suppose \\(X \\sim N(0,\\sigma^2)\\). Then by the factorization theorem \\(T(X)=X^2, |X|, X^4, exp(X^2)\\) are all sufficient. Any 1-1 function \\(g\\) of a sufficient statistic is also sufficient. Proof. Let \\(T^*(X)= g(T(X))\\). By assumption \\(g^{-1}\\) exists since g is 1-1. \\[ \\begin{aligned} f(x|\\theta) &amp;= g(T(X)|\\theta)\\\\ &amp;=g(r^{-1}(T^*(x))|\\theta)h(x)\\\\ \\end{aligned} \\] By the factorization theorem, \\(T^*(X)\\) is sufficient for \\(\\theta\\). 7.1.2 Exponential Family Sufficient Statistics If \\(X_i\\) is an exponential family, we automatically know its sufficient statistic. Consider a sample \\(X=(X_1,...,X_n)\\) from \\(f_X(x|\\theta)= h(x)c(\\theta)\\exp\\Big(\\sum_{i=1}^k w_i(\\theta)t_i(x)\\Big)\\). Then \\(T(X) = \\Big(\\sum_{i=1}^n t_1(x), ..., \\sum_{i=1}^n t_k(x)\\Big)\\) is a sufficient statistic. Note that this result follows directly from the factorization theorem. Problem-Solving Tip: Whenever possible, use exponential families to quickly find sufficient statistics! It is often easier to prove that a distribution is an exponential family than to otherwise find a sufficient statistic. In addition, showing a distribution is an exponential provides many other useful results. 7.1.3 A Note on Distributions of Sufficient Statistics Recall a convenient property of exponential families: that their maximum likelihood estimate is a function of their sufficient statistic. Because of this, in order to prove Finite Sample Properties of an estimator or construct Hypothesis Tests, it is often useful to understand their distributions. This is why the distributions of each of the sufficient statistics are included in the fourth column of the table below. These distributions are mostly derived from additive, location-scale, and other properties in Chapter 4 - Known Distributions 7.1.4 Moments of the Sufficient Statistic As stated by Casella and Berger (1990), if \\(X\\) is an exponential family, the moments of its exponential family can be easily computed using certain properties. Theorem 7.2 (I Don't Know What To Call This) If \\(X\\) is an exponential family, then \\[E\\Big(\\sum_{i=1}^k \\frac{\\partial w_i(\\theta)}{\\partial \\theta_j}t_i(X)\\Big) = -\\frac{\\partial}{\\partial\\theta_j} \\log c(\\theta)\\] and \\[Var\\Big(\\sum_{i=1}^k \\frac{\\partial w_i(\\theta)}{\\partial \\theta_j}t_i(X)\\Big) = -\\frac{\\partial}{\\partial\\theta_j} \\log c(\\theta) - E\\Big(\\sum_{i=1}^k \\frac{\\partial^2w_i(\\theta)}{\\partial \\theta_j^2}t_i(X)\\Big)\\] If \\(X\\) is a natural exponential family, then these identities simplify even further. Theorem 7.3 (I Don't Know What To Call This 2) If \\(X\\) is a natural exponential family, then$ \\[E(t_j(X)) = -\\frac{\\partial}{\\partial\\eta_j}\\log(c^*(\\eta))\\] and \\[Var(t_j(X)) = -\\frac{\\partial^2}{\\partial\\eta_j^2}\\log(c^*(\\eta))\\] Using these theorems, we can calculate the moments of the sufficient statistics of exponential families directly. This is because if \\(T(X)\\) is a sufficient statistic, then \\(T(X) = \\Big(\\sum_{i=1}^nt_1(X), ..., \\sum_{i=1}^nt_k(X)\\Big)\\). Suppose, for simplicity, that \\(T(X)\\) is one-dimensional and \\(X_i\\) are iid. Then, if \\(X_i\\) is a natural exponential family, \\[E(T(X)) = E\\Big(\\sum_{i=1}^nt(X_i)\\Big) = nE(t(X_i)) = n\\cdot-\\frac{\\partial}{\\partial \\eta}\\log(c^*(\\eta))\\] In fact, letting \\(A(\\eta) = -\\log(c^*(\\eta))\\), we can obtain all of the moments of \\(T(X)\\) by simply differentiating \\(A(\\eta)\\) 7.1.5 Table of Sufficient Statistics Translated from https://en.wikipedia.org/wiki/Exponential_family Distribution Parameter Sufficient Statistic S.S. Distribution Bernoulli p \\(\\sum_{i=1}^n x_i\\) \\(Binomial(n, p)\\) Binomial p \\(\\sum_{i=1}^n x_i\\) \\(Binomial(nm, p)\\) Poisson \\(\\lambda\\) \\(\\sum_{i=1}^n x_i\\) \\(Poisson(n\\lambda)\\) Negative Binomial p \\(\\sum_{i=1}^n x_i\\) Exponential \\(\\lambda\\) \\(\\sum_{i=1}^n x_i\\) \\(Gamma(n, \\lambda)\\) Normal (known \\(\\sigma^2\\)) \\(\\mu\\) \\(\\frac{1}{n}\\sum_{i=1}^n x_i\\) \\(Normal(\\mu, \\sigma^2/n)\\) Normal \\(\\mu\\), \\(\\sigma^2\\) \\((\\sum_{i=1}^n x_i, \\sum_{i=1}^n x_i^2)\\) Chi-Squared \\(\\nu\\) \\(\\sum_{i=1}^n \\log(x_i)\\) Pareto (known min \\(x_m\\)) \\(\\alpha\\) \\(\\sum_{i=1}^n \\log(x_i)\\) Gamma \\(\\alpha, \\beta\\) \\((\\sum_{i=1}^n\\log(x_i), \\sum_{i=1}^n x_i)\\) Beta \\(\\alpha, \\beta\\) \\((\\sum_{i=1}^n\\log(x_i), \\sum_{i=1}^n\\log(1 - x_i))\\) Weibull (known shape \\(k\\)) \\(\\lambda\\) \\(\\sum_{i=1}^n x^k\\) 7.2 Minimal Sufficiency In any setting there are many sufficient statistics. However, we should aim at dealing with the statistic that summarizes the data as concisely as possible. Let \\(S\\) be any a sufficient statistic for \\(\\theta\\). In principle, \\(W=(S,T)\\) is also a sufficient statistic, but we rather deal with \\(S\\) reduces the data to one-dimension. When no further reduction from a sufficient statistic is possible, then that statistic is minimal sufficient. Definition 7.4 (Minimal Sufficient Statistic) If \\(T\\) is sufficient for \\(\\theta\\), then it is a minimal sufficient statistic (MSS) if for any other sufficient statistic \\(T^*, T\\) is a function of \\(T^*\\). Remark: Of all sufficient statistics, a minimal sufficient statistic offers the maximal reduction of the data. Some intuition behind this definition goes as follows. A minimal sufficient statistic \\(T(X)\\) creates a partition of the sample space, \\(\\Omega\\) into sets \\(A_t\\), where \\(t \\in \\mathcal(T) = \\{t : t = T(x) \\text{ for some } x\\in \\Omega\\}\\). Now consider another sufficient statistic \\(T^*(X)\\) that creates another partition of the sample space such that \\(A^*_s = \\{x: T^*(x)=s\\}\\). Then for ever \\(s\\) there is a \\(t\\) such that \\(A^*_s \\subset A_t\\). Thus the partition associated with the minimal sufficient statistic is the coarsest possible partition for a sufficient statistic. As before, the definition for MSS is conceptually useful but it does not help us find a MSS, or how to prove a statistic is a MSS. For this task, we invoke Casella-Bergers Theorem 6.2.13. Theorem 7.4 (Casella-Berger's Theorem 6.2.13) Let \\(X \\sim f(x|\\theta)\\). Suppose the exists a statistic \\(T=T(X)\\) such that for every \\(x,y\\) in the support of \\(X\\) \\[(*) \\frac{f(x|\\theta)}{f(y|\\theta)} = g(x,y) \\Longleftrightarrow T(x)=T(y)\\] then \\(T\\) is a MSS. In practice this theorem is used both to find a MSS and to prove that a given statistic is a MSS. Let us see some examples of applications of this theorem. Example 7.2 Let \\(\\textbf{X} = (X_1,..., X_n), X_i\\) iid \\(N(\\mu, \\sigma^2), \\theta = (\\mu, \\sigma^2)\\). Consider \\(T=(\\bar{X}, S^2)\\) where \\(S^2\\) is the sample variance. Let \\(\\textbf{x} = (x_1, ..., x_n), \\textbf{y} = (y_1, ..., y_n)\\). Then, \\[\\frac{f(\\textbf{x}|\\theta)}{f(\\textbf{y}|\\theta)} = ... = exp\\{-\\frac{1}{2\\sigma^2}[n(\\bar{x}-\\bar{y})^2 + 2 n\\mu(\\bar{x}-\\bar{y}) -(n-1)(s^2_x - s^2_y)]\\}.\\] \\(\\Rightarrow\\) Suppose the ratio is independent of \\(\\theta = (\\mu, \\sigma^2)\\). Then we must have that \\(\\bar{x} = \\bar{y}, s^2_x = s^2_y\\). \\(\\Leftarrow\\) Suppose that \\(T(x)=T(y)\\). Then \\(\\bar{x} = \\bar{y}, s^2_x = s^2_y\\) and the ratio is 1. By the previous theorem we have that \\(T\\) is a MSS. Example 7.3 Let \\(\\textbf{X} = (X_1,..., X_n), X_i\\) iid \\(U(\\theta, \\theta + 1)\\). \\[f(\\textbf{x}|\\theta) = \\frac{1}{2^n} I(X_{(1)} &gt; \\theta) I(X_{(n)} &lt; \\theta + 1)\\] Then, \\[\\frac{f(\\textbf{x}|\\theta)}{f(\\textbf{y}|\\theta)} = \\frac{I(x_{(1)} &gt; \\theta ) I(x_{(n)} &lt; \\theta + 1)}{I(y_{(1)} &gt; \\theta) I(y_{(n)} &lt; \\theta + 1)}\\] and the ratio is independent of \\(\\theta\\) if and only if \\((x_{(1)}, x_{(n)}) = (y_{(1)}, y_{(n)})\\). Thus \\(T = (X_{(1)}, X_{(n)})\\) is MSS. Note that in the previous example, \\(dim(T(X))&gt;dim(\\theta)\\). This means that there is no estimator of \\(\\theta\\) that is sufficient. Theorem 7.5 If \\(X_1,..., X_n (n\\ge 1)\\) are iid with \\(X_i \\sim k\\)-parameter exponential family, then \\(T(X) = \\Big(\\sum_{i=1}^n t_1(x), ..., \\sum_{i=1}^n t_k(x)\\Big)\\) is a MSS. Fact: Like sufficient statistics, MSSs are not unique. Any 1-1 function of a MSS is also a MSS. 7.3 Ancillary Statistics Sufficiency describes where all the information in the data is contained. Ancillarity is the dual of sufficiency, describing where there is no information. Definition 7.5 (Ancillary Statistic) Let \\(X \\sim f(x|\\theta)\\). The statistic \\(S(X)\\) is ancillary for \\(\\theta\\) if \\(\\mathcal{L}(S), g(s|\\theta)\\) are independent of \\(\\theta\\), i.e. for any \\(\\theta_1, \\theta_2 \\in \\Theta\\), \\[g(s|\\theta_1)=g(s|\\theta_2) \\text{ for all } s.\\] Example 7.4 Let \\(\\textbf{X} = (X_1,..., X_n), X_i\\) iid \\(U(\\theta, \\theta + 1)\\). Consider the order statistics of the sample \\(\\textbf{X}\\). The range statistic \\(R = X_{(n)} - X_{(1)}\\) is ancillary for \\(\\theta\\) by showing that the pdf of \\(R\\) is independent of \\(\\theta\\). Intuitive, the range does not tell anything about the location of \\(\\theta\\) in the real line. In this case the ancillarity of \\(R\\) does not depend on the uniformity of the observations, but on the parameter of the distribution being a location parameter. Theorem 7.6 (Location family ancillary statistic) Let \\(\\textbf{X} = (X_1,..., X_n), X_i\\) iid \\(f_{\\mathcal{L}}(x|\\theta)\\), a location family, then \\(R = X_{(n)} - X_{(1)}\\) is ancillary for \\(\\theta\\). Theorem 7.7 (Scale family ancillary statistic) Let \\(\\textbf{X} = (X_1,..., X_n), X_i\\) iid \\(f_{\\mathcal{S}}(x|\\sigma)\\), a scale family, and let \\(S_j = X_j/X_n\\). Then \\(S=(S_1, S_2,..., S_n)\\) is ancillary for \\(\\sigma\\). Note: Since \\(S\\) is ancillary, any function of \\(S=(S_1, S_2,..., S_n)\\) is also ancillary. For example, \\(S_1 + ... + S_n\\) is also ancillary. Theorem 7.8 (Location-Scale family ancillary statistic) Let \\(\\textbf{X} = (X_1,..., X_n), X_i\\) iid \\(f_{\\mathcal{L,S}}(x|\\mu,\\sigma)\\), a location-scale family where \\(\\theta = (\\mu, \\sigma)\\). Let \\(T_1= T_1(\\textbf{X})\\) and \\(T_2= T_2(\\textbf{X})\\) be any two statistics such that \\[ (*) \\quad T_j(aX_1 + b, ..., aX_n+b) = aT_j(X_1,..., X_n) \\quad j = 1,2.\\] Then, \\(T_1/T_2\\) is ancillary for \\(\\theta\\) Example 7.5 Consider the range \\(R = X_{(n)} - X_{(1)}\\) and \\(S = \\sqrt{\\frac{1}{n-1}\\sum(X_1 -\\bar{X})^2}\\) satisfy (*) so \\(R/S\\) is ancillary for \\(\\theta = (\\mu, \\sigma)\\). 7.3.1 Why are we interested in ancillary statistics? There is some relationship between ancillarity and minimal sufficiency. Suppose there is a statistic \\(c=C(\\textbf{X})\\) and an ancillary statistic \\(S=S(\\textbf{X})\\) such that \\(T=(S,C)\\) is minimal sufficient. The ancillarity principle states that inference on the parameter should be based on the conditional distribution of \\(C\\) given the ancillary statistic \\(S\\). Furthermore, we will see later that ancillary statistics are independent of complete and sufficient statistics. Remark: Recall the \\(U(\\theta, \\theta + 1)\\) example in which we found that the MSS, T, is two dimensional, and therefore no sufficient estimator for \\(\\theta\\) exists. We can write \\(T = (S,C)\\) where \\(C\\) has a marginal distribution that is ancillary (independent of the parameter), and then \\(S\\) is conditionally sufficient, i.e. sufficient conditional on \\(C\\). Example 7.6 Let \\(N\\) be a random variable with known distribution, \\(P(N=n) = p_n\\), and let \\(X_1, ..., X_N\\) be iid with exponential family density. Then the likelihood of the data, \\((N, X_1,..., X_N)\\) is \\[p_n (\\prod_{i=1}^{n}h(x_i))c(\\theta)^{n} exp\\bigg\\{\\sum_{j=1}^{k} w_k(\\theta) \\sum_{i=1}^{n}t_j(x_i)\\bigg\\} \\] and thus \\(\\bigg\\{N, \\{\\sum_{i=1}^{N}t_j(x_i)\\}_{j=1}^{k}\\bigg\\}\\) is sufficient for \\(\\theta\\). Observe that \\(N\\) is ancillary (independent of \\(\\theta\\)) and \\(\\{\\sum_{i=1}^{N}t_j(x_i)\\}_{j=1}^{k}\\) is sufficient for \\(\\theta\\) conditional on \\(N\\). Remark: Ancillary statistics may not be unique, and there is no general method for constructing them. Example 7.7 (Bivariate normal with unknown correlation) Suppose that \\((X_1, Y_1), ..., (X_n, Y_n)\\) are iid bivariate normal with zero means and unit variances, and unknown correlations \\(\\rho\\). Thus their joint pdf is \\[\\frac{1}{(2\\pi)^{n}(1-\\rho^2)^{n/2}} exp \\bigg\\{ -\\frac{\\sum(x_i^2+y_i^2)}{2(1-\\rho^2)} + \\frac{\\rho \\sum x_iy_i}{(1-\\rho^2)}\\bigg\\}.\\] Since this is an exponential family distribution,it is easy to see that the MSS is \\(T(X,Y) = (\\sum(x_i^2+y_i^2), \\sum x_iy_i )\\). Observe that there is no ancillary statistic that is a function of \\(T\\). However, if we allow ancillary statistics that are not functions of \\(T\\), then both \\(\\sum x_i^2\\) and \\(\\sum y_i^2\\) are ancillary for \\(\\rho\\). But which one do we choose? Any one of them! 7.4 Complete Statistics An important technical property for a statistic to have is completeness. . Example 7.8 (Complete Statistics) Let \\(X\\sim f(x|\\theta), \\theta \\in \\Theta\\). A statistic \\(T=T(X)\\) (not necessarily sufficient) is complete if for any function \\(g(\\cdot)\\), \\[E_{\\theta}(g(T)) = 0 \\quad \\forall \\theta \\Rightarrow P_{\\theta}(g(T) = 0) = 1 \\quad \\forall \\theta.\\] Note: The family of pdfs of \\(T\\) is called complete. Remark: Why do we care about completeness? The definition says that if \\(T\\) is complete, then only function of \\(T\\) that can have zero expectation on \\(\\theta\\) is the zero function. In other words, there cannot be any non-zero unbiased estimates of 0 based on \\(T\\). If \\(T\\) is not complete, then it is possible for the statistic to have mean 0, but also involve a distribution around 0 containing nonzero values of \\(T(X)\\). This means that there could be another \\(\\theta\\) that yields a different expectation than 0. Conversely, if \\(T\\) is complete, then a \\(T\\) that estimates 0 must have a trivial distribution (a point mass at 0). This trivial distribution is important because it means that we can actually use it for inference. Completeness is used in the Lehmann-Scheffe Theorem to prove optimality results for estimators. 7.4.1 Techniques for Finding CSS Example 7.9 (Polynomial technique) Let \\(\\textbf{X}=(X_1,..., X_n), X_i\\) iid \\(Bern(p)\\). Consider \\(T = \\sum_{i=1}^{n}X_i\\). Note that \\(T\\sim Bin(n,p)\\) as it is the sum of idd Bernoulli random variables. Suppose we have a \\(g(\\cdot)\\) function that satisfies the definition of complete statistic. Then \\(T\\) would be complete. Now what does \\(E_{\\theta}(g(T)) = 0 \\quad \\forall p\\) mean? \\[ 0 = \\sum_{t=0}^{n} g(t) {n \\choose t} p^t (1-p)^{n-t} = (1-p)^n \\sum_{t=0}^{n} g(t) {n \\choose t} \\bigg(\\frac{p}{1-p}\\bigg)^t\\] Observe that for any \\(t, {n \\choose t} \\bigg(\\frac{p}{1-p}\\bigg)^t &gt; 0\\). Furthermore \\((1-p)^n&gt;0\\) for all \\(n, p, t\\). Observe that \\(g(t)\\) is a polynomial of degree \\(n\\) and the only option to satisfy the definition for T to be complete is for \\(g(t) = 0 \\forall t\\). Now \\(P(g(T) = 0)=1 \\Rightarrow T\\) is complete. Example 7.10 Let \\(\\textbf{X}=(X_1,..., X_n), X_i\\) iid \\(N(\\theta, 1)\\). Let \\(T(X)=(X_1,X_2)\\) be a statistic. Observe that if \\(g(T) = X_1 - X_2\\) then \\(E(g(T)) = E(X_1 - X_2) = 0\\) but \\(P(X_1 - X_2=0)\\ne 1\\) for all \\(\\theta\\). Thus \\(T\\) is not a complete statistic. Example 7.11 Let \\(\\textbf{X}=(X_1,..., X_n), X_i\\) iid \\(U(\\theta, \\theta + 1)\\). Recall that \\(T=T(X) = (X_{(1)}, X_{(n)})\\) is MSS for \\(\\theta\\). Consider the ancillary statistic $ X_{(n)} - X_{(1)}$ and \\(E(X_{(n)} - X_{(1)}) = c\\) for some \\(c\\) independent of \\(\\theta\\). Thus, \\(E(X_{(n)} - X_{(1)}-c) = 0\\) but \\(P(X_{(n)} - X_{(1)}-c=0) \\ne 1\\) for all \\(\\theta\\). Therefore \\(T\\) is not a complete statistic. Remark: If \\(T\\) is sufficient, then it contains all the information regarding \\(\\theta\\). If \\(T\\) is also complete, then \\(T\\) contains no irrelevant information about \\(\\theta\\). Intuitively, suppose that \\(T\\) is sufficient and let \\(T = (T_1, S_1)\\) where \\(S_1\\) is ancillary. Then \\(T\\) cannot be complete because it contains irrelevant information about the parameter (recall that ancillary statistics describe where there is no information about \\(\\theta\\)). Theorem 7.9 If \\(T\\) is complete and sufficient and a MSS exists, then \\(T\\) is MSS. Example 7.12 Let \\(\\textbf{X}=(X_1,..., X_n), X_i\\) iid \\(U(0, \\theta ), \\theta &gt;0\\). Consider \\(T = T(X)= X_{(n)}\\). Is \\(T\\) complete? Yes. To prove this we will employ the polynomial technique. By definition of expectation, \\[ E(g(X_{(n)})) = \\int_{0}^{\\theta}g(x)f_{X_{(n)}}(x)dx.\\] Recall order statistics results, specifically the pdf formula for a given order statistic. \\[f_{X_{(n)}}(x) = n x^{n-1}\\theta^{-n} I(0\\le x \\le \\theta)\\] Now, we have that \\(E(g(X_{(n)})) = \\int_{0}^{\\theta}g(x)n x^{n-1}\\theta^{-n}dx =n\\theta^{-n} \\int_{0}^{\\theta}g(x)x^{n-1} = 0 \\Rightarrow \\int_{0}^{\\theta}g(x)x^{n-1} = 0\\). By applying Leibnitz Rule wrt \\(\\theta\\) we obtain that \\(g(\\theta)\\theta^{n-1}=0\\), which implies that \\(g(\\cdot)\\) must be identically zero because we assumed that \\(\\theta &gt;0\\). Therefore \\(T\\) is a complete statistic. Remark: If the density of \\(T\\) satisfies \\[f(t|\\theta)=h(t)c(\\theta)I(0 \\le t \\le \\theta)\\] then \\(T\\) is complete. Certainly, the previous remark provides a useful and fast strategy to prove that a statistic is complete. However, as we have mentioned before, exponential family distributions provide what some people would call easy and fast strategies. Theorem 7.10 (Complete statistics in exponential family distributions) If \\(X_1,..., X_n, iid, X_i\\sim\\) exponential family with pdf/pmf of the form \\[h(x)c(\\theta)\\exp\\Big(\\sum_{i=1}^k w_i(\\theta)t_i(x)\\Big)\\] and \\(\\mathcal{H}=\\{(w_1(\\theta), ..., w_n(\\theta))|\\theta \\in \\Theta\\}\\) contains an open set (or open rectangle) in \\(\\mathbb{R}^{k}\\), then \\(T=(T_1,..., T_k)\\) is a complete and sufficient statistic, where \\(T_j = \\sum_{i=1}^{n}t_j(x_i)\\). At a glance, this exponential family theorem on complete and sufficient statistics is great. However, theres a catch: for most of us it is hard to grasp the concept of open set. In this part we will try to provide intuition behind the open set concept. An open \\(k-\\) dimensional ball of radius \\(r&gt;0\\) (also called epsilon-balls) centered at a point \\(x \\in \\mathbb{R}^k\\) is defined as the set \\(B(x,r) = \\{y| d(x,y) &lt; r\\}\\), where \\(d(x,y)\\) is a distance function. Observe that what makes the ball open is the fact that those points \\(y\\) whose distance to \\(x\\) is exactly \\(r\\) are not in the set. However, we are interested in a set that contains an open set, like an open ball. Let \\(A \\subset \\mathbb{R}^{k}\\). We say that the set \\(A\\) contains an open set in \\(\\mathbb{R}^{k}\\) if and only is \\(A\\) contains a \\(k-\\)dimensional ball B(x,r), i.e. there exists \\(x \\in \\mathbb{R}^{k}\\) and \\(r&gt;0\\) such that \\(B(x,r) \\subset A\\). Observe that we do not need our set \\(A\\) to be open, we just need \\(A\\) to contain at least one open set in the relevant space. For example, in the real line any open interval contains an open set, i.e. \\((a,b)\\) contains an open subset of \\(\\mathbb{R}\\) because for at least point between \\(a\\) and \\(b\\) we can fit an infinitesimally small open ball centered at the point. Let us see an example. Example 7.13 Recall that the Bernoulli pmf is an exponential family distribution: \\[f(x|\\theta) = \\theta^x (1-\\theta)^{1-x} = (1-\\theta)exp\\bigg\\{ x\\text{ } log\\bigg(\\frac{p}{1-p}\\bigg)\\bigg\\}, p \\in (0,1), x \\in \\{0,1\\}\\] For an iid \\(Bern(p)\\) sample, previous theorems say that \\(T = \\sum_{i=1}^{n} x_i\\) is a MSS for \\(p\\). Furthermore, \\(\\mathcal{H} = \\{log(p/(1-p))|p \\in (0,1)\\}\\). Does \\(\\mathcal{H}\\) contain an open set? Yes. Think about the range of \\(log(p/(1-p))\\) for \\(p \\in (0,1)\\). First observe that \\(p/(1-p) \\in (0, \\infty)\\) which implies that \\(log(p/(1-p)) \\in (-\\infty, \\infty) = \\mathbb{R}\\), which certainly contains an open set, \\((-1,1)\\) for example. Therefore \\(\\mathcal{H}\\) contains an open set in \\(\\mathbb{R}\\) and we conclude that \\(T = \\sum_{i=1}^{n} x_i\\) is a complete statistic. Most of the time when the statistic T is in fact complete, we can think about the range of the \\(w_i(\\theta)\\) functions. If we can find any open set within the range, then we are good to go. However, the task becomes more difficult when the set \\(\\mathcal{H}\\) does not contain an open set, because this does not necesarily mean that the statistic is not complete. Example 7.14 Consider the following pdf \\[f(x|\\theta) = \\frac{2 exp \\{-(x-\\theta)^4\\}}{\\Gamma(1/4)}, -\\infty &lt; x &lt; \\infty, -\\infty &lt; \\theta&lt; \\infty.\\] Observe that this distribution belongs to the exponential family as we can express the pdf in exponential family form, \\[f(x|\\theta) = \\frac{2 exp \\{-x^4\\}}{\\Gamma(1/4)} exp(\\theta^4) exp\\{(4\\theta)x^3 - (6\\theta^2) x^2 + (4\\theta^3)x\\}\\] where \\(t_1(x) = x^3, t_2(x) = x^2, t_3(x) = x\\) and \\(w_1(\\theta) = 4\\theta, w_2(\\theta)= 6\\theta^2, w_3(\\theta)=4\\theta^3\\). Therefore, this is a 3-parameter exponential family, with \\(dim(\\theta)=1\\). Now \\[ \\begin{aligned} \\mathcal{H}&amp;=\\{(w_1(\\theta), w_2(\\theta), w_3(\\theta)) | w_1(\\theta) = 4\\theta, w_2(\\theta)= 6\\theta^2, w_3(\\theta)=4\\theta^3, -\\infty &lt; \\theta&lt; \\infty \\}\\\\ &amp; = \\{(w_1(\\theta), w_2(\\theta), w_3(\\theta)) | w_1(\\theta) \\in \\mathbb{R}, w_2(\\theta)= -(6/16)w_1(\\theta)^2, w_3(\\theta)=w_1(\\theta)^3/16 \\} \\end{aligned} \\] where in the last equality we have that \\(w_2(\\theta), w_3(\\theta)\\) can be expressed in terms of \\(w_1(\\theta)\\). This implies that the set \\(\\mathcal{H}\\) is a curve in \\(\\mathbb{R}^3\\) and a sphere (open set in \\(\\mathbb{R}^3\\)) cannot fit in a curve and \\(\\mathcal{H}\\) does not contain an open set in \\(\\mathbb{R}^3\\). This means that we cannot use the theorem to prove that \\(T\\) is complete (or not complete). In this case we would have to use the definition of complete statistic to verify completeness. Example 7.15 Consider the \\(N(\\mu, \\sigma^2)\\) family with \\(\\theta = (\\mu, \\sigma^2) \\in \\Theta\\). This family has pdf that can be expressed as an exponential family with \\(w_1(\\theta) = \\mu/\\sigma^2, w_2(\\theta) = -1/2\\sigma^2, t_1(x)= x, t_2(x)=x^2\\). Let \\(\\textbf{X} = (X_1,...,X_n)\\) be iid \\(N(\\mu, \\sigma^2)\\). By theorem, \\(T(\\textbf{X}) = \\sum X_i, \\sum X_i^2)\\) is a sufficient statistic regardless of the parameter set \\(\\Theta\\). Furthermore, observe that \\(U(\\textbf{X}) = (\\bar{x}, s^2)\\) is a 1-1 function of \\(T(\\textbf{X})\\). Let us explore if \\(T\\) (or \\(U\\)) is complete. We will see that the application of the CSS for exponential family theorem will depend on the parameter set \\(\\Theta\\) and on \\(\\mathcal{H}\\). \\(\\Theta_1=\\{(\\mu, \\sigma^2)|\\sigma^2&gt;0\\} \\Rightarrow \\mathcal{H}= \\{(\\mu/\\sigma^2, -1/2\\sigma^2) | \\sigma^2&gt;0 \\}\\). Observe that in this case there are no constraints on \\(\\mu\\). Thus \\(\\mathcal{H} = \\mathbb{R}^2\\) which trivially contains an open set of \\(\\mathbb{R}^2\\). In this case we can use the theorem and conclude that \\(T\\) is complete. \\(\\Theta_2=\\{(\\mu, \\sigma^2)|\\sigma^2=\\sigma_0^2\\} \\Rightarrow \\mathcal{H}= \\{(\\mu/\\sigma_0^2, -1/2\\sigma_0^2)\\} = \\{(x,-1/2\\sigma_0^2)|x\\in \\mathbb{R}\\}\\). Now \\(\\sigma^2\\) can only take one value, which implies that \\(\\mathcal{H}\\) is a line in \\(\\mathbb{R}^2\\). Can a disk fit in a line? No, thus we cant use the theorem to assess completeness of \\(T\\). Using the definition of complete statistic one can prove that, in fact, \\(T\\) is not complete in this case. \\(\\Theta_3 = [1,3] \\times [4,6]\\) implies that \\(\\mu, \\sigma^2\\) can take values within the closed square that is \\(\\Theta_3\\). Does a disk fit into a square? Yes. Thus we can conclude that \\(T\\) is complete. Hopefully that discussion clarifies when and how to use the theorem on complete statistics in exponential family distributions. Now, let us step back a bit and summarize what we have discussed until now as we present Basus Theorem, one of the most important theorems in statistical inference. 7.4.2 Basus Theorem How do we know if two statistics \\(T\\) and \\(S\\) are independent? Determining whether two statistics are independent directly can be difficult, since it requires finding their joint distribution, which may be infeasible. Basus theorem provides a more efficient technique for evaluating the independence of two statistics. Theorem 7.11 (Basu's Theorem) Let \\(X \\sim f(x|\\theta), \\theta \\in \\Theta\\). If \\(T(X)\\) is a complete and sufficient statistic (for \\(\\theta \\in \\Theta\\)), and \\(S(X)\\) is ancillary, then \\(T(X)\\) and \\(S(X)\\) are independent for all \\(\\theta \\in \\Theta\\). Remark: This result plainly says that CSSs are independent of ancillary statistics. Note, however, that the converse is not true; just because \\(T\\) is independent of all ancillary statistics does not imply that \\(T\\) is complete. Example 7.16 Let \\(X_1, ..., X_n\\) iid \\(N(\\mu, \\sigma^2)\\). Suppose that \\(\\sigma^2\\) is known. We want to show that \\(\\bar{X}\\) and \\(S^2\\) are independent using Basus Lemma. - First, let us show that \\(\\bar{X}\\) is a CSS for \\(\\mu\\). Observe that the pdf for a \\(N(\\mu, \\sigma^2)\\) random variable where \\(\\sigma^2\\) is known is an exponential family distribution where \\(t_1(x) = x, w_1(\\theta)= \\mu/\\sigma^2\\). Here \\(\\mathcal{H} = \\{\\mu/\\sigma^2 | \\mu \\in \\mathbb{R}\\} = \\mathbb{R}\\) which trially contains an open set. Thus \\(T(X) = \\{\\sum X_i\\}\\) is a CSS. Furthermore consider \\(S(X) = T(X)/n = \\bar{X}\\) is a 1-1 function of a CSS and thus is also a CSS. - Second, we must prove that \\(S^2\\) is ancillary for \\(\\mu\\). By definition, \\[S^2 = \\frac{1}{n-1}\\sum_{i=1}^{n}(X_i - \\bar{X})^2\\]. Let \\(X_i = Z_i + \\mu\\) where \\(Z \\sim N(0,1)\\). Now, \\[ \\begin{aligned} S^2 &amp;= \\frac{1}{n-1}\\sum_{i=1}^{n}(X_i - \\bar{X})^2\\\\ &amp; = \\frac{1}{n-1}\\sum_{i=1}^{n}(Z_i + \\mu - \\frac{\\sum(Z_i + \\mu)}{n})^2\\\\ &amp;= \\frac{1}{n-1}\\sum_{i=1}^{n}(Z_i + \\mu - \\frac{\\sum Z_i}{n} - \\mu)^2\\\\ &amp;= \\frac{1}{n-1}\\sum_{i=1}^{n}(Z_i - \\bar{Z} )^2 \\end{aligned} \\] which is independent of \\(\\mu\\). Therefore, \\(S^2\\) is ancillary for \\(\\mu\\). By Basus theorem we conclude that \\(\\bar{X}\\) is independent of \\(S^2\\). Now what happens when \\(\\sigma^2\\) is also unknown? Observe that the previous work assumed that \\(\\sigma ^2\\) as known, fixed, and arbitrary. Thus \\(\\bar{X}\\) is independent of \\(S^2\\) even in the case where both \\(\\mu, \\sigma^2\\) are unknown. Example 7.17 In this example we will use Basus theorem to prove that a statistic is not complete. Let \\(\\textbf{X} = (X_1,..., X_n), X_i\\) iid \\(U(\\theta, \\theta + 1)\\). In a previous example we saw that \\(T(X) = (X_{(1)}, X_{(n)})\\) is a MSS for \\(\\theta\\). Furthermore, we also saw that \\(R=X_{(n)} - X_{(1)}\\) is an ancillary statistic for \\(\\theta\\). Observe that \\(R\\) is a function of \\(T\\) and cannot be independent. Thus, by the contrapositive of Basus Lemma, \\(T\\) cannot be complete. References "],["point-estimators-finite-samples.html", "Chapter 8 Point Estimators: Finite Samples 8.1 Identifiability 8.2 Finding estimators 8.3 Properties of Estimators 8.4 Uniform Minimum Variance Unbiased Estimators (UMVUEs) 8.5 Inferential Properties of Exponential Families Distributions", " Chapter 8 Point Estimators: Finite Samples The goal of parametric statistical inference is to estimate a parameter - or a function of said parameter - of a given distribution using a random sample. To do this, we must construct an estimator: Definition 8.1 (Estimator) An estimator is a statistic \\(T(X)\\) - a function of data - used to obtain an estimate of a parameter \\(\\theta\\) or function of a parameter \\(\\tau(\\theta)\\) in a distribution. An estimate is the value of an estimator when computed on a set of data - that is, \\(T(X)\\) for a particular realization of \\(X\\). An estimand is the parameter \\(\\theta\\) or function \\(\\tau(\\theta)\\) needed to be estimated. Point estimators estimate scalar or vector real values - as opposed to interval estimators, which estimate intervals. Estimators have various qualities - some are good, and some are bad. This chapter will discuss how to construct and evaluate the quality of estimators. 8.1 Identifiability First of all, how do we know if it even possible to estimate a given parameter in a distribution? Such a question gives rise to the concept of identifiability: Definition 8.2 (Identifiability) For a random variable \\(X\\), a parameter \\(\\theta\\) in a parameter space \\(\\Theta\\) is said to be identifiable if, for all \\(\\theta, \\theta&#39; \\in \\Theta\\), \\[\\theta \\neq \\theta&#39; \\implies F_X(x|\\theta) \\neq F_X(x|\\theta&#39;)\\] Alternatively, \\(\\theta\\) is not identifiable if there exists \\(\\theta, \\theta&#39; \\in \\Theta\\) such that, for all \\(x\\), \\[F_X(x|\\theta) = F_X(x|\\theta&#39;)\\] If \\(\\theta\\) is non-identifiable, that means two distributions from the same family will be identical under different \\(\\theta\\) values. Hence, even if we knew the true underlying distribution of \\(X\\), it would be impossible to know its parameters. Given a random sample, it would be impossible to estimate the parameter accurately. Non-identifiability usually arises when a distribution is parameterized by some function of multiple parameters. For instance, if \\(\\lambda = (\\lambda_1, \\lambda_2)\\), and \\(X \\sim \\text{Exp}(\\lambda_1 + \\lambda_2)\\), it would be impossible to estimate \\(\\lambda_1\\) or \\(\\lambda_2\\) given \\(X\\) alone since \\(\\lambda_1 + \\lambda_2\\) can take on the same value for different individual values of \\(\\lambda_1\\) and \\(\\lambda_2\\). Generally, we can assume parameters are identifiable. But how might we show a parameter is not identifiable? One clue to look for is linear dependence. Example 8.1 (Non-Identifiability Via Linear Dependence) Non-identifiability can arise in a natural exponential family. Suppose weve parametrized a natural exponential family into the form \\[f(x | \\eta) = h(x)c(\\theta)\\exp(\\sum_{i=1}^k \\eta_it_i)\\] where the vector \\(t = (t_1, ..., t_k)\\) is linearly dependent. Linear dependence implies that there exists \\(\\alpha = (\\alpha_1, ..., \\alpha_k)\\) not equal to the 0-vector such that \\(\\alpha t = 0\\). Then, \\(\\eta\\) is not identified? How do we know? Since \\(\\alpha t = \\sum_{i=1}^k \\alpha_it_i = 0\\), we can add it on the interior of the exponential function: \\[f(x | \\eta) = h(x)c(\\theta)\\exp(\\sum_{i=1}^k \\eta_it_i + \\sum_{i=1}^k \\alpha_it_i)\\\\ = h(x)c(\\theta)\\exp(\\sum_{i=1}^k (\\eta_i + \\alpha_i)t_i)\\] Letting \\(\\eta = (\\eta_1 + \\alpha_1,..., \\eta_k + \\alpha_k)\\), clearly \\(f(x|\\eta) = f(x|\\eta&#39;)\\) and \\(\\nu\\) is not identified. Non-identifiability can also arise in more practical situations, such as the following: Example 8.2 (Non-Identifiability with Measurement Error) Suppose we want to construct a simple linear regression model \\(Y = \\beta X + \\varepsilon\\) - but theres a twist. Both \\(Y\\) and \\(X\\) are measured with unknown error, so we only observe \\[ Y^* = Y + \\epsilon_1\\\\ X^* = X + \\epsilon_2 \\] If we try to fit this linear regression on \\(Y^*\\) and \\(X^*\\) instead, we get \\[ Y + \\epsilon_1 = \\beta(X + \\epsilon_2) + \\varepsilon\\\\ \\implies Y = \\beta X + (\\beta\\epsilon_2 - \\varepsilon_1 + \\varepsilon) \\] Now, since \\(\\epsilon_1\\) and \\(\\epsilon_2\\) are unknown, \\(\\beta\\) has become unidentifiable. Consider \\((\\beta&#39;, \\epsilon_2&#39;) \\neq (\\beta, \\epsilon_2)\\). Subtracting \\(Y = \\beta&#39; X + (\\beta&#39;\\epsilon_2&#39; - \\varepsilon_1 + \\varepsilon)\\) from \\(Y = \\beta X + (\\beta\\epsilon_2 - \\varepsilon_1 + \\varepsilon)\\), we get \\[0 = (\\beta - \\beta&#39;)X + (\\beta\\epsilon_2 - \\beta&#39;\\epsilon_2&#39;)\\] Multiple values of \\(\\beta\\) and \\(\\beta&#39;\\) can be chosen to satisfy this equation (since \\(\\epsilon \\neq \\epsilon&#39;\\)) meaning that \\(F_X(x|\\beta) = F_X(x|\\beta&#39;)\\) for \\(\\beta \\neq \\beta&#39;\\), proving \\(\\beta\\) is not identifiable. (can we use the MGF to show this as well?) 8.2 Finding estimators How do we take a guess at what a good estimator might be? There are several methods used to find parameter estimators. In this chapter we will discuss two very popular methods, the method of moments and the maximum likelihood method. 8.2.1 Method of Moments Recall that the moments of a distribution are given by \\(\\mu^k = E[X^k]\\), where \\(k \\in \\mathbb{N}_{+}\\). The idea of the method of moments is to equate the population quantity to the sample quantity, i.e. \\[\\mu^k = E[X^k] = \\frac{1}{n} \\sum_{i=1}^{n} X_i^k\\] where \\(k=1,2,...\\) and \\(\\textbf{X}=(X_1,..., X_n)\\) is the sample from a distribution \\(f(x|\\theta)\\) where \\(\\theta = (\\theta_1,..., \\theta_k)\\). Usually we use find \\(k\\) equations of sample average and moments to estimate \\(k\\) parameters. Example 8.3 Suppose we want to find MOM estimators for \\(\\alpha, \\beta\\) from a \\(X\\sim\\Gamma(\\alpha,\\beta)\\) random variable. Let \\(\\textbf{X}=(X_1,...,X_n)\\) be and iid sample from the gamma distribution. In this case we know that \\(E[X]=\\alpha/\\beta\\) and \\(Var(X)= \\alpha/\\beta^2\\). Since we are estimating two parameters, we will be using the first two moments, namely, \\(E[X], E[X^2]\\). Observe that \\(Var(X) = E[X^2] - E[X]^2 \\Rightarrow E[X^2] = \\alpha/\\beta^2 + (\\alpha/\\beta)^2\\). Now, we can proceed with the method of moments estimation with the following two equations. \\[ \\bar{X} = \\alpha/\\beta\\\\ \\bar{X^2} = \\alpha/\\beta^2 + (\\alpha/\\beta)^2\\] Solving for \\(\\alpha, \\beta\\) we obtain that \\[ \\hat{\\beta} = \\frac{\\bar{X}}{\\bar{X^2} - \\bar{X}^2},\\\\ \\hat{\\alpha} = \\hat{\\beta}\\bar{X} = \\frac{\\bar{X}^2}{\\bar{X^2} - \\bar{X}^2}.\\] Issues with the MoM Estimators: MOM estimators may not exist. Suppose \\(Y_1,...,Y_n\\) is a random sample from a \\(Unif(-\\theta, \\theta)\\). Then \\(E[X] = 0\\) and a MOM estimator would solve the following equation, \\(\\bar{Y} = 0\\), which has no solutions in \\(\\theta\\). Thus a MOM estimator of \\(\\theta\\) does not exist. MOM estimators may be outside of the parameter space. MOM estimators are not invariant to transformations. Advantages of MoM Estimators: They are guaranteed to be unbiased and consistent for the parameter they estimate. MOM estimators can offer a starting point for iterative methods used to find solutions of likelihood equations. 8.2.2 Maximum Likelihood Estimation Rather than simply matching moments, often a more effective strategy for finding an estimator is by finding the value which maximizes the entire likelihood of the sample. That is, for a given \\(X = X_1, X_2, ... X_n\\), we choose the estimator for which this sample is most likely. This is called the maximum likelihood estimator. Because of how common this approach is, we introduce several terms to describe the components of MLE. Definition 8.3 (Likelihood) Let \\(X\\) be a random variable or random vector, and \\(\\theta\\) a parameter or vector of parameters describing the distribution of \\(X\\). Then, the likelihood is \\[\\mathcal{L}(\\theta | X) = f_X(x|\\theta)\\] Definition 8.4 (Log-Likelihood) Let \\(X\\) be a random variable or random vector, and \\(\\theta\\) a parameter or vector of parameters describing the distribution of \\(X\\). Then, the log-likelihood is \\[\\ell(\\theta|X) = \\log\\mathcal{L}(\\theta | X) = \\log f_X(x|\\theta)\\] The log-likelihood is often easier to use. Furthermore, When \\(X\\) is an iid sample \\(X_1, X_2, ... X_n\\), by the properties of logarithms, \\(\\ell(\\theta|X) = \\log\\Big(\\prod_{i=1}^nf_{X_i}(x|\\theta)\\Big) = \\sum_{i=1}^n \\log f_{X_i}(x|\\theta)\\). Working with a sum is much easier than with a product. Of course, the whole reason this is permissible in finding a maximum likelihood estimate is that the \\(\\log\\) function is monotonic - finding the maximum of a function is equivalent to finding the maximum of its logarithm. Maximizing this likelihood often requires calculus - specifically, the First Derivative Test. In statistics, the gradient of the log-likelihood has a special name: the score. Definition 8.5 (Score Equations) If \\(\\theta\\) is a vector of parameters, the score equations are defined as the gradient of the log-likelihood (as described above). That is, \\[U(\\theta | X) = \\begin{bmatrix}\\frac{\\partial}{\\partial\\theta_1}\\ell(\\theta_1|X) &amp; ... &amp; \\frac{\\partial}{\\partial\\theta_k}\\ell(\\theta_k|X) \\end{bmatrix}\\] Of course, if \\(\\theta\\) is a single parameter, then \\(U(\\theta|X) = \\frac{d}{d\\theta}\\ell(\\theta|X)\\) Therefore, the first step in finding an MLE is to set the score to 0, and solve for the desired parameter. Lets see an example. Example 8.4 Let \\(\\textbf{X} = (X_1,..., X_n)\\) be an iid sample from a \\(Bern(p)\\) distribution. To find the MLE for \\(p\\), let us find maximize wrt \\(p\\) the log likelihood function. The likelihood function is \\(\\mathcal{L}(p|\\textbf{X}) = \\prod_{i=1}^{n} p^X_i(1-p)^{1-X_i} = p^{\\sum X_i}(1-p)^{n-\\sum X_i}\\). Now, the log-likehood is given by \\(\\mathcal{l}(p|\\textbf{X}) = log(p) \\sum X_i + log(1-p)(n-\\sum X_i)\\). The score equation goes as follows. \\[ U(p|\\textbf{X}) = \\frac{\\sum X_i}{p} - \\frac{(n-\\sum X_i)}{1-p} \\overset{set}= 0\\] and the solution wrt \\(p\\) is \\(\\bar{X}\\). Now, the second derivative test is always negative which implies that \\(\\hat{p}_{MLE} = \\bar{X}\\). The score has two useful properties in MLE theory. First, under regularity conditions, \\(E(U(\\theta|X)) = 0\\) - its mean is 0. This is because the regularity conditions imply Leibnizs rule; that is, \\[\\begin{align} E(U(\\theta|X)) = \\int_{\\mathcal{X}}\\frac{\\partial}{\\partial\\theta}\\log\\mathcal{L}(\\theta|X)f(x|\\theta)dx \\\\ = \\int_{\\mathcal{X}}\\frac{1}{f(x|\\theta)}\\frac{\\partial}{\\partial\\theta}f(x|\\theta)f(x|\\theta)dx \\\\ = \\frac{\\partial}{\\partial\\theta}\\frac{f(x|\\theta)}{f(x|\\theta)}f(x|\\theta)dx = \\frac{\\partial}{\\partial\\theta}1 = 0 \\end{align}\\] The second useful property is that, under the same regularity conditions, the variance of the score is \\[\\begin{align} Var(U(\\theta|X)) = E(U(\\theta|X)U(\\theta|X)^\\top) \\\\ = -E\\Big(\\frac{\\partial^2}{\\partial\\partial^\\top}\\ell(\\theta|X)\\Big) \\end{align}\\] If \\(\\theta\\) is one-dimensional, \\(Var(U(\\theta|X)) = E(U(\\theta|X)^2) = -E\\Big(\\frac{\\partial^2}{\\partial\\theta^2}\\ell(\\theta|X)\\Big)\\). The rather insightful derivation of this is available here. This variance has a special name, the information (or sometimes, Fisher Information) Definition 8.6 (Information) In statistics, information refers to the amount of information that a random variable \\(X\\) contains about a parameter \\(\\theta\\). It is defined mathematically as the variance of the score, which is \\[I(\\theta) = E\\Big((\\frac{\\partial}{\\partial\\theta}\\log f(X|\\theta))^2\\Big|\\theta\\Big)\\] Under MLE regularity conditions, \\[I(\\theta) = -E\\Big(\\frac{\\partial^2}{\\partial\\theta^2}\\log f(X|\\theta) \\Big|\\theta\\Big)\\] As the mean value of a second derivative, the information measures the curve of \\(\\ell(\\theta|X)\\) (expand?) The negative second derivative of the log-likelihood also has a special name itself: the observed information. Definition 8.7 (Observed Information) The observed information is defined as \\[\\mathcal{J}(\\theta|X) = -\\frac{\\partial^2}{\\partial\\theta^2}\\ell(\\theta|X)\\] When \\(\\theta\\) is a \\(k\\)-dimensional vector, the observed information is the Hessian of the log-likelihood: \\[\\mathcal{J}(\\theta|X) = -\\nabla\\nabla^\\top\\ell(\\theta|X) = \\begin{bmatrix}\\frac{\\partial^2}{\\partial\\theta_1^2} &amp; ... &amp; \\frac{\\partial^2}{\\partial\\theta_1\\partial\\theta_k}\\\\ \\vdots &amp; &amp; \\vdots\\\\ \\frac{\\partial^2}{\\partial\\theta_k\\partial\\theta_1} &amp; ... &amp; \\frac{\\partial^2}{\\partial\\theta_k^2}\\end{bmatrix}\\] The observed information can be used to estimate the Fisher information for a given sample \\(X\\). The MLE theory is essential is the task of finding estimators for a given parameter. By definition, the solution that maximizes the likelihood function is called the maximum likelihood estimator for the parameter. This estimator has very properties, namely, it is a statistic that respects the parameter space. We previously saw an example in which we found an MLE for a one-dimensional parameter. However, we can find multidimensional MLEs for multidimensional parameters. There are two main approaches. Let \\(\\theta = (\\theta_1, \\theta_2)\\), a two-dimensional parameter. 1. Find MLE candidates for \\(\\theta_1, \\theta_2\\) by setting partial derivative wrt \\(\\theta_1, \\theta_2\\) respectively to zero and solving. 2. Find second partial derivatives and show that at least one of them is negative. 3. Find the second order partial derivative matrix and show that the determinant is positive. If 2. and 3. are shown to be true, then then candidates from 1. are MLEs for \\(\\theta_1, \\theta_2\\). Let \\(\\theta\\) be a \\(k\\)-dimensional parameter, ie, \\(k\\) parameters. Then we can employ the profile-likelihood approach to find an MLE candidate. The profile-likelihood approach consists of maximizing with respect to a parameter of interest and profiling out the rest of the parameters (which can be considered nuisance parameters). In the case in which we want to find MLE for all \\(k\\) parameters, then we would have to maximize with respect to each parameter and then substitute back our solutions until we prove that the candidates maximize each of the profile likelihoods for each parameter. This method is necessary when we want to find the MLE of an unknown, but fixed, number of parameters. EXAMPLE FORTHCOMING Remarks: Suppose \\(X \\sim f(x|\\eta)\\) is an exponential family with natural parametrization. Then MOM estimators correspond to MLE. Theorem 8.1 If an MLE is unique and an MSS exists, then the MLE is a function of the MSS. Theorem 8.2 (Invariance property of MLE) If \\(\\hat{\\theta}\\) is MLE for \\(\\theta\\), then \\(\\tau(\\hat{\\theta})\\) is MLE for \\(\\tau(\\theta)\\). Example 8.5 invariance of mle example 8.3 Properties of Estimators How do we know if a particular estimator is considered good? There are two main properties by which we measure the quality of an estimator: bias, which measures the accuracy of the estimator, and variance, which measures the precision. Let \\(\\textbf{X} = (X_1,..., X_n), X_i \\sim f(x|\\theta), \\theta \\in \\Theta\\) and let \\(\\hat{\\theta}\\) be an estimator of \\(\\theta\\). Then, we have the following: Definition 8.8 (Bias of an estimator) \\[bias(\\hat{\\theta}) = E[\\hat{\\theta}] - \\theta\\] If \\(bias(\\hat{\\theta})=0\\) then \\(E[\\hat{\\theta}] = \\theta\\) and we say that \\(\\hat{\\theta}\\) is an unbiased estimator of \\(\\theta\\). Note: Bias is a measure of accuracy. Definition 8.9 (Variance of an estimator) \\[Var(\\hat{\\theta}) = E[(\\hat{\\theta} - E[\\hat{\\theta}])^2] = E[\\hat{\\theta}^2]- E[\\hat{\\theta}]^2 \\] Note: Variance is a measure of precision Definition 8.10 (Mean squared error, MSE) \\[MSE(\\hat{\\theta}) = Var(\\hat{\\theta}) + bias^2(\\hat{\\theta})\\] Note: MSE is a measure of precision and accuracy. Ideally we want an unbiased estimator with minimal variance. Often we have a variance-bias tradeoff, a situation in which when bias decreases, variance increases and viceversa. For example, for any distribution we have the following facts: \\(S^2\\) is unbiased \\(\\hat{\\sigma^2}\\) is biased \\(Var(\\hat{\\sigma^2}) &lt; Var(S^2)\\) Definition 8.11 Let \\(\\hat{\\theta}, \\tilde{\\theta}\\) be two estimators of \\(\\theta\\). If \\(MSE(\\hat{\\theta}) \\le MSE (\\tilde{\\theta}) \\forall \\theta\\) and \\(MSE(\\hat{\\theta}) &lt; MSE (\\tilde{\\theta})\\) for at least one \\(\\theta\\), then we say that \\(\\tilde{\\theta}\\) is inadmissible. Definition 8.12 Let \\(\\hat{\\theta}, \\tilde{\\theta}\\) be any unbiased estimators of \\(\\theta\\) whose variances exist. Then, \\[RE(\\hat{\\theta};\\tilde{\\theta}) = \\frac{Var(\\tilde{\\theta})}{Var(\\hat{\\theta})}\\] is the relative efficiency of \\(\\hat{\\theta}\\) to \\(\\tilde{\\theta}\\). This allows use to compare variance between estimators 8.4 Uniform Minimum Variance Unbiased Estimators (UMVUEs) Is it possible for there to be a best estimator with respect to MSE? If we restrict consideration only to unbiased estimators, then the answer may be yes. An unbiased estimator with the minimum possible variance among all possible \\(\\theta\\) is known as a Uniform Minimum Variance Unbiased Estimator, or UMVUE for short. Lets define this mathematically. Definition 8.13 (Uniform minimum variance unbiased estimator (UMVUE)) An estimator \\(W\\) of a parameter \\(\\tau(\\theta)\\) is called uniform minimum variance unbiased estimator (UMVUE) of \\(\\tau(\\theta)\\) if \\(E_{\\theta}(W)= \\tau(\\theta)\\) and \\(Var_{\\theta}(W) \\le Var_{\\theta}(Q)\\) for any other unbiased estimator, Q, of \\(\\tau(\\theta)\\). The UMVUE is the unbiased estimator of \\(\\tau(\\theta)\\) with the minimum variance among all other unbiased estimators of \\(\\tau(\\theta)\\). First, lets consider when the UMVUE exists - in this case, there are generally two strategies to construct the UMVUE. 8.4.1 Finding UMVUEs I: Cramér-Rao Bound If you can find an unbiased estimator of \\(\\tau(\\theta)\\), one way to show that it is an UMVUE is by showing that it attains the minimum possible variance among all estimators. The Cramér-Rao Bound provides such a lower bound for the variance of all estimators of a parameter \\(\\tau(\\theta)\\). This result is extremely powerful because if we can show that an unbiased estimator attains the Cramér-Rao Lower Bound (CRLB), then the estimator is UMVUE. Definition 8.14 (Cramér-Rao Bound) Let \\(\\textbf{X}=(X_1,..., X_n), \\textbf{X} \\sim f_{\\textbf{X}}(x|\\theta), \\theta \\in \\Theta\\). If \\(W=W(\\textbf{X})\\) is some estimator such that \\[\\begin{equation} \\frac{\\partial}{\\partial\\theta} E_{\\theta}(W)=\\int \\frac{\\partial}{\\partial\\theta} \\bigg[ W(\\textbf{X})f_{\\textbf{X}}(x|\\theta)\\bigg]dx \\tag{8.1} \\end{equation}\\] then, \\[Var(W) \\ge \\frac{[\\frac{\\partial}{\\partial\\theta} E_{\\theta}(W)]^2}{E_{\\theta}([\\frac{\\partial}{\\partial\\theta} \\log(f_{\\textbf{X}}(x|\\theta))]^2)}.\\] Remark: If the sample is iid and \\[ \\begin{equation} \\frac{\\partial^2}{\\partial\\theta^2} \\int f(x|\\theta) dx = \\int \\frac{\\partial^2}{\\partial\\theta^2} f(x|\\theta) dx \\tag{8.2} \\end{equation} \\] then, \\[Var(W) \\ge \\frac{[\\frac{\\partial}{\\partial\\theta} E_{\\theta}(W)]^2}{nE_{\\theta}[-\\frac{\\partial^2}{\\partial\\theta^2} ln(f_{X_i}(x|\\theta))]}.\\] Before continuing, lets discuss conditions (8.1) and (8.2). First, if (8.1) is satisfied then \\[E_{\\theta}\\bigg[\\frac{\\partial}{\\partial\\theta}\\log(f_{\\textbf{X}}(X|\\theta))\\bigg] = 0, \\forall \\theta \\in \\Theta.\\] Furthermore, if (8.2) is satisfied, then \\[E_{\\theta}\\bigg[\\frac{\\partial^2}{\\partial\\theta^2} \\log(f(x|\\theta))\\bigg] = -E_{\\theta}\\bigg[\\bigg(\\frac{\\partial}{\\partial\\theta} \\log(f(x|\\theta))\\bigg)^2\\bigg]. \\] Remarks: (8.2) is satisfied for exponential families. If (8.1) and (8.2) are not satisfied, then the CRLB might not be the lower bound for the variance. Sometimes, especially for multiple parameters, we can also write the CRLB as \\(Var(W) \\geq I(\\theta)^{-1}\\), the inverse of of the information mentioned previously. Thus, if one is computing the MLE, the CRLB follows from the same calculations used for the MLE, which can save time, especially for problems involving common families of distributions. Example 8.6 CRLB example Since the CRLB is a lower bound, if the variance of an unbiased estimator \\(Var(\\hat{\\theta})\\) equals the CRLB, then it must be the UMVUE. While it is certainly possible to compute the CRLB directly, and then compute the variance of the estimator \\(Var(\\hat{\\theta})\\), and check if these values are the same, there is also a simpler method. The Attainment Theorem tells us whether a specific estimator attains the CRLB without requiring that the bound actually be computed: Theorem 8.3 (Attainment Theorem) Let \\(W(X)\\) be an unbiased \\(\\tau(\\theta)\\) and suppose that (8.1) holds. Then \\(W\\) attains CRLB if and only if \\[\\frac{\\partial}{\\partial \\theta} \\log (\\mathcal{L}(\\theta|X)) = a(\\theta)[W-\\tau(\\theta)]\\] for some \\(a(\\theta)\\) As a result, to prove whether an estimator does or does not attain the CRLB, we simply must show that the score of the sample can or cannot be factorized as above, which is often faster than computing the variance of the estimator. Example 8.7 attainment theorem example Note: An estimator that does not attain CRLB can still be UMVUE! For some parameters, CRLB might not be achieved. 8.4.2 Rao-Blackwell and Lehmann-Scheffé Theorem The more common approach to finding an UMVUE is to use the Lehmann-Scheffé theorem, which describes specifically how to construct an UMVUE using a complete sufficient statistic (as discussed in Chapter 7). Before introducing the Lehmann-Scheffé theorem, first let us state the more general theorem upon which its proof is based: Theorem 8.4 (Rao-Blackwell) Suppose \\(W\\) is unbiased for \\(\\tau(\\theta)\\) and \\(T\\) is a SS for \\(\\tau(\\theta)\\). Let \\(Y:=E(W|T)\\). Then, \\(Y\\) is a statistic (not a function of \\(\\theta\\)) \\(E[Y] = \\tau(\\theta), \\forall \\theta\\) \\(Var(Y)\\le Var(W), \\forall \\theta\\). The Rao-Blackwell theorem provides a method to improve upon any unbiased estimator, i.e. provides a better unbiased estimator, by conditioning on a sufficient statistic. However, if this statistic is also complete, then the Lehmann-Scheffé provides a method to find an UMVUE. This is because of the following theorem: Theorem 8.5 If \\(E_{\\theta}(W(X))= \\tau(\\theta)\\), then \\(W(X)\\) is best unbiased if and only if \\(W(X)\\) is uncorrelated with all unbiased estimators of 0. How do we guarantee that \\(W\\) is uncorrelated with all unbiased estimators of 0? A complete statistic contains no unbiased estimators of 0 (other than 0 itself). This is by definition; \\(E(g(T) = 0 \\implies g(T) = 0\\) with probability 1, as established in Complete Statistics. Therefore, if we condition on a complete statistic, then the resulting statistic must be uncorrelated with all unbiased estimators of 0, and therefore be an UMVUE. This principle forms the foundation of the Lehmann-Scheffé Theorem: Theorem 8.6 (Lehmann-Scheffé) Suppose \\(W\\) is unbiased for \\(\\tau(\\theta)\\) and \\(T\\) is a complete and sufficient for \\(\\tau(\\theta)\\). Then \\(Y:=E(W|T)\\) is UMVUE of \\(\\tau(\\theta)\\). In addition, let \\(W\\) be a statistic such that \\(Var(W)&lt;\\infty\\), and \\(W\\) is UMVUE for \\(\\tau(\\theta)\\). Then \\(W\\) is the unique UMVUE of \\(\\tau(\\theta)\\). 8.4.3 Finding UMVUEs II: Using the Lehmann-Scheffé Theorem How would one use the Lehmann-Scheffé Theorem to construct an UMVUE in practice? Lets consider an example. Example 8.8 (Conditioning on a Complete Statistic) Let \\(X_i \\overset{iid}{\\sim} \\text{Bernoulli}(p)\\), and our goal is to find an UMVUE of \\(p^2\\). First, we need to find an unbiased estimator of \\(p^2\\). Since all observations are iid, \\(X_1\\cdot X_2\\) should suffice, as \\(E(X_1X_2) = E(X_1) \\cdot E(X_2) = p^2\\). Since the Bernoulli is an exponential family with \\(w(\\theta) = \\log(\\frac{p}{1-p})\\) an open set, we know its CSS is \\(\\sum_{i=1}^n X_i\\) by Lehmann-Scheffé, the UMVUE is \\[E(X_1X_2 | \\sum_{i=1}^n X_i = t)\\] Since the expected value of a Bernoulli can be written as a probability, this equals \\[ P(X_1X_2 = 1 | \\sum_{i=1}^n X_i = t) = \\frac{P(X_1 = 1, X_2 = 1, \\sum_{i=3}^n X_i = t - 2)}{P(\\sum_{i=1}^n X_i = t)} \\\\ = \\frac{P(X_1=1) \\cdot P(X_2=1) \\cdot P(\\sum_{i=3}^n X_i = t-2)}{{n\\choose t}p^x(1-p)^{n-t}}\\\\ = \\frac{p^2{n-2\\choose t-2}p^{t-2}(1-p)^{n-2 - t+2}}{{n\\choose t}p^t(1-p)^{n-t}}\\\\ = \\frac{{n-2\\choose t-2}}{{n\\choose t}} = \\frac{t(t-1)}{n(n-1)}\\\\ \\] using the fact that \\(\\sum_{i=1}^nX_i \\sim \\text{Binomial}(n,p)\\). Therefore, the UMVUE is \\(\\frac{t(t-1)}{n(n-1)}\\) 8.4.4 Finding UMVUEs III: Lehmann-Scheffé Corollary Corollary 8.1 (Lehmann-Scheffé) Suppose \\(T\\) is a complete sufficient statistic for \\(\\theta\\). Let \\(g(T)\\) be an estimator based solely on \\(T\\). Then, \\(g(T)\\) is the UMVUE of its expected value - that is, \\(E(g(T))\\) is an UMVUE. Rather than playing around with conditional probabilities to construct an UMVUE using Lehmann-Scheffé, it can often be much easier to work backwards based on what you know about the CSS and find a function \\(g\\) such that \\(E(g(T)) = \\tau(\\theta)\\). Here is an algorithm for using the Lehmann-Scheffé corollary for a function of a parameter \\(\\tau(\\theta)\\) given \\(X_1, ..., X_n \\sim f_{X_i}(x|\\theta)\\) Find the CSS of the distribution \\(f_{X_i}(x|\\theta)\\). Call it \\(T\\). Find the distribution of \\(T\\). Often, these are known based on convolution formulas. See Chapter 7 for a table of sufficient statistic distributions. Set \\(E(g(T)) = \\tau(\\theta)\\), and rearrange to get \\(E\\Big(\\frac{g(T)}{\\tau(\\theta)}\\Big) = 1\\). (Optional) Use the linearity of expectation to split the above into multiple integration problems. For example, if \\(\\tau(\\theta) = \\tau_1(\\theta) + \\tau_2(\\theta) + ... + \\tau_k(\\theta)\\), then we can let \\(E(g(T)) = E(g_1(T)) + E(g_2(T)) + ... + E(g_k(T))\\) with each \\(E(g_1(T)) = \\tau_i(\\theta)\\) Because \\(E\\Big(\\frac{g(T)}{\\tau(\\theta)}\\Big)\\) must integrate to 1, \\(\\frac{g(T)}{\\tau(\\theta)}\\) must be a pdf. Therefore, you can use the Kernel Technique to solve for the \\(g(T)\\) that converts the rest of the integrand into a pdf. That last step probably sounds a bit complicated. Lets look at an example. Example 8.9 (Constructing an UMVUE using a function of a CSS) Suppose \\(X_i \\sim \\text{Exp}(\\lambda)\\). Find an UMVUE for \\(\\tau(\\lambda) = \\lambda^k\\) with \\(k \\in \\{1, ... n\\}\\) using the algorithm above. Let us proceed step-by-step. Since the exponential distribution is (trivially) a single-parameter exponential family, \\(T = \\sum_{i=1}^nt_1(X) = \\sum_{i=1}^nX_i\\) is a sufficient statistic. It is known, by convolution properties, that \\(T = \\sum_{i=1}^nX_i \\sim \\text{Gamma}(n, \\lambda)\\). Hence, weve found the distribution of \\(T\\) Let \\(E\\Big(\\frac{g(T)}{\\tau(\\theta)}\\Big) = 1\\), which implies \\[ E\\Big(\\frac{g(T)}{\\tau(\\theta)}\\Big) = \\int_0^\\infty\\frac{g(t)}{\\lambda^k}\\cdot\\frac{1}{\\Gamma(n)\\lambda^n}t^{n-1}\\exp(-\\frac{t}{\\lambda})dt = 1\\\\ \\implies \\int_0^\\infty g(t)\\cdot\\frac{1}{\\Gamma(n)\\lambda^{n+k}}t^{n-1}\\exp(-\\frac{t}{\\lambda})dt = 1 \\] Clearly this can be converted into a \\(\\text{Gamma}(n+k, \\lambda)\\) pdf by setting \\(g(t)\\) appropriately. Lets algebraically manipulate to insert \\(n+k\\) where it is needed, and keep track of our transformations. Since \\(n\\) and \\(k\\) are integers, \\(\\frac{1}{\\Gamma(n+k)} = \\frac{1}{(n+k)(n+k-1)...(n+1)\\Gamma(n)}\\) \\(t^{n + k -1} = t^{n - 1}\\cdot t^k\\) These transformations give us the \\(\\text{Gamma}(n+k, \\lambda)\\) pdf we need. Let \\(g(T) = \\frac{T^k}{(n+k)(n+k-1)...(n+1)}\\). Then, by the Lehmann Scheffé Corollary, \\(g(T)\\) must be an UMVUE for \\(\\tau(\\lambda)\\), since \\(E(g(T)) = \\lambda^k\\) and \\(T\\) is a complete sufficient statistic. 8.4.5 Proving an UMVUE Does Not Exist 8.5 Inferential Properties of Exponential Families Distributions Suppose we draw a sample of \\(n\\) iid random variables \\(X_1,...,X_n\\) following one of the distributions below. The proceeding tables list the inferential properties of this sample. 8.5.1 Bernoulli Log-likelihood \\(\\ell(\\theta|X) = n\\log(1-p) + \\log(\\frac{p}{1-p})\\sum_{i=1}^n x_i\\) Score Equations \\(U_n(\\theta|X) = -\\frac{n}{1-p} + \\frac{1}{p(1-p)}\\sum_{i=1}^n x_i\\) Observed Information \\(-\\frac{\\partial^2}{\\partial\\theta^2}\\ell(\\theta|X) = \\frac{n}{(1-p)^2} + \\frac{1-2p}{p^2(1-p)^2}\\sum_{i=1}^n x_i\\) Fisher Information \\(I(\\theta)= \\frac{n}{p(1-p)}\\) MLE \\(\\frac{1}{n}\\sum_{i=1}^n x_i\\) 8.5.2 Binomial Since the Binomial has \\(n\\) as a parameter, notation in problems that involve a sample of \\(n\\) iid Binomial random variables can be tricky. To clarify, in the following table let \\(X_i \\sim \\text{Binomial}(m, p)\\), and let \\(n\\) represent the number of samples. Log-likelihood \\(\\ell(\\theta|X) = nm\\log(1-p) + \\log(\\frac{p}{1-p})\\sum_{i=1}^n x_i + \\log({m\\choose x_i})\\) Score Equations \\(U_n(\\theta|X) = -\\frac{nm}{1-p} + \\frac{1}{p(1-p)}\\sum_{i=1}^n x_i\\) Observed Information \\(-\\frac{\\partial^2}{\\partial\\theta^2}\\ell(\\theta|X) = \\frac{nm}{(1-p)^2} + \\frac{1-2p}{p^2(1-p)^2}\\sum_{i=1}^n x_i\\) Fisher Information \\(I(\\theta)= \\frac{nm}{p(1-p)}\\) MLE \\(\\frac{1}{nm}\\sum_{i=1}^n x_i\\) 8.5.3 Geometric Log-likelihood \\(\\ell(\\theta|X) = n\\log(p) + \\log(1-p)\\sum_{i=1}^n x_i\\) Score Equations \\(U_n(\\theta|X) = \\frac{n}{p} - \\frac{1}{1-p}\\sum_{i=1}^n x_i\\) Observed Information Fisher Information MLE \\(\\frac{n}{\\sum_{i=1}^n x_i}\\) 8.5.4 Negative Binomial Log-likelihood \\(\\ell(\\theta|X) = nr\\log(\\frac{p}{1-p}) + \\sum_{i=1}^n x_i\\log(1-p) + \\log{x_i + r - 1\\choose x_i}\\) Score Equations \\(U_n(\\theta|X) = -\\frac{nr}{p} - \\frac{1}{1-p}\\sum_{i=1}^n x_i\\) Observed Information Fisher Information \\(\\frac{r}{(1-p)^2p}\\) MLE \\(\\frac{1}{1 - \\frac{1}{nr}\\sum_{i=1}^nx_i}\\) 8.5.5 Poisson Log-likelihood \\(\\ell(\\theta|X) = n\\lambda + \\sum_{i=1}^n x_i\\log(\\lambda) - \\log(x_i!)\\) Score Equations \\(U_n(\\theta|X) = -n + \\frac{1}{\\lambda}x_i\\) Observed Information Fisher Information \\(\\frac{1}{\\lambda}\\) MLE \\(\\frac{1}{n}\\sum_{i=1}^nx_i\\) 8.5.6 Normal Log-likelihood \\(\\ell(\\theta|X) = -\\frac{n}{2}\\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2}\\sum_{i=1}^n (x_i - \\mu)^2\\) Score Equations \\(U(\\mu | x, \\sigma^2) = -n\\mu -\\frac{1}{\\sigma^2}\\sum_{i=1}^n x_i \\\\ U(\\sigma^2 | x, \\mu) = -\\frac{n}{2\\sigma^2} + \\frac{1}{2(\\sigma^2)^2}\\sum_{i=1}^n(x_i-\\mu)^2\\) Observed Information Fisher Information \\(\\begin{bmatrix}\\frac{1}{\\sigma^2} &amp; 0 \\\\ 0 &amp; \\frac{1}{2\\sigma^4}\\end{bmatrix}\\) MLE \\(\\frac{1}{n}\\sum_{i=1}^nx_i\\) 8.5.7 Exponential Log-likelihood \\(\\ell(\\theta|X) = -n\\log(\\lambda) - \\frac{1}{\\lambda}\\sum_{i=1}^n x_i\\) Score Equations \\(U(\\mu | x, \\sigma^2) = -\\frac{n}{\\lambda} + \\frac{1}{\\lambda^2}\\sum_{i=1}^n x_i\\) Observed Information Fisher Information \\(\\lambda^2\\) MLE \\(\\frac{1}{n}\\sum_{i=1}^nx_i\\) 8.5.8 Gamma Log-likelihood \\(\\ell(\\theta|X) = -n\\log(\\Gamma(k)) - nk\\log(\\lambda) + (k - 1 - \\frac{1}{\\lambda})\\sum_{i=1}^n x_i\\) Score Equations \\(U(\\mu | x, \\sigma^2) = -\\frac{nk}{\\lambda} + \\frac{1}{\\lambda^2}\\sum_{i=1}^n x_i\\) with \\(k\\) known (otherwise, requires differentiating \\(\\Gamma(k)\\)) Observed Information Fisher Information \\(\\begin{bmatrix}\\psi^{(1)}(k) &amp; \\frac{1}{\\lambda}\\\\ \\frac{1}{\\lambda} &amp; \\frac{k}{\\lambda^2}\\end{bmatrix}\\) MLE 8.5.9 Pareto Log-likelihood \\(\\ell(\\theta|X) = n\\log{\\alpha} + n\\alpha\\log(x_m) - (\\alpha + 1)\\sum_{i=1}^n\\log(x_i)\\) Score Equations \\(U(\\mu | x, \\sigma^2) = \\frac{n}{\\alpha} + n\\log(x_m) - \\sum_{i=1}^n \\log(x_i)\\) Observed Information Fisher Information \\(\\frac{n}{\\alpha^2}\\) MLE "],["point-estimators-asymptotics.html", "Chapter 9 Point Estimators: Asymptotics 9.1 Consistency 9.2 Asymptotic Efficiency 9.3 Asymptotic Properties of MLEs 9.4 Variance Stabilizing Transformations 9.5 Asymptotic Confidence Intervals", " Chapter 9 Point Estimators: Asymptotics Evaluating the asymptotic properties of point estimators. 9.1 Consistency An important asymptotic property of an estimator is that it converges in probability to the true value being estimated as \\(n \\rightarrow \\infty\\). This is called consistency. It is generally the most basic asymptotic property that an estimator can have. 9.1.1 Technique: Weak Law of Large Numbers The most common strategy for proving consistency is to use the Weak Law of Large Numbers. Theorem 9.1 (Weak Law of Large Numbers) iid version: If \\(Z_i\\) are iid with finite mean, then \\[\\bar{X} = \\frac{1}{n}\\sum_{i=1}^nX_i\\overset{p}{\\rightarrow}E(X_i)\\] Non-iid version: Alternatively, we can drop the iid assumption; if \\(X_i\\) have finite mean and variance, \\(Cov(X_i, X_j) = 0\\), and \\(\\lim_{n\\rightarrow\\infty}\\sum_{i=1}^n\\frac{\\sigma_i^2}{n^2} = 0\\), then \\[\\frac{1}{n}\\sum_{i=1}^nX_i - \\frac{1}{n}\\sum_{i=1}^nE(X_i) \\overset{\\mathcal{p}}{\\rightarrow} 0\\] This theorem guarantees that Method of Moments estimators are consistent. One hint that the WLLN (or later, the Central Limit Theorem) may be employed is that the expression to be proven contains a sum. If this is true, you can multiply and divide by \\(n\\) to form a 9.1.2 Technique: Direct Proof via Convergence in Probability Sometimes, a statistic may not consistent of a sum of random variables. This often arises in the case of order statistics. In this case, the definition of convergence in probability must be used directly. To do this, a variety of arguments may be employed. Example 9.1 (How To Prove Convergence of a Sample Maximum) Let \\(X_i \\overset{iid}{\\sim} U(a, b)\\). We can prove that \\(X_{(n)}\\overset{p}{\\rightarrow}b\\) via a direct probability argument using disjointification. By the definition of the order statistic cdf, \\[ P(|X_{(n)} - b| &gt; \\varepsilon) = P(X_{(n)} &gt; \\varepsilon + b) + P(X_{(n)} &lt; b - \\varepsilon) \\\\ = 0 + (F_{X_i}(x))^n = (\\frac{b - \\varepsilon - a}{b - a})^n = (1 -\\frac{\\varepsilon}{b - a})^n \\] The 0 arises because \\(X_{(n)} &lt; b\\) by the definition of the Uniform. As \\(\\varepsilon\\) is taken to be small, \\(\\lim_{n\\rightarrow \\infty}(1 -\\frac{\\varepsilon}{b - a})^n = 0\\). Therefore, since \\(P(|X_{(n)} - b| &gt; \\varepsilon) = 0\\), we have proven \\(X_{(n)}\\overset{p}{\\rightarrow}b\\) 9.1.3 Technique: Continuous Mapping Theorem. If you can prove that a particular estimator \\(T\\) converges in probability to some value not equal to \\(\\theta\\), chances are that some transformation can be applied to make it consistent by the Continuous Mapping Theorem. In fact, any continuous function of a consistent estimator is also consistent; that is \\(T_n(X) \\overset{p}{\\rightarrow} \\theta\\), then \\(g(T_n(X)) \\overset{p}{\\rightarrow} g(\\theta)\\) For example, if \\(T_n(X) \\overset{p}{\\rightarrow} \\frac{\\theta + \\zeta_1}{\\zeta_2}\\), then by the Continuous Mapping Theorem, \\(W_n(X) = \\zeta_2T(X) - \\zeta_1\\overset{p}{\\rightarrow}\\theta\\). As such, one strategy to finding a consistent estimator is to start with a sample mean, which is consistent by the WLLN, then transform it to obtain the desired result. 9.2 Asymptotic Efficiency In addition to checking that an estimator converges to the correct value, we are also often concerned with the estimators variance as \\(n \\rightarrow \\infty\\). Often, estimators converge asymptotically to a normal distribution. If an estimator \\(T_n(X)\\) has the property \\(k_n(T_n(X) - \\tau(\\theta)) \\overset{\\mathcal{D}}{\\rightarrow} N(0,\\sigma^2)\\), then \\(\\sigma^2\\) is called the asymptotic variance (Casella and Berger 1990). Furthermore, \\(T_n(X)\\) is asymptotically efficient* if \\(\\sigma^2\\) achieves the Cramer-Rao Lower Bound. If \\(T_n(X)\\) is one-dimensional, then the asymptotic efficiency** of \\(T_n(X)\\) can be computed as the ratio \\[AE(\\theta, T_n) = \\frac{(\\tau&#39;(\\theta))^2}{\\mathcal{I}(\\theta)\\sigma^2}\\] If \\(T_n(X)\\) is \\(k\\)-dimensional, let \\(d = \\begin{bmatrix}\\frac{\\partial}{\\partial\\theta_1}\\tau(\\theta),...,\\frac{\\partial}{\\partial\\theta_k}\\tau(\\theta)\\end{bmatrix}\\). Then, the asymptotic efficiency is \\[AE(\\theta, T_n) = \\frac{d&#39;\\mathcal{I}(\\theta)^{-1}d}{\\sigma^2}\\] We can also compare estimators via their asymptotic relative efficiency (ARE), which for estimators \\(S_n\\) and \\(T_n\\) is \\[ARE(\\theta, S_n, T_n) = \\frac{\\sigma_T^2}{\\sigma_S^2}\\] Finally, \\(T_n(X)\\) is said to be asymptotically normal if \\(\\sqrt{n}(\\hat{\\theta}_n - \\theta_0) \\overset{\\mathcal{D}}{\\rightarrow} N(0, I(\\theta_0))\\). Lets explore first how we might prove that an estimator converges asymptotically to a normal distribution to show this directly. 9.2.1 Central Limit Theorems These theorems are used to show asymptotic normality. There are many different types; let us focus on the three most common. Theorem 9.2 (Central Limit Theorem (iid)) If \\(X_i\\) are iid with finite first and second moments (\\(E(X_i), Var(X_i) &lt; \\infty\\)) then \\[\\sqrt{n}\\Big(\\frac{1}{n}\\sum_{i=1}^nX_i - E(X_i)\\Big) \\overset{\\mathcal{D}}{\\rightarrow} N(0,Var(X_i))\\] Theorem 9.2 (Lyapunov's Central Limit Theorem (non-iid)) Consider \\(X_i\\) that are independent with finite first and second moments (\\(E(X_i), Var(X_i) &lt; \\infty\\)). Let \\(S_n^2 = \\sum_{i=1}^n Var(X_i)\\). Then, for some \\(\\delta &gt; 0\\), if the condition \\[\\lim_{n\\rightarrow \\infty} \\frac{1}{S_n^{2 + \\delta}}\\sum_{i=1}^n E(|X_i - E(X_i)|^{2 + \\delta}) = 0\\] holds, then we know \\[\\frac{1}{S_n}\\sum_{i=1}^n (X_i - E(X_i)) \\overset{\\mathcal{D}}{\\rightarrow} N(0,1)\\] In practice, to prove that Lyapunovs CLT holds true, we typically take \\(\\delta = 1\\) and compute the third moments contained in the Lyapunov condition. Alternatively, we can use Lindebergs CLT for non-iid data: Theorem 9.2 (Lindeberg's Central Limit Theorem (non-iid)) Like the Lyapunov CLT, consider \\(X_i\\) that are independent with finite first and second moments (\\(E(X_i), Var(X_i) &lt; \\infty\\)). Let \\(S_n^2 = \\sum_{i=1}^n Var(X_i)\\). Then if \\[\\lim_{n\\rightarrow \\infty} \\frac{1}{S_n^{2}}\\sum_{i=1}^n E((X_i - E(X_i))^{2})\\cdot I(|X_i - E(X_i) &gt; \\varepsilon S_n) = 0\\] holds, then we know \\[\\frac{1}{S_n}\\sum_{i=1}^n (X_i - E(X_i)) \\overset{\\mathcal{D}}{\\rightarrow} N(0,1)\\] 9.2.2 The Delta Method What if our estimator is not a sample mean? In this case, if it is a function of a sample mean, we can still use the Delta Method to prove that it converges asymptotically to either a Normal or a \\(\\chi^2\\) distribution. Theorem 9.3 (Delta Method) If \\(\\sqrt{n}(\\hat{\\theta}_n - \\theta) \\overset{\\mathcal{D}}{\\rightarrow} N(0,\\sigma^2)\\), then for a continuous \\(g\\) with continuous nonzero derivative in an interval containing \\(\\theta\\) \\(\\sqrt{n}\\Big(g(\\hat{\\theta}_n) - g(\\theta)\\Big) \\overset{\\mathcal{D}}{\\rightarrow} N\\Big(0, \\sigma^2\\cdot (\\frac{d}{d\\theta}g(\\theta))^2\\Big)\\) (First-Order Delta Method) \\(n\\Big(g(\\hat{\\theta}_n) - g(\\theta)\\Big) \\overset{\\mathcal{D}}{\\rightarrow} \\frac{1}{2}\\sigma^2\\cdot g&#39;&#39;(\\theta)\\cdot\\chi^2(1)\\) (Second-Order Delta Method) Generally, the Second-Order Delta Method is required if \\(g&#39;(\\theta) = 0\\). Theorem 9.4 (Multivariate Delta Method) If \\(\\sqrt{n}(\\hat{\\theta}_n - \\theta) \\overset{\\mathcal{D}}{\\rightarrow} N(0,V)\\), where \\(V\\) is a \\(k \\times k\\) matrix, then for a continuous real-valued function \\(g(x)\\) of \\(k\\) variables with continuous nonzero first partial derivatives, \\[\\sqrt{n}\\Big(g(\\hat{\\theta}_n) - g(\\theta)\\Big) \\overset{\\mathcal{D}}{\\rightarrow} MVN_k\\Big(0, u&#39;Vu\\Big)\\] where \\(u = \\begin{bmatrix}\\frac{\\partial}{\\partial \\theta_1}g(\\theta),...,\\frac{\\partial}{\\partial \\theta_k}g(\\theta)\\end{bmatrix}\\) 9.2.3 Cramer-Wold Device One final technique for proving convergence in distribution is to use the Cramer-Wold Device. Theorem 9.5 (Cramer-Wold Device) For a sequence of random vectors \\(X_n\\), a random vector \\(X\\), and a vector \\(a\\) of constants, \\[X_n \\overset{\\mathcal{D}}{\\rightarrow} X \\iff a&#39;X_n \\overset{\\mathcal{D}}{\\rightarrow}a&#39;X, \\forall a\\] In other words, we can show convergence in distribution of a random vector by showing that every linear combination of that random vector converges in distribution to the same linear combination of \\(X\\). The Cramer-Wold Device allows us to convert a problem involving convergence of random vectors into a problem involving convergence of a single random variable. ::: {.example name=Using the Cramer-Wold Device} Suppose \\(X_{1n} \\overset{\\mathcal{D}}{\\rightarrow} X_1 \\sim N(\\mu_1, \\sigma^2_1)\\) and \\(X_{2n} \\overset{\\mathcal{D}}{\\rightarrow} X_2 \\sim N(\\mu_2, \\sigma^2_2)\\). Then, by the Cramer-Wold Device, \\((X_{1n}, X_{2n}) \\overset{\\mathcal{D}}{\\rightarrow} MVN(\\begin{bmatrix}\\mu_1 \\\\ \\mu_2\\end{bmatrix}, \\Sigma)\\) where \\[\\Sigma = \\begin{bmatrix}\\sigma_1^2 &amp; \\rho\\\\\\rho &amp; \\sigma_2^2\\end{bmatrix}\\] where \\(\\rho = Cov(X_1, X_2)\\). Proof: To use the Cramer-Wold Device, we need to hypothesize a distribution to which the vector \\((X_{1n}, X_{2n})\\) will converge. Fortunately, were given one - \\(MVN(\\begin{bmatrix}\\mu_1 \\\\ \\mu_2\\end{bmatrix}, \\Sigma)\\). Now, we show what happens when we apply an arbitrary linear transformation. By additivity of the normal, \\[ a_1X_1 + a_2X_2 = N(a_1\\mu_1 + a_2\\mu_2, a_1^2\\sigma_1^2 + a_2^2\\sigma_2^2 + 2\\rho a_1a_2\\sigma_1\\sigma_2)\\\\ = \\begin{bmatrix}a_1 \\\\ a_2\\end{bmatrix}\\cdot MVN(\\begin{bmatrix}\\mu_1 \\\\ \\mu_2\\end{bmatrix}, \\Sigma) \\] By the Cramer-Wold Device, this implies that \\((X_{1n}, X_{2n}) \\overset{\\mathcal{D}}{\\rightarrow} MVN(\\begin{bmatrix}\\mu_1 \\\\ \\mu_2\\end{bmatrix}, \\Sigma)\\) ::: The above example can be generalized to a \\(k\\)-vector of random variables \\(X_i\\) which converge in distribution to a normal. 9.2.4 Asymptotic Distribution in Practice. Statistical problems are often more complicated than simply applying the CLT or Delta Method. Often, we may need to consider the convergences of other random variables. To do this, we can combine the CLT/Delta Method with Slutskys Theorem or the Continuous Mapping Theorem to prove desired results. Or, we may need to employ a more direct proof. Here are some examples. 9.3 Asymptotic Properties of MLEs While the CLT and Delta Method are extremely useful, if you are working with MLEs, it can often be faster to rely on their known properties. For \\(X_i\\) satisfying a certain set of regularity conditions (where? better link), the MLE $_n has the two import properties discussed above: Consistency: \\(\\hat{\\theta}_n \\overset{p}{\\rightarrow} \\theta\\) Asymptotic Efficiency: \\(\\sqrt{n}(\\hat{\\theta}_n - \\theta) \\overset{\\mathcal{D}}{\\rightarrow} N(0, \\mathcal{I}(\\theta_0)^{-1})\\), the Cramer-Rao Lower Bound This holds true even for multi-parameter MLEs. Note that the regularity conditions are met by the MLE of all exponential families for which \\(\\nu \\in \\Theta \\subset \\mathbb{R}\\) is an open set, which can be useful in problem-solving. Estimating \\(\\mathcal{I}(\\theta_0)\\) can be performed in two ways: (is this correct?) \\(-\\frac{1}{n} \\frac{\\partial^2}{\\partial\\theta^2}\\ell(\\hat{\\theta}_n | X_n) \\overset{p}{\\rightarrow} \\mathcal{I}(\\theta_0)\\) \\(\\frac{1}{n} I_n(\\hat{\\theta}_n) \\overset{p}{\\rightarrow} \\mathcal{I}(\\theta_0)\\) 9.4 Variance Stabilizing Transformations Sometimes, the asymptotic variance of a distribution may depend on the value of a particular parameter. This may be undesirable because the parameter might be unknown. In this case, we can perform a variance-stabilizing transformation to remove the variance. This is done by setting \\(f&#39;(\\theta) = \\frac{c}{\\tau(\\theta)}\\). Then, for \\(T_n(X)\\) such that \\(\\sqrt{n}(T_n - \\theta) \\overset{\\mathcal{D}}{\\rightarrow} N(0, \\tau^2(\\theta))\\), by the Delta Method we get \\[\\sqrt{n}(f(T_n) - f(\\theta)) \\overset{\\mathcal{D}}{\\rightarrow} N(0, f&#39;(\\theta)^2 \\cdot\\tau^2(\\theta)) = N(0, c)\\] 9.5 Asymptotic Confidence Intervals References "],["hypothesis-tests-finite-samples.html", "Chapter 10 Hypothesis Tests: Finite Samples 10.1 Constructing a Test 10.2 Likelihood Ratio Tests 10.3 Power 10.4 How to Find Optimal Tests 10.5 Nuisance Parameters", " Chapter 10 Hypothesis Tests: Finite Samples Scientists often formulate scientific hypotheses about the world. Subsequently, statisticians are called upon to determine whether these scientific hypotheses are supported by the data at hand. As statisticians, how do we do this? Here, we prove a simple procedure: 10.1 Constructing a Test Step 1: Determine Hypotheses. To start, we first formulate statistical hypotheses, which are mathematical statements about a population parameter. These hypotheses come in pairs: one null hypothesis, \\(H_0\\), and one alternative hypothesis, \\(H_1\\). We write these \\[H_0: \\theta \\in \\Theta_0 \\text{ and } H_1: \\theta \\in \\Theta_1\\] Commonly, the sets \\(\\Theta_0\\) and \\(\\Theta_1\\) are just (in)equality statements. A simple hypothesis is one where \\(\\theta = \\theta_0\\), where \\(\\theta_0\\) is known to be some fixed value. A composite hypothesis is one where \\(\\theta\\) is not known to be a fixed value, but rather is in some range. Examples of composite hypotheses include \\(\\theta &lt; \\theta_0\\) and \\(\\theta &gt; \\theta_0\\) (one-sided hypotheses), as well as \\(\\theta \\neq \\theta_0\\) (a two-sided hypothesis). In any case, it must be true that \\(\\Theta_0 \\cap \\Theta_1 = \\emptyset\\). This guarantees that only one of the hypotheses can be true. In addition, we assume the sample sample of \\(\\theta\\) is \\(\\Theta = \\Theta_0 \\cup \\Theta_1\\). In a problem, the hypotheses are often given - our job is to construct a test, which is a decision rule telling us which hypotheses we ought to accept. Step 2: Determine test statistic and its rejection region. How do we determine which hypothesis is correct? To this, we specify two components: The test statistic, a function to summarize the data. The test statistic is analogous to an estimator in Chapter 8 - it should be selected on the basis of certain properties, which we discuss shortly. The rejection region, a set of values of the test statistic for which the null hypothesis \\(H_0\\) is rejected. The rejection region constitutes our decision rule. For a given test statistic \\(T\\), it is commonly expressed in two ways: Directly as a set, like so: \\(\\{T &gt; c\\}\\), where \\(c\\) is a constant. As an indicator function: \\(\\phi(T) = I(T &gt; c)\\), which outputs 1 if we reject the null \\(H_0\\) and 0 if we reject the alternative \\(H_1\\). Step 3: Compute the rejection region for a given \\(\\alpha\\). For a rejection region \\(\\{T &gt; c\\}\\), the goal is to set \\(c\\) such that we control \\(\\alpha\\), the maximum probability of wrongly rejecting \\(H_0\\). This is discussed in the Power section below. Accomplishing all three of these steps yields a valid hypothesis test! Now lets delve more deeply into how exactly each step is accomplished. 10.2 Likelihood Ratio Tests Firstly, how do we find a valid test statistic? In some cases, the test statistic may be given. In others, we can use the likelihood ratio test (LRT), which is analogous to the MLE for point estimators. The likelihood ratio test statistic for \\(H_0: \\theta \\in \\Theta_0\\) is \\[\\lambda(x) = \\frac{\\sup_{\\theta\\in\\Theta_0}\\mathcal{L}(\\theta|x)}{\\sup_{\\theta\\in\\Theta}\\mathcal{L}(\\theta|x)}\\] aka, the ratio of the likelihood under the null to the full likelihood. Then, the likelihood ratio test rejection region is \\[R = \\{\\lambda(x) \\leq c\\}\\] where \\(0 &lt; c &lt; 1\\). The easiest way to compute \\(\\lambda(x)\\) is to do the following: Find the MLE of \\(\\theta\\), call it \\(\\hat{\\theta}\\). Find the restricted MLE, such that \\(\\theta\\in\\Theta_0\\), call it \\(\\hat{\\theta}_0\\). Plug the MLE and restricted MLE into the likelihood, effectively computing the ratio \\[\\lambda(x) = \\frac{\\mathcal{L}(\\hat{\\theta}_0|x)}{\\mathcal{L}(\\hat{\\theta}|x)}\\] This yields the LRT. Another potentially simpler way to find the LRT is to use a sufficient statistic \\(T(X)\\). By the Factorization Theorem, \\(\\mathcal{L}(\\theta|x) = f_X(x|\\theta) = f_{T(X)}(t|\\theta)\\cdot h(x)\\). In this case, \\[\\lambda(x) = \\frac{\\sup_{\\theta\\in\\Theta_0}\\mathcal{L}(\\theta|x)}{\\sup_{\\theta\\in\\Theta}\\mathcal{L}(\\theta|x)} = \\frac{\\sup_{\\theta\\in\\Theta_0}f_{T(X)}(t|\\theta)\\cdot h(x)}{\\sup_{\\theta\\in\\Theta}f_{T(X)}(t|\\theta)\\cdot h(x)} = \\frac{\\sup_{\\theta\\in\\Theta_0}\\mathcal{L}(\\theta|t)}{\\sup_{\\theta\\in\\Theta}\\mathcal{L}(\\theta|t)}\\] Therefore, if we know the distribution of a sufficient statistic \\(T\\), we can construct an LRT based on \\(T\\) instead of \\(X\\). This can be very useful if \\(X\\) is known to be an exponential family or otherwise admits an easy-to-find sufficient statistic. 10.3 Power Once we have the LRT or some other statistic, we need to find \\(c\\) in the rejection region \\(R = \\{T(X) \\leq c\\}\\) (or \\(\\{T(X) \\geq c\\}\\), depending on the hypothesis). This is done by controlling error rates. There are two types of error: Type I: When \\(\\theta \\in \\Theta_0\\), but the test incorrectly rejects \\(H_0\\). Type II: When \\(\\theta \\in \\Theta_1\\), but the test incorrectly rejects \\(H_1\\) (or equivalently, accepts \\(H_0\\)) Here is a nifty table summarizing the errors: Truth Accept \\(H_0\\) Reject \\(H_0\\) \\(H_0\\)  Type I \\(H_1\\) Type II  If youre familiar with the childrens fable The Boy Who Cried Wolf, one way to remember which error is which is via the mnemonic: When the boy cried wolf, the villagers committed a Type I and a Type II error, in that order. Back to statistics! These two error rates are summarized by the tests power. The power function of a test is \\[\\beta(\\theta) = P(T \\in R | \\theta) = \\begin{cases}P(\\text{Type I error}) &amp; \\text{if } \\theta \\in \\Theta_0\\\\ 1 - P(\\text{Type II error}) &amp; \\text{if } \\theta \\in \\Theta_1\\end{cases}\\] When choosing \\(c\\) in the hypothesis test, we want to control these errors. The most common way of doing so is by specifying the size, \\(\\alpha\\), of the test, defined as: \\[\\alpha = \\sup_{\\theta \\in \\Theta_0}\\beta(\\theta)\\] Hence, the size measures the maximum type I error that we may commit when \\(H_0\\) is true. When \\(X\\) is discrete, sometimes consider the level , which is defined analogously as \\[\\alpha \\leq \\sup_{\\theta \\in \\Theta_0}\\beta(\\theta)\\] Level \\(\\alpha\\) is essentially the same as size \\(\\alpha\\), but conservative. It accounts for the fact that when \\(X\\) is discrete, we may not be able to set \\(c\\) to achieve an exact \\(\\alpha\\). We may also be asked to consider the maximum Type II error, which is defined analogously assuming \\(H_1\\) is true instead as \\[\\alpha = \\sup_{\\theta \\in \\Theta_1}\\beta(\\theta)\\] How might we do this in practice? Lets see an example. ::: {.example name=Finding an LRT and its Power Function} FORTHCOMING ::: 10.4 How to Find Optimal Tests In Chapter 8, we discussed how construction of a point estimator must consider two properties - unbiasedness and minimum variance - and how the ultimate goal is to construct an UMVUE. Hypothesis tests have analogous properties. In this section, we define these properties and show how to achieve them. 10.4.1 Properties Unbiased: A test is said to be unbiased if, for all \\(\\theta_1 \\in \\Theta_1\\) and \\(\\theta_0 \\in \\Theta_0\\), \\[\\beta(\\theta_1) &gt; \\beta(\\theta_0)\\] or, equivalently, \\[\\inf_{\\theta\\in\\Theta_1} \\beta(\\theta) \\geq \\sup_{\\theta\\in\\Theta_0}\\beta(\\theta)\\] Intuitively, this means that an unbiased test, even in the worst-case scenario, has a higher probability of rejecting \\(H_0\\) when it is actually false than rejecting \\(H_0\\) when it is true. Unbiasedness can be proven directly from the power function. Most Powerful Consider the set \\(C\\) of all level \\(\\alpha\\) tests for a given \\(\\theta_1 \\in \\Theta_1\\). Among these tests, \\(W\\) is considered the most powerful (MP) if \\[\\beta_W(\\theta_1) \\geq \\beta_{W^*}(\\theta_1)\\] for all \\(W^* \\in C\\). Intuitively, a most powerful test has the highest probability of correctly rejecting \\(H_0\\) when \\(H_0\\) is actually false (considering the worst-case scenario across all other possible tests). An MP test is defined for a specific \\(\\theta_1\\). Uniformly Most Powerful Usually, we care about a tests performance across all \\(\\theta_1 \\in \\Theta_1\\). A test is considered uniformly most powerful (UMP) if it is the most powerful for all \\(\\theta_1 \\in \\Theta_1\\). In other words, for a class \\(C\\) of level \\(\\alpha\\) tests, if \\(\\beta_W(\\theta) \\geq \\beta_{W^*}(\\theta)\\) for all \\(\\theta \\in \\Theta_1\\) and \\(W^* \\in C\\), then \\(W\\) is the UMP test. Locally Most Powerful Sometimes it may not be possible to find a UMP test. In this case, we can consider Locally Most Powerful (LMP) tests. FORTHCOMING 10.4.2 Optimality for Simple Hypotheses: The Neyman-Pearson Lemma The Neyman-Pearson Lemma defines a UMP test when both the null and alternative hypotheses are simple. Lemma 10.1 (Neyman-Pearson Lemma) Suppose we have a set of simple hypotheses: \\[H_0: \\theta = \\theta_0, H_1: \\theta = \\theta_1\\] For \\(X \\sim f(x|\\theta)\\), let the rejection region be \\[R = \\Big\\{\\frac{f(x|\\theta_1)}{f(x|\\theta_0)} &gt; k\\Big\\}\\] with \\(k \\geq 0\\). Then, The test with rejection region \\(R\\) is UMP level \\(\\alpha = P(X \\in R | \\theta = \\theta_0)\\) (Sufficiency) Every UMP level \\(\\alpha\\) test satisfies the above and is size \\(\\alpha\\) (except on a set with probability 0) (Necessary). In addition, a corollary to the Neyman-Pearson lemma states that we can also construct such a test based on a sufficient statistic, using the Factorization theorem on both the numerator and the denominator to cancel out the extraneous \\(h(x)\\). Then, \\[R = \\Big\\{\\frac{f(t|\\theta_1)}{f(t|\\theta_0)} &gt; k\\Big\\}\\] is an UMP level \\(\\alpha\\) test. 10.4.3 Optimality for One-Sided Hypotheses: The Karlin-Rubin Theorem It is more common to see one-sided tests. The Karlin-Rubin Theorem defines a UMP test when both the null and alternative hypotheses are one-sided composite. Theorem 10.1 (Karlin-Rubin Theorem) Suppose \\(H_0: \\theta \\leq \\theta_0\\) and \\(H_1: \\theta &gt; \\theta_0\\). Then, if \\(T\\) is a sufficient statistic \\(T\\) has a monotone likelihood ratio \\(P(T &gt; t_0 |\\theta_0) = \\alpha\\) then the test with rejection region \\(R = \\Big\\{T &gt; t_0 \\Big\\}\\) is the UMP level \\(\\alpha\\) test of \\(H_0\\) versus \\(H_1\\). This theorem follows from the Neyman-Pearson corollary. As a result, finding an UMP test typically amounts to identifying a sufficient statistic, and proving that it has a monotone likelihood ratio. Definition 10.1 (Monotone Likelihood Ratio) A family \\(f(t|\\theta)\\) has a monotone likelihood ratio if, for all \\(\\theta_1, \\theta_2 \\in \\Theta\\) such that \\(\\theta_1 &lt; \\theta_2\\), \\[\\frac{f(t|\\theta_2)}{f(t|\\theta_1)}\\] is a monotone nondecreasing function of \\(t\\) when either \\(f(t|\\theta_1) &gt;0\\) or \\(f(t|\\theta_0) &gt;0\\) If instead \\(f(t|\\theta)\\) is nonincreasing, use \\(-T\\) instead of \\(T\\) in the Karlin-Rubin theorem to obtain a statistic with a monotone likelihood ratio. Also, note that all 1-parameter exponential families have a monotone likelihood ratio - a convenient property simplifying proofs. 10.4.4 Optimality for Two-Sided Hypotheses. In this section we will show how to construct a UMP test for a two-sided composite hypothesis. Sike! This is actually impossible. No UMP test exists for \\(H_0: \\theta = \\theta_0, H_1: \\theta \\neq \\theta_0\\). For such a test to be UMP, it must be UMP for both \\(\\theta &lt; \\theta_0\\) and for \\(\\theta &gt; \\theta_0\\), which creates a contradiction. 10.5 Nuisance Parameters FORTHCOMING "],["hypothesis-tests-asymptotics.html", "Chapter 11 Hypothesis Tests: Asymptotics 11.1 Wald Test 11.2 Score Test 11.3 Likelihood Ratio Test 11.4 Composite Null Hypotheses", " Chapter 11 Hypothesis Tests: Asymptotics Constructing a finite-sample hypothesis test requires deriving the full distribution of the test statistic, which may be difficult. Oftentimes, however, we can use the Central Limit Theorem and other asymptotic tools to prove that, as \\(n \\rightarrow \\infty\\), a test statistic converges to either a standard normal or chi-squared distribution. This permits the construction of asymptotic hypothesis tests. This section will focus on constructing and evaluating tests based on the MLE \\(\\hat{\\theta}_n\\) when regularity conditions are met under both the null hypothesis \\(H_0\\) and the alternative \\(H_1\\).. In this scenario, there are three possible tests that may be constructed: Wald Test Score Test Likelihood Ratio Test All of these tests are asymptotically equivalent. One-sided tests are generally based on a Normal approximation, while two-sided are based on a \\(\\chi^2\\) approximation. Lets discuss each in detail, including their strengths and weaknesses. 11.1 Wald Test Definition 11.1 (Wald Test) The one-sided, one-dimensional Wald test is constructed based on \\[W_n = (\\hat{\\theta}_n - \\theta_0)(I_n(\\theta_0))^\\frac{1}{2} \\approx N(0,1)\\] The two-sided one-dimensional Wald test is constructed based on the square of this; that is, \\[W_n = (\\hat{\\theta}_n - \\theta_0)^2(I_n(\\theta_0)) \\approx \\chi^2(1)\\] It can be extended to the multidimensional setting by representing \\(\\hat{\\theta}_n\\) and \\(\\theta\\) as \\(k\\)-dimensional vectors and \\(I_n(\\hat{\\theta}_0)\\) a \\(k \\times k\\) matrix. \\[W_n = (\\hat{\\theta}_n - \\theta_0)^\\top(I_n(\\theta_0))(\\hat{\\theta}_n - \\theta_0) \\approx \\chi^2_k\\] which follows from the fact that \\(\\hat{\\theta}_n \\approx MVN_k(\\theta_0, (I_n(\\theta_0))^{-1})\\) To obtain \\(I_n(\\theta_0)\\), we can estimate it using either \\(I_n(\\hat{\\theta}_n)\\); simply plug the MLE into the expected information. \\(-\\frac{\\partial^2}{\\partial\\theta^2}\\ell(\\hat{\\theta}_n | X_n)\\); plug the MLE into the observed information However, \\(I_n(\\hat{\\theta}_0)\\) is generally preferred, because it is more efficient. Advantages: - Simple to compute if you know the form of the MLE - Constructing confidence intervals is easy Disadvantages: - Requires knowing the form of the MLE - Not invariant if you transform the MLE - Approximation may not be as accurate as the other two tests 11.2 Score Test Definition 11.2 (Score Test) The one-sided, one-dimensional Score Test is constructed by \\[S_n = \\Big(\\frac{\\partial}{\\partial \\theta}\\ell(\\theta_0|X_n)\\Big)(I_n(\\theta_0))^{-\\frac{1}{2}}\\] The two-sided, one-dimensional Score test is constructed based on its square: \\[S_n = \\Big(\\frac{\\partial}{\\partial\\theta}\\ell(\\theta_0 | X_n)\\Big)^2(I_n(\\theta_0))^{-1}\\] It can be extended to the multivariate setting by representing \\(U_n(\\theta_0) = \\begin{bmatrix}\\frac{\\partial}{\\partial\\theta_1}\\ell(\\theta|X_n) &amp; ... &amp; \\frac{\\partial}{\\partial\\theta_k}\\ell(\\theta|X_n) \\end{bmatrix}\\) and \\(I_n(\\hat{\\theta}_0)\\) a \\(k \\times k\\) matrix: \\[(U_n(\\theta_0))^\\top(I_n(\\theta_0))^{-1}(U_n(\\theta_0)) \\approx \\chi^2_k\\] which follows from the fact that \\(U_n(\\theta_0) \\approx MVN_k(0, I_n(\\theta_0))\\) Advantages: - Not actually necessary to know the form of the MLE to construct the test - all that is needed is the score equation and information under \\(H_0\\) - More computationally efficient as a result Disadvantages: - Constructing confidence intervals is more complicated, since inverting the Score test statistic may be challenging - Not invariant to under reparametrization 11.3 Likelihood Ratio Test Definition 11.3 (Likelihood Ratio Test) The one-dimensional asymptotic Likelihood Ratio Test (LRT) is constructed based on \\[Q_n = 2\\log\\Big(\\frac{\\mathcal{L}(\\hat{\\theta}_n)}{\\mathcal{L}(\\theta_0)}\\Big) = 2(\\ell(\\hat{\\theta}_n|X_n) - \\ell(\\theta_0|X_n)) \\approx \\chi^2(1)\\] This can be extended to multiple dimensions by letting \\(\\ell(\\theta|X_n)\\) by a \\(k\\)-dimensional vector. Then, \\[Q_n = 2(\\ell(\\hat{\\theta}_n|X_n) - \\ell(\\theta_0|X_n)) \\approx\\chi^2(k)\\] Advantages: - No derivatives need to be calculated - Invariant to transformations of the MLE - Provides the most accurate approximation, especially if we decide to reparametrize the model, in which case the derivative in the Wald/Score tests would need to be recomputed. Disadvantages: - Inverting the test to construct a confidence interval is difficult - Requires knowledge of the log-likelihood under both the null hypothesis \\(H_0\\) and the alternative \\(H_1\\). 11.4 Composite Null Hypotheses 11.4.1 Multiple Parameters What if our hypothesis tests multiple parameters at once? In this case, we must construct a matrix \\(C\\) describing how these parameters may be combined and tested. Let \\(\\theta = \\begin{bmatrix}\\theta_0 &amp; \\theta_1 &amp; \\theta_2 \\end{bmatrix}\\). Suppose we want to test \\(\\theta_0 = \\theta_1 - 2\\theta_2\\). This equates to \\(\\theta_0 - \\theta_1 + 2\\theta_2 = 0\\). Then, we can construct the \\(C\\) matrix as \\[C = \\begin{bmatrix}1 &amp; -1 &amp; 2\\end{bmatrix}\\] And \\(C\\hat{\\theta} \\sim N(0, C(X^\\top X)^{-1}C^\\top\\sigma^2)\\), which allows us to construct a chi-squared test \\[C\\hat{\\theta}^\\top(C(X^\\top X)^{-1}C^\\top)C\\hat{\\theta} / \\sigma^2 \\sim \\chi^2(k)\\] where \\(k\\) is the number of parameters (in this case, \\(k=3\\)) 11.4.2 Nuisance Parameters A composite null hypothesis may instead involve nuisance parameters. Suppose we have a null hypothesis \\(H_0: \\alpha = \\alpha_0\\), where \\(\\alpha\\) is a vector of length \\(k\\), and we have a vector \\(\\beta\\) of nuisance parameters. In a composite null, we can partition \\(\\theta^\\top = (\\alpha^\\top, \\beta^\\top)\\). ::: {#adjusted-information .definition name=Adjusted Information} To use a normal asymptotic approximation in this situation, it is necessary to compute the adjusted information \\(I_{n,\\alpha\\alpha|\\beta}(\\alpha, \\beta)\\) - the information for \\(\\alpha\\), conditional on our \\(\\beta\\) estimate: \\[I_{n,\\alpha\\alpha|\\beta}(\\alpha, \\beta) = I_{n,\\alpha\\alpha}(\\alpha, \\beta) - I_{n,\\alpha\\beta}(\\alpha, \\beta)(I_{n,\\beta\\beta}(\\alpha, \\beta))^{-1}I_{n,\\beta\\alpha}(\\alpha, \\beta)\\] :: The adjusted information is derived from the block-matrix partition formula for the partition \\[I_n(\\alpha, \\beta) = \\begin{bmatrix} I_{n,\\alpha\\alpha}(\\alpha, \\beta) &amp; I_{n,\\alpha\\beta}(\\alpha, \\beta) \\\\ I_{n,\\beta\\alpha}(\\alpha, \\beta) &amp; I_{n,\\beta\\beta}(\\alpha, \\beta) \\\\\\end{bmatrix}\\] With the adjusted information computed, the previously-discussed asymptotic tests become the following: Wald: \\(W_n = (\\hat{\\alpha}_n - \\alpha_0)^\\top I_{n,\\alpha\\alpha|\\beta}(\\hat{\\alpha}_n, \\hat{\\beta}_n)(\\hat{\\alpha}_n - \\alpha_0)\\) Score: \\(S_n = U_{n,\\alpha}(\\alpha_0, \\hat{\\beta}_n)^\\top I_{n,\\alpha\\alpha|\\beta}(\\alpha_0, \\hat{\\beta}_n)U_{n,\\alpha}(\\alpha_0, \\hat{\\beta}_n)\\) Likelihood Ratio: \\(Q_n = 2(\\ell(\\hat{\\alpha}_n, \\hat{\\beta}_n) - \\ell(\\alpha_0, \\hat{\\beta}_n))\\) all of which follow a \\(\\chi^2(k)\\) distribution, where \\(k\\) is the number of parameters being tested in \\(\\alpha\\). "],["generating-random-variables.html", "Chapter 12 Generating Random Variables", " Chapter 12 Generating Random Variables What the title says. "],["random-processes.html", "Chapter 13 Random Processes 13.1 Branching Processes 13.2 Poisson Processes", " Chapter 13 Random Processes A random process is a set or sequence of random variables indexed by time. That is, \\(X(t)\\) or \\(X_t\\) is a random variable indexed by time \\(t\\), or sometimes \\(X(n)\\) or \\(X_n\\) indexed by generation \\(n\\) as in the branching process which we discuss first. Several techniques have been developed to analyze these special types of random variables. 13.1 Branching Processes Also known as a Galton-Watson process, a branching process uses random variables to model population growth. Lets start with notation. 13.1.1 Random Variables Well start by defining the random variables that characterize a branching process. \\(X_n\\) = size of the population at time (or generation) \\(n\\) \\(Y_{i,n-1}\\) = number of offspring produced by the \\(i^{th}\\) individual in generation \\(n-1\\)   How are random variables \\(X_n\\) and \\(Y_{i,n-1}\\) related? Intuitively, we can think of the population size of the current generation (\\(n\\)) as the sum of the offspring produced by each individual in the previous generation (\\(n-1\\)): \\[ \\begin{aligned} X_n &amp;= \\sum_{i=1}^{X_{n-1}} Y_{i,n-1}\\\\ &amp;= Y_{1,n-1} + Y_{2,n-1} +...+Y_{X_{n-1},n-1} \\end{aligned} \\] This is a recursive relation, where the number of summands depends on the previous value \\(X_{n-1}\\). Typically, we assume \\(X_n = 1\\). This recursive property necessiates the use of a special tool to analyze \\(X_n\\): the probability generating function. 13.1.2 Probability Generating Function The Probability Generating Function (PGF) allows us to compute probabilities from branching processes. It is defined as \\[ g_X(t)=\\mathbb{E}_X\\left(t^X\\right)=\\sum_{n=0}^{\\infty} t^n P(X=n) \\] What makes the PGF so useful for branching processes is that its derivative evaluated at 0 yields the probability of that \\(X\\) takes on a particular value - that is, \\[ P(X = k) = \\frac{1}{k!}\\cdot\\frac{d^k}{dt^k}g_X(0) \\] Similar to MGFs, independent random variables can be convoluted by multiplying their PGFs. If \\(Y = X_1 + X_2\\), then \\[ g_Y(t) = g_{X_1}(t)\\cdot g_{X_2}(t) \\] and by extension, if \\(Y = \\sum_{i=1}^nX_i\\) and \\(X_i\\) are iid, then \\[ g_Y(t) = (g_{X_i}(t))^n \\] For i.i.d. \\(Y_{i,j}\\), the PGF also gives the first two moments: \\[ \\begin{aligned} \\mathbb{E}\\left(X_n\\right) &amp; =[\\mathbb{E}(Y)]^n \\\\ \\operatorname{Var}\\left(X_n\\right) &amp; =\\operatorname{Var}(Y)\\left(\\sum_{i=n-1}^{2(n-1)}[\\mathbb{E}(Y)]^i\\right) \\end{aligned} \\]   Asymptotics exists for Branching Processes on i.i.d. \\(\\left\\{Y_{i,j}\\right\\}\\) with finite variance: \\[ \\begin{gathered} \\text{Population-Level} \\quad \\quad \\text{Individual-Level} \\\\ \\\\ \\mathrm{E}\\left(X_n\\right) \\rightarrow \\begin{cases}0 &amp; \\mathrm{E}(Y)&lt;1 \\\\ 1 &amp; \\mathrm{E}(Y)=1 \\\\ \\infty &amp; \\mathrm{E}(Y)&gt;1\\end{cases} \\\\ \\\\ \\operatorname{Var}\\left(X_n\\right) \\rightarrow \\begin{cases}0 &amp; \\mathrm{E}(Y)&lt;1 \\\\ \\infty &amp; \\mathrm{E}(Y) \\geq 1\\end{cases} \\end{gathered} \\] This allows us to analyze the long-run behavior of the branching process as \\(n\\rightarrow \\infty\\) 13.1.3 Finding the PGF of a Branching Process As a result of the recursive property of the branching process, where the number of children produced in generation \\(n\\) depends on the number in \\(n-1\\), it turns out that the PGF of a branching process is \\[g_{X_n}(t) = g_{X_{n-1}}(g_{Y}(t))\\] With this, a number of techniques are possible: We can find the probability that a given generation is of size \\(k\\) by computing the probability generating function of \\(X_n\\) and differentiating, as mentioned above. We may be able to use Mathematical Induction to derive the full PGF of the branching process for arbitrary \\(n\\). We can compute the probability of extinction, as we discuss in the next section 13.1.4 Finding the Probability of Extinction: Criticality Theorem The Criticality Theorem states that the probability of ultimate extinction of a branching process is the smallest solution to \\[\\eta=g_Y(\\eta)\\] In general, we can solve for \\(\\eta\\) by factoring the equation, or by using the quadratic equation: \\[ \\begin{aligned} &amp;0=a\\eta^2+b\\eta+c\\\\ \\\\ &amp;\\eta=\\frac{-b \\pm \\sqrt{b^2-4 a c}}{2 a} \\end{aligned} \\] 13.1.5 Example Consider a Branching Process where individuals duplicate with probability \\(p\\) and die with probability \\(q\\). Describe the mean and variance over time for this branching process. For what values of \\(p\\) will the process go extinct with probability 1? Establish the probability of eventual extinction for arbitrary \\(p\\).   Step 1: Define the pmf of \\(Y\\) based on the given reproduction probabilities. Does \\(Y\\) follow a known distribution? \\(f_y(y)=\\left\\{\\begin{array}{ll}y=0 &amp; \\text { w.p. } q=1-p \\\\ y=2 &amp; \\text { w.p. } p\\end{array} \\quad \\Rightarrow \\quad y \\sim 2\\right.\\) Bernoulli\\((p)\\)   Step 2: Calculate the moments of \\(Y\\), which can be used to calculate the moments of \\(X_n\\). \\[ \\begin{aligned} \\mathbb{E}[Y]=2 p \\quad \\quad \\quad \\quad \\mathbb{E}\\left[X_n\\right] &amp; =(2 p)^n \\\\ \\operatorname{Var}(Y)=4 p q \\quad \\quad \\operatorname{Var}\\left(X_n\\right) &amp; =\\operatorname{Var}(y) \\sum_{i=n-1}^{2(n-1)} \\mathbb{E}[Y]^i \\\\ &amp; =4 p q \\sum_{i=n-1}^{2(n-1)}(2 p)^i \\end{aligned} \\] Based on our asymptotic results, we know that the process \\(X_n\\) will go extinct with probability 1 if \\(\\mathbb{E}[Y]&lt;1\\) \\[ \\mathbb{E}[Y]=2 p \\Rightarrow \\text { process will go extinct with probability 1 if } p&lt; \\frac{1}{2} \\]   Step 3: Find the probability of ultimate extinction using Criticality Theorem. First, find the PGF of \\(Y\\): \\[ \\begin{aligned} g_Y(\\eta) =\\mathbb{E}\\left[\\eta^Y\\right]&amp;=\\sum_Y \\eta^Y f_Y(y) \\\\ &amp; =\\eta^2 P(Y=2)+\\eta^0 P(Y=0) \\\\ &amp; =\\eta^2 p+q \\end{aligned} \\] Second, find the probability of ultimate extinction, which is the smallest solution to \\(\\eta=g_Y(\\eta)\\): \\[ \\begin{aligned} \\eta&amp;=\\eta^2 p+q \\\\ 0&amp;=\\eta^2 p-\\eta+q\\\\ \\\\ a&amp;=p, b=-1, c=q\\\\ \\\\ \\eta&amp;= \\frac{1 \\pm \\sqrt{1-4pq}}{2p} \\\\&amp;= \\frac{1 \\pm \\sqrt{1-4p(1-p)}}{2p} \\\\&amp;= \\frac{1 \\pm \\sqrt{1-4p+4p^2}}{2p} \\\\&amp;= \\frac{1 \\pm \\sqrt{(2p-1)^2}}{2p} \\\\&amp;= \\frac{1 \\pm (2p-1)}{2p}\\\\ &amp;=\\text{min}\\left(1 \\quad \\text{or} \\quad \\frac{1}{p}-1\\right) \\end{aligned} \\] For \\(p \\leq \\frac{1}{2}, 1\\) is the minimum. Thus, \\(\\eta=P(\\text{Extinction})=1.\\) 13.2 Poisson Processes A Poisson Process is a model for a series of discrete events where the average time between events is known, but the exact timing of events is random. A poisson process has the following properties: Events are independent of each other. The average rate (events per time period) is constant. Two events cannot occur at the same time. 13.2.1 Memorylessness of the Exponential Recall that the exponential distribution is memoryless, meaning \\(P(X&gt;x+a \\mid X&gt;a)=P(X&gt;x)\\). The memoryless property of the exponential distribution applies to waiting times in a Poisson process. It means that the time between events remains independent of past events, allowing us to predict future waiting times solely based on the average rate of event occurrences (i.e., the interarrival times between events are i.i.d.). 13.2.2 Count-Time Duality \\[ \\{T_n&gt;t\\}=\\{N_t&lt;n\\} \\] In words, the following 2 statements are equivalent: \\(T_n\\) (time to the \\(n^{th}\\) event) is greater than some fixed time \\(t\\) \\(N_t\\) (number of events up to time \\(t\\)) is less than some fixed number \\(n\\) \\[ \\int_t^{\\infty} \\underbrace{\\frac{1}{\\Gamma(n)\\lambda^{-n}} x^{n-1} e^{-\\lambda x}}_{T_n \\sim \\text{Gamma}(n,\\lambda)} d x \\quad = \\quad \\sum_{x=0}^{n-1} \\underbrace{\\frac{e^{-\\lambda t} (\\lambda t)^x}{x!}}_{N_t \\sim \\text{Poisson}(\\lambda t)} \\] 13.2.3 Poisson Distribution Suppose that we are interested in the expected number of events that will occur over a particular interval. The probability of observing a particular number of events can be modeled using the (discrete) poisson distribution: \\(X=\\) Discrete number of events occurring over a finite interval Moments: \\(\\mathbb{E}[X]=\\lambda\\), \\(\\operatorname{Var}(X)=\\lambda\\) \\(\\lambda=\\) Expected number of events over interval \\(=\\underbrace{\\frac{Events}{Time}}_{Rate}\\times Time\\)   13.2.4 Exponential Distribution Suppose that we are interested in the expected time before the next event. The probability of observing a particular time before the next event can be modeled using the (continuous) exponential distribution: \\(X=\\) Continuous time between events Moments: \\(\\mathbb{E}[X]=\\frac{1}{\\lambda}\\), \\(\\operatorname{Var}(X)=\\frac{1}{\\lambda^2}\\) \\(\\lambda=\\) Rate of events \\(=\\underbrace{\\frac{Events}{Time}}_{Rate}\\) Note: There is an inverse relationship between the rate of events (\\(\\lambda\\)) and expected time before the next event (\\(x\\)). As the rate of events (\\(\\lambda\\)) increases, the time before the next event (\\(x\\)) decreases.     13.2.5 Example Consider a Poisson process \\((\\lambda)\\) with a twist: After every event there is a guaranteed period of length \\(\\nu\\) during which no event can occur. Typical Poisson process: Distribution of time between events: \\(T_n-T_{n-1} \\sim \\operatorname{Exp}(\\lambda)\\) Distribution of time to the \\(n^{th}\\) event: \\(\\sum_{i=1}^n T_i \\sim \\operatorname{Gamma}(n, \\lambda)\\) Poisson process with a twist: Distribution of time between events: \\(T_n-T_{n-1} \\sim \\operatorname{Exp}(\\lambda)+\\nu\\) \\[ f_{T_n-T_{n-1}}(x)=\\lambda e^{-\\lambda(x-\\nu)} \\] Distribution of time to the \\(n^{th}\\) event: \\(\\sum_{i=1}^n T_i \\sim \\operatorname{Gamma}(n, \\lambda)+n \\nu\\) \\[ f_{T_n}(x)=\\frac{1}{\\Gamma(n) \\lambda^{-n}}(x-n \\nu)^{n-1} e^{-\\lambda(x-n \\nu)} \\] Further, by count time duality, we can write: \\[ P(T_n&gt;t)=P(N_t&lt;n) \\] \\[ \\int_t^{\\infty} \\frac{1}{\\Gamma(n)\\lambda^{-n}} (x-n \\nu)^{n-1} e^{-\\lambda (x-n \\nu)} d x \\quad = \\quad \\sum_{x=0}^{n-1} \\frac{e^{-\\lambda (t-n \\nu)} (\\lambda (t-n \\nu))^x}{x!} \\] "],["references.html", "References", " References "]]
