[["index.html", "Solving Statistical Problems Chapter 1 Introduction", " Solving Statistical Problems Salvador Balkus, Kimberly Greco, and Mónica Robles Fontán 2023-06-20 Chapter 1 Introduction Welcome to “Solving Statistical Problems”, a compilation of problem-solving tools and tricks for graduate-level Probability and Statistical Inference courses. This site is currently under construction in preparation for the 2023 Harvard Biostatistics PhD Qualifying Exam, so stay tuned! "],["math-tricks.html", "Chapter 2 Math Tricks 2.1 Combinatorics 2.2 Geometric Series 2.3 Exponential Taylor Series 2.4 Exponential Limit 2.5 Integration by Parts 2.6 Leibniz’s Rule 2.7 Gamma Function 2.8 Triangle Inequality", " Chapter 2 Math Tricks This chapter will cover highly specific mathematical techniques used to solve stats problems but themselves require no statistical knowledge, i.e… Exponential function as limits/series Inductive integration by parts substitution tricks for integration binomial theorem Gamma function properties Matrices (determinants, jacobians, etc.) Taylor Series (univariate and multivariate) Lagrange multipliers? (Maybe save for MLEs) Open versus closed intervals 2.1 Combinatorics 2.2 Geometric Series The geometric series is a useful series convergence, defined as follows: \\[\\sum_{x=0}^\\infty ar^x = \\frac{a}{1-r} \\text{ for } |r| &lt; 1\\] One place it arises is computing the moments of the Geometric Distribution. 2.3 Exponential Taylor Series Some distributions such as the Poisson rely on infinite sums. Often, we can simplify these infinite sums to an exponential by rewriting them using the following series convergence: \\[\\sum_{x=0}^\\infty \\frac{\\lambda^x}{x!} = \\exp(\\lambda)\\] This is the Taylor series for the exponential function, sometimes called the “exponential series”. 2.4 Exponential Limit Another way to express the exponential function is by the limit \\[\\exp(x) = \\lim_{n\\rightarrow \\infty}(1 + \\frac{x}{n})^n\\] 2.5 Integration by Parts Another technique, especially for computing moments, is integration by parts, defined by: \\[\\int udv = uv - \\int vdu\\] Integration by parts is typically used for products of functions. By setting \\(u\\) equal to a function which has a finite number of nonzero derivatives (for example, \\(x^k\\)), and \\(v\\) equal to a function which does not (such as \\(e^x\\)), this technique can be repeatedly applied to compute integration. Of course, take caution: If both functions can be repeatedly differentiated infinitely, this may not work! However, it may be possible to use inductive integration by parts to prove that if we apply integration by parts infinitely, the result will yield a series which converges to some value (where?) 2.6 Leibniz’s Rule Theorem 2.1 (Leibniz's Rule: Simple Version) If \\(a, b\\) are constant and \\(f(x, \\theta)\\) is differentiable w.r.t to \\(\\theta\\), then \\[\\frac{d}{d\\theta} \\int_{a(\\theta)}^{b(\\theta)} f(x,\\theta)dx = \\int_a^b \\frac{\\partial}{\\partial\\theta}f(x,\\theta)dx\\] Theorem 2.2 (Leibniz's Rule: Complicated Version) If \\(a(\\theta)\\), \\(b(\\theta)\\), and \\(f(x, \\theta)\\) are differentiable w.r.t to \\(\\theta\\), then \\[\\int_{a(\\theta)}^{b(\\theta)}f(x,\\theta)dx = f(b(\\theta), \\theta)\\frac{d}{d\\theta}b(\\theta) - f(a(\\theta), \\theta)\\frac{d}{d\\theta}a(\\theta) + \\int_{a(\\theta)}^{b(\\theta)} \\frac{\\partial}{\\partial\\theta}f(x,\\theta)dx\\] 2.7 Gamma Function Defined as \\[\\Gamma(z) = \\int_{0}^\\infty t^{z-1}e^{-t}dt\\] the Gamma function commonly appears in the probability density function of several random variables, including the Gamma (duh!) and the Beta. The most important property of \\(\\Gamma(z)\\) is that \\[\\Gamma(z + 1) = z\\Gamma(z)\\] This is because we can substitute \\(\\Gamma(z) = \\frac{\\Gamma(z + 1)}{z}\\) to perform the Kernel Technique on distributions involving the Gamma function in their pdf As a corollary, when \\(n\\) is an integer, \\(\\Gamma(n) = (n-1)!\\). 2.8 Triangle Inequality The triangle inequality is defined as \\[|x + y| \\leq |x| + |y|\\] with \\(|x + y| \\leq |x| + |y|\\) unless \\(x, y \\geq 0\\). This inequality is useful for proving moment bounds as well as asymptotic convergence in Chapter 9 and Chapter 11 "],["probability.html", "Chapter 3 Probability 3.1 Conditional Probability 3.2 Independence 3.3 Order Statistics 3.4 Convergence", " Chapter 3 Probability This chapter is about how to manipulate probability functions directly 3.1 Conditional Probability Definition 3.1 (Conditional Probability) The probability of event \\(A\\) occurring given that we know event \\(B\\) has occurred is given by \\[P(A|B) = \\frac{P(A\\cap B)}{P(B)}\\] Definition 3.2 (Bayes' Theorem) If we need to invert the order of \\(A\\) and \\(B\\) in a conditional probability express, we can use the property \\[P(A|B) = \\frac{P(B|A)P(A)}{P(B)}\\] More generally, let \\(A_1, A_2, ...\\) be a partition of the sample space and let \\(B\\) be any set \\[P(A_i|B) = \\frac{P(B|A_i)P(A_i)}{\\sum_{j=1}^{\\infty} P(B|A_j)P(A_j)}\\] Definition 3.3 (Law of Total Probability) Let \\(A_1, A_2, ..., A_n\\) be a partition of the sample space and let \\(B\\) be any set. \\[P(B) = \\sum_{i=1}^{n} P(B \\cap A_i) = \\sum_{i=1}^{n} P(B|A_i) P(A_i)\\] 3.1.1 Conditional Probability in practice Public health research often relies on the task of comparison. There are several standard measures of association that are widely used in public health research. Here we present two of them. Definition 3.4 (Relative Risk, RR) Let \\(D\\) be an outcome event and \\(S\\) be an exposure event. The relative risk (or risk ratio), denoted by \\(RR\\) is given by \\[RR = \\frac{P(D|S)}{P(D|\\bar{S})}\\] where \\(\\bar{S}\\) is the complement of \\(S\\). Definition 3.5 (Odds Ratio, OR) Let \\(D\\) be an outcome event and \\(S\\) be an exposure event. The odds ratio, denoted by \\(OR\\), is given by \\[OR = \\frac{P(D|S)/P(\\bar{D}|S)}{P(D|\\bar{S})/P(\\bar{D}|\\bar{S})}\\] Note that \\(P(\\bar{D}|S) = 1 - P(D|S)\\). The odds ratio is invariant to switching \\(D\\) and \\(S\\), ie. \\[\\frac{P(D|S)/P(\\bar{D}|S)}{P(D|\\bar{S})/P(\\bar{D}|\\bar{S})} = \\frac{P(S|D)/P(\\bar{S}|D)}{P(S|\\bar{D})/P(\\bar{S}|\\bar{D})}.\\] Proof. \\[ \\begin{aligned} \\frac{P(D|S)/P(\\bar{D}|S)}{P(D|\\bar{S})/P(\\bar{D}|\\bar{S})} &amp;= P(D|S) \\cdot \\frac{1}{P(\\bar{D}|S)} \\cdot \\frac{1}{P(D|\\bar{S})} \\cdot P(\\bar{D}|\\bar{S})\\\\ &amp;= \\frac{P(S|D) P(D)}{P(S)} \\cdot \\frac{P(S)}{P(S|\\bar{D})P(\\bar{D})} \\cdot \\frac{P(\\bar{S})}{P(\\bar{S}|D)P(D)} \\cdot \\frac{P(\\bar{S}|\\bar{D})P(\\bar{D})}{P(\\bar{S})} \\\\ &amp;= \\frac{P(S|D)/P(\\bar{S}|D)}{P(S|\\bar{D})/P(\\bar{S}|\\bar{D})} \\end{aligned} \\] The second step plugs in the following equalities which follow from Bayes’ Rule \\[P(D|S) = \\frac{P(S|D) P(D)}{P(S)}, P(\\bar{D}|S) = \\frac{P(S|\\bar{D})P(\\bar{D})}{P(S)},\\] \\[P(D|\\bar{S}) = \\frac{P(\\bar{S}|D)P(D)}{P(\\bar{S})}, \\text{and } P(\\bar{D}|\\bar{S}) = \\frac{P(\\bar{S}|\\bar{D})P(\\bar{D})}{P(\\bar{S})}\\] 3.2 Independence Definition 3.6 (Independence) Two events are statistically independent if the occurrence of one has no impact on the other. Mathematically, \\[P(A | B) = P(A)\\] By Bayes’ Theorem, \\[P(A \\cap B) = P(A)P(B)\\] Often, we prefer to use the second definition because it is independent and easier to generalize to multiple events or random variables. For example, if we know that the random variables \\(X\\) and \\(Y\\) are independent, then \\[f_{X,Y}(x, y) = f_X(x)f_Y(y)\\] This property is the foundation of statistical inference for iid random variables discussed in Chapters 8 and 10. 3.3 Order Statistics Definition 3.7 (Order statistic) The \\(k\\)th order statistic of a random sample from a random variable \\(X\\) is the \\(k\\)th smallest value. We denote the \\(k\\)th order statistic as \\(X_{(k)}\\). Let \\(X_1, X_2, ..., X_n\\) be a random sample from \\(X \\sim f_X(x)\\) with cumulative distribution \\(F_X(x)\\). The order statistics are random variables themselves that satisfy \\(X_{(1)} \\leq X_{(2)}\\leq ... \\leq X_{(n)}\\). In particular, \\(X_{(1)} = min(X_1, X_2, ..., X_n)\\) and \\(X_{(n)} = max(X_1, X_2, ..., X_n)\\). The sample range, \\(R = X_{(n)} - X_{(1)}\\), is the distance between the smallest and the largest order statistics of the sample. The sample median is a number \\(M\\) such that approximately half of the observations in the sample are less than \\(M\\) and the other half are greater. \\[M = \\begin{cases} X_{((n+1)/2)}&amp; \\text{if } n \\text{ is odd} \\\\ (X_{(n/2)} + X_{(n/2 + 1)})/2 &amp; \\text{if } n \\text{ is even.} \\end{cases}\\] Theorem 3.1 Let \\(X_1, X_2, ..., X_n\\) be a random sample from a discrete distribution with pmf \\(f_X(x_i) = p_i\\), where \\(x_1&lt;x_2&lt; \\cdots\\) are the possible values of \\(X\\) in ascending order. Define \\(P_i = p_1 + p_2 + \\cdots p_i\\). Let \\(X_{(1)}, X_{(2)}, ... ,X_{(n)}\\) denote the order statistics from the sample. Then \\[P(X_{(j)}\\leq x_i) = \\sum_{k=j}^{n} {n \\choose k} P_i^k(1-P_i)^{n-k}\\] and \\[P(X_{(j)}= x_i) = \\sum_{k=j}^{n} {n \\choose k} [P_i^k(1-P_i)^{n-k} - P_{i-1}^k(1-P_{i-1})^{n-k}].\\] Theorem 3.2 Let \\(X_1, X_2, ..., X_n\\) be a random sample from a continuous distribution with pdf \\(f_X(x_i)\\) and cdf \\(F_X(x)\\). Then the pdf of \\(X_{(j)}\\) is \\[f_{X_{(j)}}(x) = \\frac{n!}{(j-1)!(n-j)!} f_X(x)[F_X(x)]^{j-1}[1-F_X(x)]^{n-j}.\\] Theorem 3.3 Let \\(X_1, X_2, ..., X_n\\) be a random sample from a continuous distribution with pdf \\(f_X(x_i)\\) and cdf \\(F_X(x)\\). Then the joint pdf of \\(X_{(i)}\\) and \\(X_{(j)}, 1\\leq i \\leq j \\leq n\\), is \\[f_{X_{(i)}, X_{(j)}} (u,v) = \\frac{n!}{(i-1)!(j-1-i)!(n-j)!}f_X(u)f_X(v)[F_X(u)]^{i-1} [F_X(v)-F_X(u)]^{j-1-i}[1-F_X(v)]^{n-j}.\\] The joint pdf of all order statistics is \\[f_{X_{(1)}, X_{(2)}, ..., X_{(n)}} (x_1,x_2, ..., x_n) = \\begin{cases} n!f_X(x_1)f_X(x_2) \\cdots f_X(x_n) &amp; -\\infty&lt; x_1&lt;x_2&lt;\\cdots&lt;x_n&lt;\\infty\\\\ 0 &amp; otherwise \\end{cases}\\] 3.4 Convergence What happens when, instead of a set of events or random variables, we observe a sequence of \\(n\\) random variables? There exist two major types of convergence of random variables with which we are typically concerned: convergence in probability and convergence in distribution 3.4.1 Convergence in Probability Definition 3.8 (Convergence in Probability) If \\(Z\\) is a random variable and \\(Z_n\\) is a sequence of random variables, then \\[Z_n \\overset{p}{\\rightarrow} Z \\iff\\lim_{n\\rightarrow\\infty}P(|Z_n - Z| &gt; \\epsilon) = 0\\] Convergence in probability has several properties. If \\(A_n \\overset{p}{\\rightarrow} a\\) and \\(B_n\\overset{p}{\\rightarrow}b\\), then \\(A_n + B_n \\overset{p}{\\rightarrow} a + b\\) \\(A_n - B_n \\overset{p}{\\rightarrow} a - b\\) \\(A_n \\cdot B_n \\overset{p}{\\rightarrow} a \\cdot b\\) \\(A_n / B_n \\overset{p}{\\rightarrow} a / b\\) Convergence in probability can also be extended to the multivariate setting, where it takes on a slightly different meaning. Definition 3.9 (Multivariate Convergence in Probability) If \\(X\\) is random vector and \\(X_n\\) is a sequence of random vectors, then \\[\\begin{align} X_n \\overset{p}{\\rightarrow} X \\iff\\lim_{n\\rightarrow\\infty}P(||X_n - X|| &gt; \\epsilon) = 0 \\\\ \\iff X_{jn} \\overset{p}{\\rightarrow}X_j, \\forall j \\in 1,...,k \\end{align}\\] 3.4.2 Convergence in Distribution Definition 3.10 (Convergence in Probability) \\[Z_n \\overset{\\mathcal{D}}{\\rightarrow} Z \\iff\\lim_{n\\rightarrow\\infty}F_n(Z) = F(z), \\forall\\text{ continuity points of }F\\] Note that convergence in probability implies convergence in distribution, but not the converse. That is, \\[Z_n \\overset{p}{\\rightarrow} Z \\implies Z_n \\overset{\\mathcal{D}}{\\rightarrow} Z\\] Unless, that is, the random variable converges in distribution to a constant - then, convergence in distribution does imply convergence in probability! That is, \\[Z_n \\overset{\\mathcal{D}}{\\rightarrow} c \\implies Z_n \\overset{p}{\\rightarrow} c \\] :::{.definition name=“Multivariate Convergence in Distribution”} If \\(X\\) is random vector and \\(X_n\\) is a sequence of random vectors, then \\[X_n \\overset{\\mathcal{D}}{\\rightarrow} X \\iff \\lim_{n\\rightarrow\\infty}F_n(X_1, ..., X_k) = F(X_1,...,X_k), \\forall\\text{ continuity points of }F\\] 3.4.3 Important Theorems Theorem 3.4 (Slutsky's Theorem) If \\(Z_n \\overset{\\mathcal{D}}{\\rightarrow} Z\\) and \\(Y_n \\overset{p}{\\rightarrow} c\\), then \\(Z_n + Y_n \\overset{\\mathcal{D}}{\\rightarrow} Z + c\\) \\(Z_nY_n \\overset{\\mathcal{D}}{\\rightarrow} cZ\\) \\(\\frac{Z_n}{Y_n}\\overset{\\mathcal{D}}{\\rightarrow}\\frac{Z}{c}\\). For problem-solving, Slutsky’s theorem is generally applied whenever we deal with both convergence in probability and convergence in distribution together. Theorem 3.5 (Continuous Mapping Theorem) Suppose \\(Y_n\\) is a sequence of random variables (possibly vectors), \\(Y\\) is a random variable (or vector the same length as \\(Y_n\\)), \\(c\\) is a constant, and \\(g\\) is a function. Then, If \\(Y_n \\overset{p}{\\rightarrow} c\\), and \\(g\\) is continuous at \\(c\\), then \\(g(Y_n) \\overset{p}{\\rightarrow} g(c)\\) If \\(Y_n \\overset{\\mathcal{D}}{\\rightarrow} Y\\), and \\(g\\) is continuous (with \\(g: \\mathbb{R}^k \\mapsto \\mathbb{R}^m\\) for vectors), then \\(g(Y_n) \\overset{\\mathcal{D}}{\\rightarrow} g(Y)\\) (in \\(\\mathbb{R}^m\\) for vectors) "],["known-distributions.html", "Chapter 4 Known Distributions 4.1 Families of Distributions 4.2 Location and Scale Families (#location-scale) 4.3 Exponential Families 4.4 Known Univariate Exponential Families 4.5 Exponential families with certain parameters fixed 4.6 Non-exponential families 4.7 Multivariate Distributions", " Chapter 4 Known Distributions This will cover proofs and useful properties of commonly used distributions, as well as location-scale and exponential families. 4.1 Families of Distributions Many so-called “distributions” are actually families of distributions, meaning that their pdf involves one or more parameters. That is, their pdfs represent a family of curves, a set of pdfs with variable parameters. For example, the \\(\\text{Normal}(\\mu, \\sigma^2)\\) distribution contains two parameters, \\(\\mu\\) (the mean), and \\(\\sigma^2\\) (the variance). These are also examples of two types of parameters with special properties - called location and scale parameters, respectively - that can be used to simply calculations. 4.2 Location and Scale Families (#location-scale) 4.2.1 Location Families Definition 4.1 (Location Family) Let \\(Z \\sim f_Z(z)\\). Given a constant location parameter \\(b\\), \\(X\\) is a location family if \\(X \\sim f_Z(z - b)\\) or if \\(X = Z + b\\). The two above definitions are equivalent because if \\(X = Z + b\\), then \\(P(X &lt; z) = P(Z + b &lt; z) = P(Z &lt; z - b)\\), so the cdf of \\(Z\\) is \\(F_Z(z - b)\\) and therefore \\(X \\sim f_Z(z - b)\\) (note this makes use of a direct probability argument) 4.2.2 Scale Families Definition 4.2 (Scale Family) Let \\(Z \\sim f_Z(Z)\\). Given a constant scale parameter \\(a\\), \\(X\\) is a scale family if… \\(X \\sim \\frac{1}{a}f_Z(\\frac{z}{a})\\) \\(X \\sim F_Z(\\frac{z}{a})\\) or \\(X = aZ\\) All of the above definitions are equivalent because if \\(X = aZ\\), then \\(P(X &lt; z)\\) = \\(P(aZ &lt; z) = P(Z &lt; \\frac{z}{a} = F_Z(z)\\). Also, \\(f_X(x) = \\frac{d}{dx}F_X(x) = \\frac{d}{dx}F_Z(\\frac{z}{a}) = \\frac{1}{a}f_Z(\\frac{z}{a})\\) 4.2.3 Properties of Location-Scale Families We can compute moments by using general properties of expectation (see Moments) \\(E(X) = aE(Z) + b\\), by linearity of expectation. If the support of \\(Z\\) includes \\(0\\), then we typically define \\(Z\\) such that \\(E(Z) = 0\\) so that \\(E(X) = b\\). \\(Var(X) = a^2Var(Z)\\), since \\(Var(Z + b) = Var(Z)\\). We typically define \\(Z\\) such that \\(Var(Z) = 1\\), so that \\(Var(X) = a^2Var(Z) = a^2\\). An example of this is the standard normal. \\(\\mathcal{M}_X(t) = e^{tb} \\mathcal{M}_Z(at)\\) It may seem like the sum of a scale family should also follow the same family - indeed, this is true for a number of distributions include the Normal, Poisson, and Gamma. However, it is not true always. For instance, \\(X_i \\sim \\text{Uniform}(0,a)\\) is a scale family, but \\(X_1 + X_2\\) does not follow a uniform distribution: X1 = runif(1000, 0, 1) X2 = runif(1000, 0, 1) hist(X1 + X2) 4.3 Exponential Families Definition 4.3 (Exponential Family) \\(X \\sim f_X(x|\\theta)\\) is an exponential family if its pdf can be written in the form \\[f_X(x|\\theta) = h(x)c(\\theta)\\exp\\Big(\\sum_{i=1}^k w_i(\\theta)t_i(x)\\Big)\\] How do we prove that a pdf can be written in the above form? Often, the easiest way is to use a simple trick to get the necessary \\(\\exp\\) function: \\(f(x) = \\exp(\\log(f(x)))\\). Then, we algebraically manipulate to obtain this form. How do we prove that a pdf cannot be written in the above form? FORTHCOMING 4.3.1 Properties Exponential families have a number of incredibly useful properties: Leibniz’s rule holds, meaning that the Cramer-Rao Lower Bound provides a lower bound on the variance of estimators. Among most common families, only exponential families admit sufficient statistics with dimension bounded in \\(n\\). This is proven by the Pitman-Koopman-Darmois theorem for families with smooth, nowhere-vanishing pdfs whose domain does not depend on the parameter being estimated. If \\(X\\) is an exponential family, \\[T(X) = \\Big(\\sum_{i=1}^n t_1(x), ..., \\sum_{i=1}^n t_k(x)\\Big)\\] is a minimal sufficient statistic. Furthermore, if \\(\\{w_1(\\theta),...,w_k(\\theta)\\}\\) contains an open set, then \\(T(X)\\) is a complete sufficient statistic, which we can use to compute an UMVUE. The Method of Moments (MOM) estimator is equal to the Maximum Likelihood Estimator (MLE) The regularity conditions required for consistency and asymptotic normality of the MLE are guaranteed to hold. The family must have a Monotone Likelihood Ratio, meaning that the Karlin-Rubin Theorem may be employed to construct an UMP test. 4.3.2 Natural Exponential Families Definition 4.4 (Natural Exponential Family) \\(X \\sim f_X(x|\\theta)\\) is a natural exponential family if its pdf can be written in the form \\[f_X(x|\\theta) = h(x)c^*(\\boldsymbol{\\eta})\\exp\\Big(\\sum_{i=1}^k \\eta_i t_i(x)\\Big)\\] 4.4 Known Univariate Exponential Families Many common distributions follow exponential families. As you will come to find, virtually all of them arise in order to model variations on a common idea: the Bernoulli trial. Let’s discuss this distribution, and the situations in which it arises. 4.4.1 Bernoulli The Bernoulli distribution, represented mathematically as \\(\\text{Bernoulli}(p)\\), describes the outcome of a random variable \\(X\\) that takes only two possible values, 0 and 1. Such an event is often termed a “Bernoulli trial”. Description Parameters Support pmf Any random variables whose value can be either 0 or 1 \\(0 \\leq p \\leq 1\\) \\(x \\in \\{0, 1\\}\\) \\(p^x(1-p)^{1-x}\\) The Bernoulli distribution occurs very commonly because many situations can be described in terms of 0 or 1 outcomes. For example, all indicator functions \\(I(A)\\) of random variables, where \\(A\\) is a statement about the random variable (for instance, \\(A = \\{x: x &gt; 1\\}\\)) are Bernoulli random variables with \\(p = P(A)\\). The Bernoulli is a special case of the Binomial distribution: \\(\\text{Bernoulli}(p) = \\text{Binomial}(1, p)\\) As we discuss regarding the Binomial distribution, the Bernoulli has an additive property: \\(\\sum_{i=1}^n \\sim \\text{Binomial}(n, p)\\). This can be proven by the additivity technique discussed imminently. If \\(X\\sim \\text{Bernoulli}\\) then \\(E(X) = P(Y = 1)\\), a fact which is often useful for computing moments as well as finding UMVUEs. 4.4.2 Binomial What happens when we repeat a Bernoulli trial many times and count how many 1’s occur? The Binomial distribution is represented mathematically as \\(\\text{Bernoulli}(n, p)\\). It describes the number of successes in a series of Bernoulli trials. Description Parameters Support pmf The number of times an event was successful out of \\(n\\) attempts \\(0 \\leq p \\leq 1\\), \\(n \\in \\mathbb{N}\\) \\(x \\in \\mathbb{N}\\) \\({n\\choose x}p^x(1-p)^{n-x}\\) Example 4.1 (Proving Additive Properties of Distributions) The Binomial distribution is additive: if \\(X \\sim \\text{Binomial}(n,p)\\) and \\(X \\sim \\text{Binomial}(m,p)\\), then \\(X + Y \\sim \\text{Binomial}(m + n, p)\\). One can prove the additivity of any distribution, not just the Binomial, by relying on the convolution property of mgfs: \\(\\mathcal{M}_{X + Y}(t) = \\mathcal{M}_X(t)\\cdot\\mathcal{M}_Y(t)\\). Here’s an example with the Binomial: if \\(X \\sim \\text{Binomial}(n, p)\\) and \\(Y \\sim \\text{Binomial}(m, p)\\), then \\[\\mathcal{M}_{X + Y}(t) = ((1-p) + pe^t)^n\\cdot ((1-p) + pe^t)^m = ((1-p) + pe^t)^{mn}\\] which we can recognize as the mgf of a \\(\\text{Binomial}(n + m, p)\\) distribution. Since, like the cdf and the pdf, the mgf fully characterizes a probability distribution, we’ve proven the additive property mentioned above. Here’s another example with the Binomial, this time generalizing it to an arbitrary summation: If \\(X_i \\overset{iid}{\\sim} \\text{Binomial}(m, p)\\), and \\(Y = \\sum_{i=1}^nX_i\\), then \\[ \\mathcal{M}_Y(t) = \\prod_{i=1}^n\\mathcal{M}_{X_i}(t) = (\\mathcal{M}_{X_i}(t))^n \\\\ = (((1-p) + pe^t)^m)^n = ((1-p) + pe^t)^{mn} \\] proving the generalized additive property that if \\(X_i \\sim \\text{Binomial}(m, p)\\), then \\(\\sum_{i=1}^n X_i \\sim \\text{Binomial}(nm, p)\\). 4.4.3 Geometric Suppose instead of counting the number of successes, we wish to count the number of attempts until a single success occurs? The Geometric distribution is represented mathematically as \\(\\text{Geo}(p)\\). It describes the number of trials before a success occurs in a series of Bernoulli trials. Note that the parametrization below does not include the final success in the number of trials, but alternatives exist in which it may. Description Parameters Support pmf The number of Bernoulli trials attempted before a success occurs \\(0 &lt; p \\leq 1\\) \\(x \\in \\mathbb{N}\\) \\(p(1-p)^{x}\\) The Geometric is a special case of the Negative Binomial: \\(\\text{Geo}(p) = \\text{NegBin}(1, p)\\) Just like the Bernoulli, the Geometric has an additive property: \\(\\sum_{i=1}^n X_i \\sim \\text{NegBin}(n, p)\\), proven via the same addivity technique discussed previously. The Geometric is the only discrete memoryless distribution; that is, for \\(k &gt; i\\), \\(P(X \\geq k | X &gt; i) = P(X \\geq k - i)\\). 4.4.4 Negative Binomial The Negative Binomial distribution generalizes the Geometric distribution to instead represent the number of Bernoulli trials until \\(r\\) successes have occurred. It is represented mathematically as \\(\\text{NegBin}(r, p)\\). Description Parameters Support pmf The number of Bernoulli trials attempted before a success occurs \\(0 &lt; p \\leq 1\\), \\(r \\in \\mathbb{N}\\) \\(x \\in \\mathbb{N}\\) \\({x+r-1\\choose x}p^r(1-p)^{x}\\) The Negative Binomial is additive: If \\(X_i \\sim \\text{NegBin(r, p)}\\), then \\(\\sum_{i=1}^n X_i \\sim \\text{NegBin(nr, p)}\\) 4.4.5 Poisson The Poisson distribution describes one possible behavior of a count random variable. It describes the probability that a certain number of events occur within a fixed interval, such as a time period, distance or area. It is mathematically represented as \\(\\text{Poisson}(\\lambda)\\). Description Parameters Support pmf The number of events occurring in a fixed interval \\(\\lambda \\in (0, \\infty)\\) \\(x \\in \\mathbb{N}_0\\) \\(\\frac{1}{x!}\\lambda^xe^{-\\lambda}\\) Like the Binomial, the Poisson is formulated by counting the number of successes within a set of Bernoulli trials. The distribution describes the asymptotic behavior of the Binomial distribution as \\(n \\rightarrow \\infty\\) and \\(np \\rightarrow \\lambda\\), a fixed rate parameter. The Poisson is additive. If \\(X_i \\sim \\text{Poisson}(\\lambda)\\), then \\(\\sum_{i=1}^n X_i \\sim \\text{Poisson}(n\\lambda)\\) 4.4.6 Normal Often denoted \\(N(\\mu, \\sigma^2)\\), the Normal distribution is especially common in asymptotics - by the Central Limit Theorem, the sample mean converges in distribution to a normal. Many other variables also converge to a normal. For \\(X \\sim N(\\mu, \\sigma^2)\\), the mean is \\(\\mu\\) and the variance is \\(\\sigma^2\\). This means that the Normal is a location-scale family. Description Parameters Support pdf Describes the asymptotic behavior of sample means and many distributions \\(\\mu \\in \\mathbb{R}\\), \\(\\sigma^2 \\in (0, \\infty)\\) \\(x \\in \\mathbb{R}\\) \\(\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\Big(-\\frac{1}{2}\\frac{(x-\\mu)^2}{\\sigma^2}\\Big)\\) The following distributions converge to a Normal: \\(\\text{Binomial}(n, p) \\approx N(np, np(1-p))\\) for large \\(n\\) and \\(p\\) bounded away from 0 or 1 \\(\\text{Pois(\\lambda)} \\approx N(\\lambda, \\lambda)\\) for large \\(\\lambda\\). \\(\\chi^2(\\nu)\\approx N(\\nu, 2\\nu)\\) for large \\(\\nu\\). \\(t(\\nu) \\approx N(0, 1)\\) for large \\(\\nu\\). The Normal is additive. If \\(X_1 \\sim N(\\mu_1, \\sigma_1^2)\\) and \\(X_2 \\sim N(\\mu_2, \\sigma_2^2)\\), then \\(X + Y \\sim N(\\mu_1 + \\mu_2, \\sigma_1^2 + \\sigma_2^2)\\). 4.4.7 Exponential The Poisson process that models the number of events occuring in an interval also gives rise to another distribution: the Exponential. This distribution models the size of the interval (time, distance, etc.) between events. The Exponential is described mathematically as \\(\\text{Exp}(\\lambda)\\). It can be parametrized in two ways: with \\(\\lambda\\) as a rate parameter, describing how often events occur; or with \\(\\lambda\\) as a scale parameter (yes, the same scale parameter discussed in Location Scale Families) that is the inverse of the rate. Hence, the Exponential is a scale family. Note that the parametrization below describes the distribution in terms of the scale parameter. The scale parameter version simply replaces \\(\\lambda\\) with \\(\\frac{1}{\\lambda}\\). Description Parameters Support pdf Models the size of the interval between events in a Poisson process \\(\\lambda \\in (0, \\infty)\\) \\(x \\in (0, \\infty)\\) \\(\\lambda e^{-\\lambda x}\\) The Exponential is a special case of the Gamma distribution: \\(\\text{Exp}(\\lambda) = \\text{Gamma}(1, \\lambda)\\) By extension, the exponential has an additive property: If $X_iExp() $, then \\(\\sum_{i=1}^n X_i \\sim \\text{Gamma}(n, \\lambda)\\). The Exponential is the only continuous memoryless distribution; that is, for \\(k &gt; i\\), \\(P(X \\geq k | X &gt; i) = P(X \\geq k - i)\\). 4.4.8 Gamma Similar to how the Negative Binomial generalizes the Geometric to multiple successes, the Gamma generalizes the Exponential to multiple events. The Gamma is useful for modeling random variables that are known to be greater than 0. It is represented mathematically as \\(\\text{Gamma}(k, \\lambda)\\), where \\(k\\) is a shape parameter and \\(\\lambda\\) is a scale parameter. This means the Gamam is a scale family. It is called “Gamma” because it involves the gamma function. Description Parameters Support pdf Generalization of the exponential distribution \\(k \\in (0, \\infty)\\), \\(\\lambda \\in (0, \\infty)\\) \\(x \\in (0, \\infty)\\) \\(\\frac{1}{\\Gamma(k)\\lambda^k}x^{k-1}\\exp(-\\frac{x}{\\theta})\\) The Gamma is additive: If $X_i(k, ) $, then \\(\\sum_{i=1}^n X_i \\sim \\text{Gamma}(nk, \\lambda)\\). 4.4.9 Beta The Beta distribution models proportions. It is mathematically denoted \\(\\text{Beta}(\\alpha, \\beta)\\) where \\(\\alpha\\) and \\(\\beta\\) are two shape parameters. It is known as “Beta” because its pdf contains the beta function: \\(B(\\alpha, \\beta) = \\frac{\\Gamma(\\alpha)\\Gamma(\\beta)}{\\Gamma(\\alpha + \\beta)}\\) Description Parameters Support pdf Model of a proportion \\(\\lambda \\in (0, \\infty)\\) \\(x \\in (0, 1)\\) \\(\\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)}x^{\\alpha-1}(1-x)^{\\beta-1}\\) 4.4.10 Chi-squared The Chi-squared distribution describes the distribution of the sum of squared standard Normal (\\(N(0,1)\\)) random variables. As a result, it is useful in asymptotics, especially asymptotic hypothesis testing, since if an estimator is asymptotically normal, its square is asymptotically \\(\\chi^2\\). It is represented mathematically as \\(\\chi^2(\\nu)\\) or \\(\\chi^2_\\nu\\) where \\(\\nu\\) is the “degrees of freedom” - the number of squared normal random variables in the sum. Description Parameters Support pdf Squared standard normals \\(\\nu \\in \\mathbb{N}\\) \\(x \\in (0, \\infty)\\) \\(\\frac{1}{\\Gamma(\\nu/2)2^{\\nu / 2}}x^{\\nu/2 - 1}\\exp(-\\nu/2)\\) If \\(X_i \\sim N(0,1)\\), then \\(\\sum_{i=1}^nX_i^2\\sim \\chi^2(n)\\) The Chi-squared distribution is a special case of the Gamma. That is, if \\(X \\sim \\chi^2(\\nu)\\), then \\(X \\sim \\text{Gamma}(\\frac{\\nu}{2}, \\frac{1}{2})\\). This can be observed directly from the pdf. 4.5 Exponential families with certain parameters fixed Some families are exponential, but only when one or more parameters are fixed. 4.5.1 Weibull The Weibull is a generalization of the exponential distribution. The extra parameter \\(k\\) describes how the failure rate changes over time. It is an exponential family when \\(k\\) is fixed. Like the exponential, \\(\\lambda\\) is a scale parameter, meaning that the Weibull is a scale family. Description Parameters Support pdf Models time-to-event variables \\(\\lambda \\in (0, \\infty)\\), \\(k \\in (0, \\infty)\\) \\(x \\in [0, \\infty)\\) \\(\\frac{k}{\\lambda}\\Big(\\frac{x}{\\lambda}\\Big)^{k-1}\\exp(-(x/\\lambda)^k)\\) When \\(k = 1\\), the Weibull is equal to an \\(\\text{Exponential}(\\frac{1}{\\lambda})\\), which can be observed directly from its pdf. 4.5.2 Pareto The Pareto distribution, written \\(\\text{Pareto}(x_m, \\alpha)\\), models variables involving power-law relationships. \\(\\alpha \\in (0, \\infty)\\) is a shape parameter, while \\(x_m\\) is a scale parameter. This means that the Pareto is a scale family. It is an exponential family when \\(x_m\\) is fixed. Description Parameters Support pdf Power-law models \\(\\alpha \\in (0, \\infty)\\), \\(x_m \\in (0, \\infty)\\) \\(x \\in [0, \\infty)\\) \\(\\frac{\\alpha x_m^\\alpha}{x^{\\alpha+1}}\\) The Pareto is related to the Exponential. If \\(X\\sim \\text{Pareto}(x_m, \\alpha)\\), then \\(Y = \\log(\\frac{X}{x_m}) \\sim \\text{Exp}(\\alpha)\\) 4.6 Non-exponential families The following families are not exponential, but still commonly arise. 4.6.1 Uniform The Uniform distribution is parametrized by its minimum \\(a\\) and maximum \\(b\\). Denoted \\(U(a,b)\\), it describes a scenario where every possible value of \\(x\\) has the same probability. Description Parameters Support pdf Every \\(x\\) has same probability \\(-\\infty &lt; a &lt; b &lt; \\infty\\) \\(x \\in [a, b]\\) \\(\\frac{1}{b-a}\\) If \\(X \\sim \\text{Beta}(1,1)\\), then \\(X \\sim U(0,1)\\) If \\(X \\sim U(0,1)\\), then \\(-\\lambda \\log(X) \\sim \\text{Exp}(\\lambda)\\). If \\(X_i \\overset{iid}{\\sim} U(0,1)\\), then \\(X_{(k)} - X_{(j)} \\sim \\text{Beta}(k - j, n - (k - j) + 1)\\) By the Probability Integral Transform, inverse cdfs always follow a standard uniform distribution. That is, if \\(X = F_X^{-1}(Y)\\), then \\(Y \\sim U(0,1)\\). This can be used to generate any random variable with a known cdf. 4.6.2 Cauchy The Cauchy arises in situations involving ratios of standard normal variables, as well as rotations. It is \\(\\text{Cauchy}(x_0, \\gamma)\\), where \\(x_0\\) is a location parameter and \\(\\gamma\\) is a scale parameter, making it a location-scale family. Description Parameters Support pdf Rotations and ratios of normals \\(x_0 \\in \\mathbb{R}\\), \\(\\gamma \\in (0, \\infty)\\) \\(x \\in \\mathbb{R}\\) \\(\\frac{1}{\\pi \\gamma\\Big(1 + \\Big(\\frac{x - x_0}{\\gamma}\\Big)^2\\Big)}\\) If \\(U, V \\sim N(0,1)\\) independently, then \\(U/V \\sim Cauchy(0,1)\\) The Cauchy is often used as a pathological example in statistical problems, since it famously has no mean or variance (\\(E(X) = Var(X) = \\infty\\)) If \\(X\\sim t(1)\\), then \\(X\\sim \\text{Cauchy}(0,1)\\) 4.6.3 t-distribution The t-distribution describes the distribution of the t-statistic for \\(X_1, ... X_n \\overset{iid}{\\sim}N(\\mu, \\sigma^2)\\) \\[t = \\frac{\\bar{X} - \\mu}{\\sqrt{S^2/n}}\\] As a result, it is commonly used in hypothesis testing. Denoted \\(t(\\nu)\\), the parameter \\(\\nu\\) represents the degrees of freedom - the number of \\(X_i\\) in the sample that are being summed in the computation of \\(\\bar{X}\\) and \\(S^2\\). Description Parameters Support pdf Distribution of the t-statistic \\(\\nu \\in \\mathbb{N}\\) \\(x \\in \\mathbb{R}\\) \\(\\frac{\\Gamma((\\nu + 1) / 2)}{\\sqrt{\\nu\\pi}\\Gamma(\\nu/2)}\\Big(1 + \\frac{x^2}{\\nu}\\Big)^{-(\\nu+1)/2}\\) As \\(\\nu \\rightarrow \\infty\\), the t-distribution converges to a \\(N(0,1)\\). If \\(X \\sim t(1)\\) then \\(X \\sim \\text{Cauchy}(0,1)\\). 4.6.4 F-distribution Also useful for [hypothesis testing]](#hypothesis-tests-finite-samples), the F-distribution, denoted \\(F(n, m)\\), describes the distribution of the F-statistic: \\[X = \\frac{S_1 / n}{S_2 / m}\\] where \\(S_1\\) and \\(S_2\\) are the sums of independent standard normal random variables with degrees of freedom \\(n\\) and \\(m\\), respectively - that is, \\(S_1 \\sim \\chi^2(n)\\) and \\(S_2 \\sim \\chi^2(m)\\). Description Parameters Support pdf Distribution of the \\(F\\)-statistic \\(n \\in \\mathbb{N}\\), \\(m \\in \\mathbb{N}\\) \\(x \\in (0, \\infty)\\) \\(\\sqrt{\\frac{(nx)^nm^m}{(nx + m)^{n + m}}}\\frac{\\Gamma(n + m)}{x\\Gamma(n)\\Gamma(m)}\\) If \\(X \\ F(n, m)\\), then \\(\\frac{1}{X} \\sim F(m, n)\\) If \\(X \\sim t(n)\\), then \\(X^2 \\sim F(1, n)\\) If \\(X \\sim \\chi^2(n)\\) and \\(Y \\sim \\chi^2(m)\\), then \\(\\frac{X / n}{Y / m} \\sim F(n, m)\\) If \\(X_i \\overset{iid}{\\sim} \\text{Gamma}(\\alpha_i, \\beta_i)\\), then \\(\\frac{\\alpha_2\\beta1X_1}{\\alpha_1\\beta_2X_2} \\sim F(2\\alpha_1, 2\\alpha_2)\\) If \\(X \\sim \\text{Beta}(n/2, m/2)\\), then \\(\\frac{mX}{n(1-X)}\\sim F(n, m)\\) 4.6.5 Hypergeometric Imagine drawing \\(n\\) samples without replacement from a finite population of size \\(N\\). Suppose \\(K\\) of the units in the population are considered “successes” if drawn. The Hypergeometric distribution, denoted \\(\\text{HGeo}(N, K, n)\\), describes the probability that you will draw \\(x\\) “successes” under these circumstances. Description Parameters Support pmf Sampling without replacement \\(N \\in \\mathbb{N}_0\\), \\(K \\in \\{0, 1, ..., N\\}\\), \\(n \\in \\{0,1,...,N\\}\\) \\(x \\in \\{\\max(0, n+K-N),..., \\min(n,K)\\}\\) \\(\\frac{{K\\choose x}{N - K \\choose n - x -1}}{N \\choose n}\\) Fisher’s Exact Test is based on the Hypergeometric distribution. If \\(X \\sim \\text{HGeo}(N, K, n)\\), and \\(N\\) and \\(K\\) are sufficiently large compared to \\(n\\), then \\(X \\approx \\text{Binom}(n, p)\\) 4.7 Multivariate Distributions 4.7.1 Bivariate Normal The bivariate normal describes a situation where two random variables \\(X\\) and \\(Y\\) are normally distributed, and their sum is also normally distributed. Description Parameters Support pmf Two-dimensional normal \\(\\mu_x, \\mu_y \\in \\mathbb{R}\\), \\(\\sigma_x, \\sigma_y \\in \\mathbb{R} &gt; 0\\), \\(\\rho \\in [-1, 1]\\) \\(x\\in\\mathbb{R}^2\\) \\(\\frac{1}{2\\pi\\sigma_x\\sigma_y\\sqrt{1-\\rho^2}}\\exp\\Big(-\\frac{1}{2(1-\\rho^2)}\\Big((\\frac{x-\\mu_x}{\\sigma_x})^2 - 2\\rho(\\frac{x - \\mu_x}{\\sigma_x})(\\frac{y - \\mu_y}{\\sigma_y}) + (\\frac{y-\\mu_y}{\\sigma_y})^2\\Big)\\Big)\\) Suppose that \\((X, Y)\\) follows the bivariate normal distribution above. Then, The marginal distributions are \\(X \\sim N(\\mu_x, \\sigma_x^2)\\) and \\(Y \\sim N(\\mu_y, \\sigma_y^2)\\) \\(Corr(X, Y) = \\rho\\) Any linear combination of \\(X\\) and \\(Y\\) is univariate normal. That is, \\(aX + bY \\sim N(a\\mu_x + b\\mu_y, a^2\\sigma_x^2 + b^2\\sigma_y^2 +2ab\\rho\\sigma_x\\sigma_y\\). 4.7.1.1 Multivariate Normal The Multivariate Normal, often denoted \\(MVN(\\mu, \\Sigma)\\), generalizes the normal to a random vector \\(X\\) where all linear combinations of its components have a univariate normal distribution. Description Parameters Support pmf \\(k\\)-dimension normal \\(\\mu \\in \\mathbb{R}^k\\), \\(\\Sigma \\in \\mathbb{R}^{k \\times k}\\) \\(x\\in\\mathbb{R}^k\\) \\(\\frac{1}{\\sqrt{(2\\pi)^{k}\\det(\\Sigma)}}\\exp\\Big(-\\frac{1}{2}(x - \\mu)^\\top\\Sigma^{-1}(x-\\mu)\\Big)\\) If a vector \\(X\\sim MVN(\\mu, \\Sigma)\\), then All marginal distributions of \\(X\\) follow a multivariate normal with the marginalized means and rows/columns in the covariance matrix dropped. For example, \\(X_1 \\sim N(\\mu_1, \\sigma_1)\\) If \\(Y = c + BX\\), then \\(Y \\sim MVN(c + B\\mu, B\\Sigma B^\\top)\\) Note that two normally distributed random variables may not be jointly bivariate normal! 4.7.2 Multinomial Consider an event in which one of \\(k\\) discrete outcomes is guaranteed to occur - for instance, rolling a 6-sided die, where there are \\(k = 6\\) possible outcomes. Repeat this event \\(n\\) times. If outcome \\(i\\) occurs with probability \\(p_i\\) (not necessarily equal), then the number of times \\(X_i\\) that each outcome \\(i = 1,...,k\\) occurs after \\(n\\) trials is modeled by the Multinomial distribution. \\(X\\) is a vector representing the number of successes of each event. Description Parameters Support pmf \\(k\\) joint binomials \\(n \\in \\mathbb{N}\\), \\(k \\in \\mathbb{N}\\), \\(p_1,...,p_k\\in (0,1)\\), \\(\\sum_{i=1}^kp_i = 1\\) \\(x_i\\in \\mathbb{N}\\), \\(\\sum_{i=1}^kx_i = n\\) \\(\\frac{n!}{\\prod_{i=1}^kx_i!}\\prod_{i=1}^kp_i^{x_i}\\) If the vector \\(X\\) is multinomial, then All marginal distributions of \\(X\\) are multinomial All conditional distributions are multinomial \\(X_i \\sim \\text{Binomial}(n, p_i)\\) \\(Cov(X_i, X_j) = E((X_i - mp_i)(X_j - mp_j)) = -mp_ip_j, \\forall i\\neq j\\) "],["new-distributions.html", "Chapter 5 New Distributions 5.1 Transformations 5.2 Probability Integral Transform", " Chapter 5 New Distributions This chapter will demonstrate how to derive the PDF or CDF of a random variable that is a function of other random variables, including hierarchical models. 5.1 Transformations 5.1.1 Theorems We’ll start by introducing two useful theorems for finding the PDF of a transformed random variable \\(Y=g(X)\\) where \\(g(X)\\) is a monotone, one-to-one function over the domain of interest.   Definition 5.1 (Theorem 2.1.5) Let \\(X\\) have \\(p d f f_X(x)\\) and let \\(Y=g(X)\\), where \\(g\\) is a monotone function. Let \\(\\mathcal{X}\\) and \\(\\mathcal{Y}\\) be defined by (2.1.7). Suppose that \\(f_X(x)\\) is continuous on \\(\\mathcal{X}\\) and that \\(g^{-1}(y)\\) has a continuous derivative on \\(\\mathcal{Y}\\). Then the PDF of \\(Y\\) is given by \\[ f_Y(y)= \\begin{cases}f_X\\left(g^{-1}(y)\\right)\\left|\\frac{d}{d y} g^{-1}(y)\\right| &amp; y \\in \\mathcal{Y} \\\\ 0 &amp; \\text { otherwise}\\end{cases} \\] In some cases, \\(g(X)\\) will only be a monotone, one-to-one function over subsets of the domain of interest. Below, Theorem 2.1.8 provides a generalization of Theorem 2.1.5 that can be applied in these situations. Definition 5.2 (Theorem 2.1.8) Let \\(X\\) have PDF \\(f_X(x)\\), let \\(Y=g(X)\\), and define the sample space \\(\\mathcal{X}\\) as in (2.1.7). Suppose there exists a partition, \\(A_0, A_1, \\ldots, A_k\\), of \\(\\mathcal{X}\\) such that \\(P\\left(X \\in A_0\\right)=0\\) and \\(f_X(x)\\) is continuous on each \\(A_i\\). Further, suppose there exist functions \\(g_1(x), \\ldots, g_k(x)\\), defined on \\(A_1, \\ldots, A_k\\), respectively, satisfying \\(g(x)=g_i(x)\\), for \\(x \\in A_i\\), \\(g_i(x)\\) is monotone on \\(A_i\\), the set \\(\\mathcal{Y}=\\left\\{y: y=g_i(x)\\right.\\) for some \\(\\left.x \\in A_i\\right\\}\\) is the same for each \\(i=1, \\ldots, k\\), and \\(g_i^{-1}(y)\\) has a continuous derivative on \\(\\mathcal{Y}\\), for each \\(i=1, \\ldots, k\\). Then \\[ f_Y(y)= \\begin{cases}\\sum_{i=1}^k f_X\\left(g_i^{-1}(y)\\right)\\left|\\frac{d}{d y} g_i^{-1}(y)\\right| &amp; y \\in \\mathcal{Y} \\\\ 0 &amp; \\text { otherwise }\\end{cases} \\] 5.1.2 Practical Strategy Now, we will outline a practical strategy for solving problems that involve transformations. Given a transformation \\(y=g(x)\\) (univariate) or \\(u,v=g(x,y)\\) (multivariate), perform a transformation by following these steps: Step 1: Define PMF/PDF of the untransformed random variable(s). Univariate: \\(f(x)\\) (given) Multivariate: calculate joint density \\(f(x,y)\\) (not always given) Step 2: Find the inverse. Univariate: \\(y=g(x) \\quad \\Rightarrow \\quad x=g^{-1}(y)\\) Multivariate: \\(u=g(x,y), v=g(x,y) \\quad \\Rightarrow \\quad x=g^{-1}(u,v),y=g^{-1}(u,v)\\) (Note: for multivariate case, solve system of equations) Step 3: Calculate jacobian of the transformation. For transformations of multiple random variables, recall bivariate Jacobian: \\[ |J|=\\left|\\begin{array}{ll} \\frac{\\partial x}{\\partial u} &amp; \\frac{\\partial x}{\\partial v} \\\\ \\frac{\\partial y}{\\partial u} &amp; \\frac{\\partial y}{\\partial v} \\end{array}\\right|=\\left| \\frac{\\partial x}{\\partial u} \\frac{\\partial y}{\\partial v}-\\frac{\\partial y}{\\partial u} \\frac{\\partial x}{\\partial v} \\right| \\] Step 4: Calculate PMF/PDF of the transformed random variable. \\[ f_Y(y)= f_X\\left(g^{-1}(y)\\right)\\left|\\frac{d}{d y} g^{-1}(y)\\right| \\] \\[ f_{U,V}(u,v)= f_{X,Y}\\left(g^{-1}(u,v)\\right) \\left|\\begin{array}{ll} \\frac{\\partial x}{\\partial u} &amp; \\frac{\\partial x}{\\partial v} \\\\ \\frac{\\partial y}{\\partial u} &amp; \\frac{\\partial y}{\\partial v} \\end{array}\\right| \\] 5.2 Probability Integral Transform The probability integral transform is a powerful technique that allows for the simulation of random variables from any distribution, given that you have access to a source of uniformly distributed random numbers. This method is particularly useful because it provides a straightforward way to transform uniform random variables into random variables following a desired distribution. The general idea is as follows: When you plug any continuous random variable \\(X\\) into its own CDF, you get \\(U \\sim\\) Uniform(0,1): \\[ F_X(X) \\sim \\text{Uniform} (0,1) \\] When you plug \\(U \\sim\\) Uniform(0,1) into an inverse CDF, you get a continuous random variable \\(X\\) with that CDF: \\[ F_X^{-1}(U) \\text{ has CDF } F_X(x) \\] Procedure: Construct a random variable \\(X\\) with a particular CDF \\(F_X(x)\\) Generate \\(U \\sim\\) Uniform(0,1) Plug \\(U\\) into the inverse CDF: \\(X=F^{-1}_X(U)\\) \\(X\\) is distributed according to the CDF \\(F_X(x)\\)   Click Here: Visual Explanation of Universality of the Uniform 5.2.1 Hiearchical Models (Iterated Moments) Definition 5.3 (Theorem 4.4.3 and Theorem 4.4.7) If \\(X\\) and \\(Y\\) are any two random variables and the relevant expectations exist, then \\[ \\begin{aligned} E_X(X) &amp; =E_Y\\left(E_{X \\mid Y}(X \\mid Y)\\right) \\\\ \\operatorname{Var}_X(X) &amp; =E_Y\\left(\\operatorname{Var}_{X \\mid Y}(X \\mid Y)\\right)+\\operatorname{Var}_Y\\left(E_{X \\mid Y}(X \\mid Y)\\right) \\end{aligned} \\] 5.2.2 Convolutions Convolutions arise when we calculate the probability distribution of the sum or linear combination of independent random variables. In the following table, we provide a compilation of convolutions involving random variables from well-established probability distributions. Acquainting yourself with these convolutions can significantly improve the efficiency of computing moment generating functions and other statistical expressions of interest. \\[ \\begin{array}{l|l} f_{X_i} &amp; f_{\\Sigma X_i}\\left(X_i \\text { independent }\\right) \\\\ \\hline \\operatorname{Ber}(p) &amp; \\operatorname{Bin}(n, p) \\\\ \\operatorname{Bin}\\left(n_i, p\\right) &amp; \\operatorname{Bin}\\left(\\sum_i n_i, p\\right) \\\\ \\operatorname{Geo}(p) &amp; \\operatorname{NBin}(n, p) \\\\ \\operatorname{Exp}(\\beta) &amp; \\operatorname{Gam}(n, \\beta) \\quad \\quad \\text { Note: } E\\left(X_i\\right)=\\frac{1}{\\beta}, E\\left(\\sum X_i\\right)=\\frac{n}{\\beta} \\\\ \\operatorname{Gam}\\left(n_i, \\beta\\right) &amp; \\operatorname{Gam}\\left(\\sum_i n_i, \\beta\\right) \\\\ \\operatorname{Pois}\\left(\\lambda_i\\right) &amp; \\operatorname{Pois}\\left(\\sum_i \\lambda_i\\right) \\\\ \\chi^2(1) &amp; \\chi^2(n) \\\\ \\chi^2\\left(n_i\\right) &amp; \\chi^2\\left(\\sum_i n_i\\right) \\\\ \\operatorname{Norm}\\left(\\mu_i, \\sigma_i^2\\right) &amp; \\operatorname{Norm}\\left(\\sum_i \\mu_i, \\sum_i \\sigma_i^2\\right) \\end{array} \\]   Convolutions can also be computed directly as follows: Definition 5.4 (Theorem) Let \\(X\\) and \\(Y\\) be two independent continuous random variables with possible values \\(\\mathcal{X}\\) and \\(\\mathcal{Y}\\) and let \\(Z=X+Y\\). Then, the probability density function of \\(Z\\) is given by \\[ \\begin{aligned} f_Z(z) &amp; =\\int_{-\\infty}^{+\\infty} f_X(z-y) f_Y(y) \\mathrm{d} y \\\\ \\text { or } \\quad f_Z(z) &amp; =\\int_{-\\infty}^{+\\infty} f_Y(z-x) f_X(x) \\mathrm{d} x \\end{aligned} \\] where \\(f_X(x), f_Y(y)\\) and \\(f_Z(z)\\) are the probability density functions of \\(X, Y\\) and \\(Z\\). Tip: For convolutions, this theorem provides an alternative to the multivariate Jacobian above.   Lastly, we can apply the below theorem to easily compute the moment generating function and characteristic function of a convolution: Definition 5.5 (Theorem 4.2.12) Let \\(X\\) and \\(Y\\) be independent random variables with moment generating \\(M_X(t)\\) and \\(M_Y(t)\\). Then the moment generating and characteristic functions of the random variable \\(Z=X+Y\\) are given by \\[ M_Z(t)=M_X(t) \\cdot M_Y(t) \\] \\[ \\varphi_Z(t) =\\varphi_X(t) \\cdot \\varphi_Y(t) \\] &lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD For the random variable \\(Z = X-Y\\), the moment generating and characteristic functions are analogously given by: ======= For the random variable \\(Z = X-Y\\), the moment generating and characteristic functions are given by: &gt;&gt;&gt;&gt;&gt;&gt;&gt; 111b97e1cc614f657524efe62f33b45cb7fd6858 \\[ M_Z(t)=M_X(t) \\cdot M_Y(-t) \\] \\[ \\varphi_Z(t) =\\varphi_X(t) \\cdot \\varphi_Y(-t) \\] "],["moments.html", "Chapter 6 Moments 6.1 Basic Definitions 6.2 \\(E(X)\\) Properties 6.3 \\(Var(X)\\) Properties 6.4 Covariance and Correlation 6.5 Conditional Expectation 6.6 Moment Generating Functions 6.7 Moment Inequalities 6.8 Techniques for Deriving Moments 6.9 Other Moments (for reference)", " Chapter 6 Moments Finding the moments of a random variable is a chief problem in statistics. This is because moments characterize important properties about a distribution - for example, the mean measures the central tendency of a random variable, while the variance measures its dispersion. This chapter will define expected value and moments, summarize their useful properties, and discuss strategies for finding moments, especially for common distributions. 6.1 Basic Definitions Definition 6.1 (Expected Value) The expected value \\(E(g(X))\\) is the average value of a random variable \\(g(X)\\) across its support \\(\\mathcal{X}\\), weighted by their probability. If \\(X\\) is discrete, then this is defined \\[E(g(X)) = \\sum_{x \\in \\mathcal{X}}g(x)P(X = x)\\] If \\(X\\) is continuous, then the expected value is \\[E(g(x)) = \\int_{\\mathcal{X}}g(x)f_X(x)dx\\] Definition 6.2 (Moments) The \\(n\\)th moment of a random variable \\(X\\) is defined as \\(E(X^n)\\). Similarly, the \\(n\\)th central moment is defined as \\(E((X - E(X))^n)\\). The first moment \\(E(X)\\) is also known as the mean. The second central moment is the variance, denoted \\(Var(X) = E((X - E(X))^2)\\) 6.2 \\(E(X)\\) Properties The linearity of expectation is defined as \\(E(aX + b) = aE(X) + b\\). If multiple random variables \\(X\\) and \\(Y\\) are involved, then \\[E(ag_1(X) + bg_2(Y) + c) = aE(g_1(X)) + bE(g_2(Y)) + c\\] This follows from the linearity of the integral operator. Since sums of random variables are so common, this property is incredibly useful, especially for proving the unbiasedness of estimators (see Chapter 8). For example, we can use the linearity of expectation to prove that the sample mean \\(\\bar{X} = \\frac{1}{n}\\sum_{i=1}^nX_i\\) is unbiased for a set of iid \\(X_i\\) by noting, by linearity of expectation, \\[E(\\bar{X}) = E\\Big(\\frac{1}{n}\\sum_{i=1}^nX_i\\Big) = \\frac{1}{n}\\sum_{i=1}^nE(X_i) = \\frac{n}{n}E(X_i) = E(X_i)\\] When \\(X\\) and \\(Y\\) are independent, \\[E(XY) = E(X)E(Y)\\] This property can also be useful for computing the expectation of iid random variables in statistical inference problems. 6.3 \\(Var(X)\\) Properties The most important variance property is its alternative definition: \\[Var(X) = E(X^2) - E(X)^2\\] Often, we are interested in both the mean and the variance. By simplifying \\(Var(X)\\) into a function of the first and second moments, we can compute \\(E(X^2)\\) (which is often much easier) and use what we know about \\(E(X)\\) to more easily compute \\(Var(X)\\). While variance is not exactly linear, the variance of a linear transformation of a random variable is \\[Var(aX + b) = a^2Var(X)\\] When multiple random variables are involved in a linear expression, then \\[Var(aX + bY) = a^2Var(X) + b^2Var(Y) + 2ab\\cdot Cov(X, Y)\\] where \\(Cov(X,Y)\\) is described in the next subsection. If \\(X\\) and \\(Y\\) are independent then \\(Cov(X,Y) = 0\\) and \\(Var(aX + bY) = a^2Var(X) + b^2Var(X)\\) 6.4 Covariance and Correlation From the expected value function and moments, we can also define the covariance and correlation Definition 6.3 (Covariance) Covariance measures the joint dispersion of two random variables. Mathematically, \\[Cov(X,Y) = E((X - E(X))(Y - E(Y)))\\] Equivalently, \\[Cov(X,Y) = E(XY) - E(X)E(Y)\\] which is usually the more convenient definition. Note that if \\(X\\) and \\(Y\\) are independent, then \\(Cov(X,Y) = 0\\), which simplifies calculations. However, make no mistake, this implication is not bidirectional: \\(Cov(X,Y) = 0\\) does NOT imply X, Y$ independent!!! Occasionally, we might want to work with a measure of joint dispersion that is normalized. This is called the correlation. Definition 6.4 (Covariance) Correlation is a measure of the joint dispersion of two random variables normalized to [0,1], with \\(Corr(X,Y) = 0\\) indicating that the variables are independent and \\(Corr(X,Y) = 1\\) indicating perfect collinearity. Mathematically, \\[Cov(X,Y) = E\\Big(\\frac{X - E(X)}{\\sqrt{Var(X)}}\\cdot \\frac{Y - E(Y)}{\\sqrt{Var(Y)}}\\Big) = \\frac{Cov(X,Y)}{\\sqrt{Var(X)Var(Y)}}\\] 6.5 Conditional Expectation When an expectation is computed using a conditional probability, it is known as a conditonal expectation. For discrete random variables (or when \\(Y\\) is simply an event), it is denoted \\[E(g(X)|Y) = \\sum_{\\mathcal{X}}g(x)P(X = x | Y)\\] and for continuous, \\[E(g(X) | Y) = \\int_{\\mathcal{X}}g(x)f_{X|Y}(x|y)\\] Two fundamental properties regarding conditional expectations exist that simply the computation of its unconditional counterpart. Definition 6.5 (Law of Iterated Expectation (Adam's Law)) \\(E(X) = E(E(X|Y))\\) Definition 6.6 (Law of Total Variance (EVVE's Law)) \\(Var(X) = E(Var(X|Y)) + Var(E(X|Y))\\) These two properties can be used to compute \\(E(X)\\) when only \\(E(X|Y)\\) is known. For example, this occurs in Chapter 8 when finding UMVUEs. 6.6 Moment Generating Functions Definition 6.7 (MGF) The moment generating function (MGF) is defined as \\[\\mathcal{M}_X(t) = E(e^{tx})\\] This function has three important properties: The MGF fully characterizes a distribution. That is, if \\(\\mathcal{M}_X(t) = \\mathcal{M}_Y(t)\\), then \\(X\\) and \\(Y\\) are identically distributed. \\(E(X^n) = \\frac{d^n}{dt^n}\\mathcal{M}_X(t)\\Big|_{t=0}\\). This means the MGF can also be used to compute any given moment simply by taking a derivative! Convolution: If \\(X\\) and \\(Y\\) and independent, then the mgf of \\(X+Y\\) is \\(\\mathcal{M}_{X+Y}(t) = \\mathcal{M}_{X}(t)\\mathcal{M}_{Y}(t)\\). This is useful for finding the distribution of sums of random variables. Do note, however, that the MGF may not exist for some distributions. In this case it may be preferable to work with the characteristic function. 6.7 Moment Inequalities Several inequalities exist that can bound moments. A general bound is that \\(a &lt; g(X) &lt; b \\implies a &lt; E(g(X)) &lt; b\\). This, coupled with the triangle inequality can be used to prove the following inequalities. The next two inequalities bound probabilities based on moments. They are named the student-teacher pair that developed them. Definition 6.8 (Markov's Inequality) \\[P(X \\geq t) \\leq \\frac{E(X)}{t}\\] To prove Markov’s inequality, Definition 6.9 (Chebychev's Inequality) This inequality can be stated in several ways: Chebychev’s inequality is instrumental in proving the Weak Law of Large Numbers Next, we present an equality regarding functions of moments. Definition 6.10 (Jensen's Inequality) For a convex function \\(f\\), \\[E(f(X)) \\geq f(E(X))\\] If \\(f\\) is instead concave, \\[E(f(x)) \\leq f(E(X))\\] Jensen’s inequality is useful for showing that an estimator is biased The next three inequalities are less commonly used, but are still useful is certain situations (where?) Definition 6.11 (Cauchy-Schwarz Inequality) \\[E(XY)^2 \\leq E(X)^2E(Y)^2\\] This equality can be generalized as follows Definition 6.12 (Holder's Inequality) For \\(p, q \\geq 1\\) such that \\(\\frac{1}{p} + \\frac{1}{q} = 1\\), \\[E(|XY|) \\leq E(|X|^p)^\\frac{1}{p}E(|Y|^q)^\\frac{1}{q}\\] While the above inequalities deal with products of moments, the following handles sums: Definition 6.13 (Minkowski's Inequality) \\[E(|X + Y|^p)^\\frac{1}{p} \\leq E(|X|^p)^\\frac{1}{p} + E(|Y|^p)^\\frac{1}{p}\\] 6.8 Techniques for Deriving Moments Below, we’ve listed the important moments of each distribution that can be reasonably derived by hand. In this section, we discuss a myriad of techniques to derive these moments, each of which can be applied in general for their respective distributions. Distribution \\(E(Y)\\) \\(Var(Y)\\) mgf \\(\\text{Bernoulli}(p)\\) \\(p\\) \\(p(1-p)\\) \\((1-p) + pe^t\\) \\(\\text{Binom}(n, p)\\) \\(np\\) \\(np(1-p)\\) \\(((1-p) + pe^t)^n\\) \\(\\text{Geo}(p)\\) \\(\\frac{1-p}{p}\\) \\(\\frac{1-p}{p^2}\\) \\(\\frac{p}{1 - (1 - p)e^t}\\) for \\(t &lt; -\\log(1-p)\\) \\(\\text{NegBinom(r, p)}\\) \\(\\frac{r(1-p)}{p}\\) \\(\\frac{r(1-p)}{p^2}\\) \\(\\Big(\\frac{p}{1 - (1 - p)e^t}\\Big)^r\\) for \\(t &lt; -\\log(p)\\) \\(\\text{Pois}(\\lambda)\\) \\(\\lambda\\) \\(\\lambda\\) \\(\\exp(\\lambda(e^t - 1))\\) \\(\\text{Normal}(\\mu, \\sigma^2)\\) \\(\\mu\\) \\(\\sigma^2\\) \\(\\exp(\\mu t + \\sigma^2 t^2 / 2)\\) \\(\\text{Exp}(\\lambda)\\) \\(\\lambda\\) \\(\\lambda^2\\) \\(\\frac{1}{1-\\lambda t}\\) for \\(t &gt; \\lambda\\) \\(\\text{Gamma}(k, \\lambda)\\) \\(k\\lambda\\) \\(k\\lambda^2\\) \\((1 - \\lambda t)^{-k}\\) for \\(t &lt; \\frac{1}{\\lambda}\\) \\(\\text{Beta}(\\alpha, \\beta)\\) \\(\\frac{\\alpha}{\\alpha + \\beta}\\) \\(\\frac{\\alpha\\beta}{(\\alpha + \\beta)^2(\\alpha + \\beta + 1)}\\) \\(1 + \\sum_{k=1}^\\infty\\Big(\\prod_{r=0}^{k-1}\\frac{\\alpha + r}{\\alpha + \\beta + r}\\Big)\\frac{t^k}{k!}\\) \\(\\chi^2(\\nu)\\) \\(\\nu\\) \\(2\\nu\\) \\((1 - 2t)^{-\\nu/2}\\) for \\(t &lt; \\frac{1}{2}\\) \\(\\text{Uniform}(a, b)\\) \\(\\frac{1}{2}(a+b)\\) \\(\\frac{1}{12}(b-a)^2\\) \\(\\begin{cases}\\frac{e^{tb}-e^{ta}}{t(b-a)} &amp; t \\neq 0 \\\\ 1 &amp; t = 0 \\\\\\end{cases}\\) \\(F(n, m)\\) \\(\\frac{m}{m - 2}\\) for \\(m &gt; 2\\) \\(\\frac{2m^2(n + m - 2)}{n(m - 2)^2(m - 4)}\\) for \\(m &gt; 4\\) Does Not Exist \\(\\text{HyperGeo}(N, K, n)\\) \\(\\frac{nK}{N}\\) \\(\\frac{nK(N-K)(N-n)}{N^2(N-1)}\\) Too complicated to reproduce here! 6.8.1 Bernoulli: Direct Summation The moments of a Bernoulli distribution are simple to compute, because \\(x\\) is only 0 or 1. When \\(x = 0\\), \\(0 * P(X = 0) = 0\\). Hence, \\(E(X) = 1 * P(X = 1) = p^1(1 - p)^{1 - 1} = p\\). Since \\(1^k = 1\\), a convenient property follows: \\[X \\sim \\text{Bernoulli}(p) \\implies E(X^k) = p\\] This also suggests that \\(E(X^k) = P(X = 1)\\), which can be exceptionally useful in many statistical problems, including finding UMVUEs. Similarly, we can compute the variance using the property \\(Var(X) = E(X^2) - E(X^2) = p - p^2 = p(1-p)\\). Finally, the moment generating function follows directly by noting \\[E(e^{tx}) = e^{t*0}p^0(1-p)^{1-0} + e^{t*1}p^1(1-p)^{1-1} = (1-p) + pe^t\\] 6.8.2 Uniform: Direct Integration Moments of distributions with simple pdfs bounded at both ends such as the Uniform can be solved directly by integrating their pdf. The Uniform has pdf \\(f_X(x) = \\frac{1}{b-a}\\), so its moments are as follows: \\[\\begin{align} E(X) = \\int_{a}^b \\frac{x}{b-a} dx = \\frac{x^2}{2(b-a)}\\Big|_{a}^b\\\\ = \\frac{b^2 - a^2}{2(b-a)} = \\frac{(b+a)(b-a)}{2(b-a)} = \\frac{1}{2}(b+a)\\\\ \\end{align}\\] Generalizing this, \\(E(X^k) = \\frac{b^k - a^k}{2(b-a)}\\). As a result, solving the variance is easier to do directly: \\[\\begin{align} Var(X) = \\int_{a}^b \\frac{(x - \\frac{1}{2}(b-a))^2}{b-a} dx \\\\ = \\frac{(x - \\frac{1}{2}(b-a))^3}{3(b-a)}\\Big|_{a}^b \\\\ = \\frac{(b - \\frac{1}{2}b - \\frac{1}{2}a)^3 - (a - \\frac{1}{2}b - \\frac{1}{2}a)^3}{3(b-a)}\\Big|_{a}^b\\\\ = \\frac{2(b-a)^3}{24(b-a)} = \\frac{1}{12}(b-a)^2\\\\ \\end{align}\\] 6.8.3 Geometric: Series Convergence Deriving the moments of the Geometric distribution requires the use of the Geometric Series (from where we can speculate its name originates). Note \\(E(X) = \\sum_{x=0}^\\infty xp(1-p)^x\\), which does not quite match the geometric series; first, we need to take the derivative, and then interchange differentiation and summation like so: \\[\\begin{align} \\sum_{x=0}^\\infty xp(1-p)^x = p(1-p)\\sum_{x=0}^\\infty x(1-p)^{x-1} &amp;&amp; \\text{factor out for correct form}\\\\ = p(1-p)\\sum_{x=0}^\\infty \\frac{d}{dx}-(1-p)^{x} &amp;&amp; \\text{notice derivative}\\\\ = -p(1-p) \\frac{d}{dx}\\sum_{x=0}^\\infty (1-p)^{x} &amp;&amp; \\text{interchange derivative}\\\\ = -p(1-p) \\frac{d}{dx}\\frac{1}{1 - (1 - p)} &amp;&amp; \\text{geometric series}\\\\ = \\frac{p(1-p)}{p^2} = \\frac{1-p}{p} &amp;&amp; \\\\ \\end{align}\\] This can also be performed to compute \\(E(X)\\) for the variance, though the computation is relatively long to be reproduced here. A quicker way might be to employ the moment generating function from which all moments can be computed, which can easily be found by substituting the geometric series: \\[\\begin{align} E(e^{tx}) = \\sum_{x=0}^\\infty e^{tx}(p(1-p)^x) \\\\ = p\\sum_{x=0}^\\infty ((1-p)e^t)^x) \\\\ = \\frac{p}{1 - (1-p)e^t} \\end{align}\\] 6.8.4 Binomial: Kernel Technique, Series Version Moments of the Binomial are trickier since they involve an infinite sum. Since the Binomial is a discrete distribution, we can compute its moments using the discrete moment formula \\(E(X) = \\sum_{i=1}^\\infty xP(X = x)\\). This introduces one technique for moment calculations: the Kernel Technique Example 6.1 (Kernel Technique with Infinite Series) Fact: pmfs integrate to 1. That is, \\(\\sum_{x=0}^\\infty f_X(x) = 1\\). We can use this fact to compute moments by: Recognizing the kernel of the distribution within the moment formula Factoring out appropriate constants to turn the kernel into the full pmf, and simplifying the infinite series to \\(\\sum_{x=0}^\\infty f_X(x) = 1\\). The left-over components then, are the value of the moment. Binomial Distribution: We can use this technique to compute moments of the Binomial distribution like so: \\[\\begin{align} E(X) = \\sum_{x=0}^\\infty xP(X = x) = \\sum_{x=0}^\\infty x{n\\choose x}p^x(1-p)^{n-x} &amp;&amp; \\\\ = \\sum_{x=0}^\\infty \\frac{x\\cdot n!}{x!(n-x)!}p^x(1-p)^{n-x} &amp;&amp; \\text{(kernel)} \\\\ = 0 + \\sum_{x=1}^\\infty \\frac{n \\cdot (n-1)!}{(x-1)!((n-1) - (x - 1)!}\\cdot p \\cdot p^{x-1}(1-p)^{(n - 1) - (x - 1)} &amp;&amp; \\text{(form pmf)}\\\\ = np\\sum_{x=0}^\\infty \\cdot {n - 1 \\choose x}p^x(1-p)^{(n-1) - x} &amp;&amp; \\text{(sum pmf to 1)}\\\\ = np \\end{align}\\] To compute the \\(E(X^2)\\) component of the variance, this process needs to be repeated twice: \\[\\begin{align} E(X^2) = \\sum_{x=0}^\\infty x^2P(X = x) = \\sum_{x=0}^\\infty x^2{n\\choose x}p^x(1-p)^{n-x} &amp;&amp; \\\\ = np \\sum_{x=0}^\\infty(x+1) \\frac{(n-1)!}{x!(n-1-x)!}p^x (1-p)^{n-x-1} &amp;&amp; \\text{(from E(X))}\\\\ = np(0 + (n-1)p\\sum_{x=1}^\\infty \\frac{(n-2)!}{(x-1)!((n-2)-(x-1))!}p^{x-1}(1-p)^{(n-2)-(x-1)} + 1 &amp;&amp; \\\\ = np((n-1)p + 1) = (np)^2 + np(1-p) &amp;&amp; \\text{(pmf sums to 1)}&amp;&amp; \\\\ \\end{align}\\] Then, \\(Var(X) = E(X^2) - E(X)^2 = (np)^2 + np(1-p) - (np)^2 = np(1-p)\\) Alternatively, we could have computed this using the fact that the Binomial is equal to a sum of Bernoulli random variables. By the linearity of expectation, if \\(X_i \\sim \\text{Bernoulli}(p)\\), then \\(E(\\sum_{i=1}^n X_i) = n\\cdot E(X_i) = np\\). \\(Var(X)\\) follows similarly. 6.8.5 Negative Binomial and Hypergeometric: Computing \\(E(X(X-1))\\) The second moment of distributions with pmfs/pdfs involving factorials can often be computing more easily by finding \\(E(X(X-1))\\) instead of \\(E(X^2)\\) directly. The Negative Binomial is one such distribution. Its mean can be computed by the same Kernel Technique used for the Binomial: \\[\\begin{align} E(X) = \\sum_{x=0}^\\infty x{x + r - 1 \\choose x} \\cdot (1-p)^x p^r &amp;&amp; \\\\ = (1-p)\\sum_{x=1}^\\infty \\frac{((x-1) + r)!}{(x-1)!(r-1)!}\\cdot (1-p)^{x-1}p^{r} &amp;&amp; \\\\ = \\frac{r(1-p)}{p}\\sum_{x=1}^\\infty \\frac{((x-1) + r)!}{(x-1)!(r-1)!}\\cdot (1-p)^{x-1}p^{r} &amp;&amp; \\\\ = \\frac{r(1-p)}{p}\\sum_{x=0}^\\infty \\frac{((x-1) + r)!}{(x-1)!r!}\\cdot (1-p)^{x-1}p^{r+1} &amp;&amp; \\\\ = \\frac{r(1-p)}{p} &amp;&amp; \\text{pmf sums to 1} \\end{align}\\] With \\(E(X)\\) known, we can now take advantage of the factorial to compute \\(E(X(X-1))\\), using the same technique of simplifying the combinatorial fraction and “pulling out” components to reform a pdf: \\[\\begin{align} E(X(X-1)) = \\sum_{x=0}^\\infty x(x-1){x + r - 1 \\choose x}(1-p)^x p^r \\\\ = \\frac{r(r+1)}{1-p)^2}{p^2}\\sum_{x=0}^\\infty \\frac{((x-2) + (r+1))!}{(x-2)!(r+1)!}(1-p)^{x-2}p^{r+2}\\\\ = \\frac{r(r+1)}{1-p)^2}{p^2}\\Big(0 + 0 + \\sum_{x=2}^\\infty {(x-2) + r + 1\\choose x-2}(1-p)^{x-2}p^{r+2}\\\\ = \\frac{r(r + 1)(1-p)^2}{p^2} \\text{ pmf sums to 1}\\\\ \\end{align}\\] Since, by linearity of expectation, \\(E(X(X-1)) = E(X^2) - E(X)\\), we can write \\[\\begin{align} Var(X) = E(X^2) - E(X) + E(X) - E(X)^2 \\\\ = E(X(X-1)) + E(X) - E(X)^2 \\\\ = \\frac{r(r+1)(1-p)^2}{p^2} + \\frac{r(1-p)}{p} - \\frac{r^2(1-p)^2}{p^2}\\\\ = \\frac{r(1-p)^2}{p^2} + \\frac{r(1-p)}{p} \\\\ = \\frac{r(1-p)(1 - p + p)}{p^2} = \\frac{r(1-p)}{p^2} \\end{align}\\] Note: The moments of a Negative Binomial can also be computed simply by relying on its additive property in relation to the geometric, and then using the linearity expectation. That is, if \\(Y \\sim \\text{NegBin}(r, p)\\), then \\(Y = \\sum_{i=1}^r X_i\\) where \\(X_i \\sim \\text{Geo}(p)\\). Then, \\(E(Y) = E(\\sum_{i=1}^r X_i) = \\frac{r(1-p)}{p}\\). Hypergeometric Distribution: The above technique can also be used to find the variance of a \\(\\text{HGeo}(N, K, n)\\) random variable. First, let us use the Kernel Technique to compute its first moment. Writing the combinatorial functions in their full form, noting in general \\(y - x = (y - 1) - (x - 1)\\), we can rewrite this as the pmf of a \\(\\text{HGeo}(N-1, K-1, n-1)\\): \\[\\begin{align} E(X) = \\sum_{x=0}^n x \\cdot \\frac{K!}{x!(K-x)}\\cdot\\frac{(N-K)!}{(n-x)!(N-K-n+x)!} \\cdot \\frac{n!(N-n)!}{N!}\\\\ = \\sum_{x=1}^n \\frac{K(K-1)!}{(x-1)!((K-1) - (x-1))!} \\\\ \\cdot \\frac{((N-1) - (K - 1))!}{((n-1) - (x-1))!((N-1) - (k-1) - (n-1) + (x-1))!}\\cdot\\frac{n(n-1)!((N-1) - (n-1))!}{N(N-1)!}\\\\ = \\frac{NK}{n}\\sum_{x=0}^{n-1} \\frac{{K-1\\choose x}{(N-1) - (K-1)\\choose (n-1) - x}}{{N-1\\choose n-1}}\\\\ = \\frac{NK}{n} \\end{align}\\] For the variance, we first compute \\(E(X(X-1))\\) in the same fashion. For brevity, we exclude writing down \\(y - x = (x - 2) - (y-2)\\) expansion, but the calculations below do rely on this principle. \\[\\begin{align} E(X(X-1)) = \\sum_{x=0}^n x(x-1) \\cdot \\frac{K!}{x!(K-x)}\\cdot\\frac{(N-K)!}{(n-x)!(N-K-n+x)!} \\cdot \\frac{n!(N-n)!}{N!}\\\\ = \\sum_{x=2}^n \\frac{K(K-1)!}{(x-1)!(K - X)!} \\\\ \\cdot \\frac{(N - K)!}{((n-2) - x-2))!(N - k - (n-2) + (x-2))!}\\cdot\\frac{n(n-1)(n-2)!((N-2) - (n-2))!}{N(N-1)(N-2)!}\\\\ = \\frac{N(N-1)K(K-1)}{n(n-1)}\\sum_{x=0}^{n-2} \\frac{{K-2\\choose x}{(N-2) - (K-2)\\choose (n-2) - x}}{{N-2\\choose n-2}}\\\\ = \\frac{N(N-1)K(K-1)}{n(n-1)} \\end{align}\\] Then, \\[\\begin{align} Var(X) = E(X(X-1)) + E(X) - E(X)^2\\\\ = \\frac{K(K-1)N(N-1)}{n(n-1)} + \\frac{KN}{n} - \\frac{KN}{n^2}\\\\ = n\\frac{K}{N} \\cdot \\frac{N - K}{N}\\cdot \\frac{N - n}{N-1} \\end{align}\\] after lengthy algebra. 6.8.6 Poisson: Exponential Taylor Series Like the Geometric, we can also derive the moments of the Poisson by relying the convergence of an infinite series. This time, we rely on the Taylor Series for the exponential distribution, which is \\(\\sum_{x=0}^\\infty \\frac{\\lambda^x}{x!} = e^\\lambda\\). Proceed as follows: \\[\\begin{align} E(X) = \\sum_{x=0}^\\infty x\\frac{e^{-\\lambda}\\lambda^x}{x!} &amp;&amp; \\\\ = \\lambda e^{-\\lambda}\\sum_{x=1}^\\infty \\frac{\\lambda^{x-1}}{(x-1)!} &amp;&amp; \\text{when }x = 0, \\text{ the series term is 0}\\\\ = \\lambda e^{-\\lambda}e^{\\lambda} = \\lambda &amp;&amp; \\text{exponential series} \\end{align}\\] 6.8.7 Exponential: Integration By Parts Sometimes, a pdf can be integrated directly using more advanced integration techniques. The Exponential is one such distribution - its moments can be computed using integration by parts For the mean, note \\(E(X) = \\int_0^\\infty \\lambda e^{-\\lambda x}dx\\). Let \\(u = x \\implies du = 1\\) and \\(du = \\lambda e^{-\\lambda x} \\implies v = -e^{-\\lambda x}\\). Then, \\[\\begin{align} E(X) = uv - \\int_{0}^\\infty vdu = -xe^{-\\lambda x}\\Big|_{0}^\\infty + \\int_0^\\infty e^{-\\lambda x}dx\\\\ = 0 + 0 - \\frac{1}{\\lambda}e^{-\\lambda x} \\Big|_0^\\infty = \\frac{1}{\\lambda} \\end{align}\\] noting that \\(\\lim_{x\\rightarrow\\infty}xe^{-\\lambda x} = \\lim_{x\\rightarrow\\infty}-\\lambda e^{-\\lambda x} = 0\\) by applying L’Hopital’s rule. Hence \\(E(X) = \\frac{1}{\\lambda}\\) For the variance, start by computing \\(E(X^2)\\). Applying integration by parts with \\(u = x^2 \\implies du = 2x\\) and \\(dv = \\lambda e^{-\\lambda x} \\implies -e^{-\\lambda x}\\), \\[E(X^2) = uv - \\int_{0}^\\infty vdu = -x^2e^{-\\lambda x} \\Big|_0^\\infty + \\int_{0}^\\infty 2xe^{-\\lambda x}\\] Now, we could apply integration by parts again, but a faster way to solve this is by moment recognization: noting that the second term can be transformed to equal \\(E(X)\\) like so: \\(\\int_{0}^\\infty 2xe^{-\\lambda x} = \\frac{2}{\\lambda}\\int_{0}^\\infty x\\lambda e^{-\\lambda x} = E(X) = \\lambda\\) as we’ve already solved. As before, the first term is \\(\\lim_{x\\rightarrow \\infty}-x^2e^{-\\lambda x} \\Big|_0^\\infty = \\lim_{x\\rightarrow\\infty}\\lambda^2e^{-\\lambda x} = 0\\) by applying L’Hopital’s rule twice. Plugging in \\(E(X) = \\frac{1}{\\lambda}\\), we get \\(E(X^2)\\) = \\(\\frac{2}{\\lambda^2}\\) and \\[Var(X) = E(X^2) - E(X)^2 = \\frac{2}{\\lambda^2} - \\frac{1}{\\lambda^2} = \\frac{1}{\\lambda^2}\\] 6.8.8 Gamma and Beta: Kernel Technique, Integration Version Similar to discrete distributions, we can also use the Kernel Technique to more easily integrate continuous distributions as well. For example, noting Gamma function property $(k) = , \\[\\begin{align} E(X) = \\int_{0}^\\infty \\frac{x}{\\Gamma(k)\\lambda^k}x^{k-1}e^{-\\frac{1}{\\lambda}x}dx &amp;&amp;\\\\ = \\int_{0}^\\infty \\frac{\\lambda}{(\\Gamma(k+1) / k)\\lambda^{k+1}}x^k e^{-\\frac{1}{\\lambda}x}dx &amp;&amp; \\text{recognize Gamma kernel}\\\\ = \\lambda k \\int_{0}^\\infty \\frac{1}{\\Gamma(k+1)\\lambda^{k+1}}x^k e^{-\\frac{1}{\\lambda}x}dx &amp;&amp; \\text{pull out excess terms}\\\\ = \\lambda k &amp;&amp; \\text{ integrate } Gamma(k+1, \\lambda) \\text{ to 1} \\end{align}\\] Similarly, the variance of the Gamma can be calculated like so: Solve for \\(E(X^2)\\) \\[\\begin{align} E(X^2) = \\int_{0}^\\infty \\frac{x^2}{\\Gamma(k)\\lambda^k}x^{k-1}e^{-\\frac{1}{\\lambda}x}dx = \\int_{0}^\\infty \\frac{\\lambda^2}{(\\Gamma(k+2) / (k(k+1)))\\lambda^{k+2}}x^{k+1} e^{-\\frac{1}{\\lambda}x}dx\\\\ = \\lambda^2k(k+1) \\text{ Pull out excess terms from } Gamma(k+1, \\lambda) \\text{ kernel, integrate to 1} \\end{align}\\] Compute variance using \\(Var(X) = E(X^2) - E(X)^2\\) using the components solved previously. \\[Var(X) = \\lambda^2k(k+1) - \\lambda^2k^2 = \\lambda^2k\\] Since the Beta distribution also involves Gamma function, we can compute the moments in a similar fashion. Let’s start with the mean: \\[\\begin{align} E(X) = \\int_{0}^\\infty x \\cdot\\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)}x^{\\alpha-1}(1-x)^{\\beta-1}dx &amp;&amp; \\\\ = \\frac{\\alpha}{\\alpha + \\beta}\\int_{0}^{1} \\frac{\\Gamma(\\alpha + 1 + \\beta)}{\\Gamma(\\alpha + 1)\\Gamma(\\beta)}x^\\alpha (1-x)^{\\beta-1}dx &amp;&amp; \\text{form kernel of } Beta(\\alpha+1, \\beta)\\\\ = \\frac{\\alpha}{\\alpha + \\beta} &amp;&amp; \\text{ integrate Beta pdf to 1}\\\\ \\end{align}\\] Now, we apply this same principle twice to compute the variance: Compute \\(E(X^2)\\) by using \\(\\Gamma(\\alpha) = \\alpha(\\alpha+1)\\Gamma(\\alpha+2)\\) and \\(\\Gamma(\\alpha + \\beta) = (\\alpha + \\beta)(\\alpha + \\beta + 2)\\) to form the Beta kernel: \\[\\begin{align} E(X^2) = \\int_{0}^1 x^2 \\cdot \\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)}x^{\\alpha-1}(1 - x)^{\\beta-1}dx &amp;&amp; \\\\ = \\frac{\\alpha(\\alpha+1)}{(\\alpha + \\beta)(\\alpha + \\beta + 1)}\\int_{0}^1x^{\\alpha-1}(1-x)^{\\beta-1}dx &amp;&amp; \\text{form kernel of } Beta(\\alpha + 2, \\beta)\\\\ = \\frac{\\alpha(\\alpha+1)}{(\\alpha + \\beta)(\\alpha + \\beta + 1)} \\text{integrate Beta pdf to 1} \\end{align}\\] Compute the variance using \\(Var(X) = E(X^2) - E(X)^2\\) \\[\\begin{align} Var(X) = E(X^2)\\frac{\\alpha^2 + \\alpha}{(\\alpha + \\beta)(\\alpha + \\beta + 1)} - \\frac{\\alpha^2}{(\\alpha + \\beta)^2}\\\\ = \\frac{\\alpha^2(\\alpha + \\beta) + \\alpha(\\alpha + \\beta) - \\alpha^2(\\alpha + \\beta + 1)}{(\\alpha + \\beta)^2(\\alpha + \\beta + 1)}\\\\ = \\frac{\\alpha\\beta}{(\\alpha + \\beta)^2(\\alpha + \\beta + 1)} \\end{align}\\] Virtually identical steps are used to compute the moments of the Chi-squared (which is a special case of the Gamma) and F distributions as well. 6.8.9 Normal: Location-Scale Trick and Polar Integration Proving that the moments of the \\(\\text{Normal}(\\mu, \\sigma^2)\\) can be a bit tricky. However, if we use the fact that it is a location-scale family, it becomes much easier. Letting \\(X = \\sigma Z + \\mu\\), then \\[E(X) = \\sigma E(Z) + \\mu\\] by linearity of expectation. Since \\(Z \\sim N(0,1)\\), noting that the antiderivative of \\(z\\exp(-\\frac{1}{2}z^2)\\) is \\(-\\exp(-\\frac{1}{2}z^2)\\), we get \\[\\begin{align} E(Z) = \\int_{\\infty}^\\infty \\frac{z}{\\sqrt{2\\pi}}\\exp(-\\frac{1}{2}z^2)dx\\\\ = -\\exp(-\\frac{1}{2}z^2)\\Big|_{-\\infty}^\\infty = 0 \\end{align}\\] since \\(\\lim_{z^2\\rightarrow\\infty} \\exp(-\\frac{1}{2}z^2) = 0\\). Therefore, \\(E(X) = E(\\sigma Z + \\mu) = \\sigma E(Z) +\\mu = \\mu\\). Computing the variance necessitates a more advanced integration technique: Polar Coordinates. Proceed with Integration by Parts, letting \\(u = \\frac{z}{\\sqrt{2\\pi}} \\implies du = \\frac{1}{\\sqrt{2\\pi}}\\) and \\(dv = z\\exp(-\\frac{1}{2}z^2)\\implies v = -\\exp(-\\frac{1}{2}z^2)\\) as used previously. Then, \\[\\begin{align} E(Z^2) = \\int_{\\infty}^\\infty \\frac{z^2}{\\sqrt{2\\pi}}\\exp(-\\frac{1}{2}z^2)dx\\\\ = \\frac{z}{\\sqrt{2\\pi}}\\exp(-\\frac{1}{2}z^2)\\Big|_{-\\infty}^\\infty + \\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^\\infty \\exp(-\\frac{1}{2}z^2)dz \\end{align}\\] By L’Hopital’s rule, \\[\\lim_{z\\rightarrow\\infty} \\frac{z}{\\sqrt{2\\pi}}\\exp(-\\frac{1}{2}z^2) = \\lim_{z\\rightarrow\\infty}\\frac{1}{\\sqrt{2\\pi}z\\exp(\\frac{1}{2}z^2)} = 0\\] so \\(\\frac{z}{\\sqrt{2\\pi}}\\exp(\\frac{1}{2}z^2)\\Big|_{-\\infty}^\\infty = 0\\). But how do we solve the second integral? This is where polar coordinates come into play. From Strang (2010), if \\(A = \\int_{-\\infty}^\\infty \\exp(-\\frac{1}{2}x^2)dx\\), then we can solve the integral by converting into polar like so: \\[\\begin{align} A^2 = \\int_{-\\infty}^\\infty \\exp(-\\frac{1}{2}x^2)dx \\cdot \\int_{-\\infty}^\\infty \\exp(-\\frac{1}{2}xy^2)dy\\\\ = \\int_{-\\infty}^\\infty\\int_{-\\infty}^\\infty \\exp(-\\frac{1}{2}(x^2 + y^2))dxdy \\\\ = \\int_{0}^{2\\pi}\\int_{0}^\\infty r\\exp(-\\frac{1}{2}r^2(\\cos^2(\\theta) + \\sin^2(\\theta)))drd\\theta\\\\ = \\int_{0}^{2\\pi}\\int_{0}^\\infty r\\exp(-\\frac{1}{2}r^2)drd\\theta\\\\ = 2\\pi \\implies A = \\sqrt{2\\pi} \\end{align}\\] Hence, \\(E(Z^2) = 0 + \\frac{\\sqrt{2\\pi}}{\\sqrt{2\\pi}} = 1\\) and therefore, \\[\\begin{align} Var(X) = E((\\sigma Z + \\mu - E(Z))^2) \\\\ = E((\\sigma Z + \\mu - \\mu)^2) = \\sigma^2E(Z^2) \\\\ = \\sigma^2 \\end{align}\\] Hence, we have proven that, for \\(X \\sim \\text{Normal}(\\mu, \\sigma^2)\\), that the mean is \\(\\mu\\) and variance is \\(\\sigma^2\\) - as we expected (ba-dum tss). 6.9 Other Moments (for reference) Distribution \\(E(Y)\\) \\(Var(Y)\\) mgf \\(\\text{Weibull}(k, \\lambda)\\) \\(\\lambda\\Gamma(1 + \\frac{1}{k})\\) \\(\\lambda^2\\Big(\\Gamma(1 + \\frac{2}{k}) - (\\Gamma(1 + \\frac{1}{k}))^2\\Big)\\) \\(\\sum_{n=0}^\\infty\\frac{t^n \\lambda^n}{n!}\\Gamma(1 + \\frac{n}{k})\\), \\(k \\geq 1\\) \\(\\text{Pareto}(x_m, \\alpha)\\) \\(\\begin{cases}\\infty &amp; \\alpha \\leq 1 \\\\ \\frac{\\alpha x_m}{a - 1} &amp; \\alpha &gt; 1\\end{cases}\\) \\(\\begin{cases}\\infty &amp; \\alpha \\leq 2 \\\\ \\frac{x_m^2 \\alpha}{(a-1)^2(\\alpha - 2)} &amp; \\alpha &gt; 2\\end{cases}\\) Does not exist \\(\\text{Cauchy}(x_0, \\gamma)\\) Does Not Exist Does Not Exist \\(\\exp(x_0it - \\gamma|t|\\) (cf) \\(t(\\nu)\\) 0 \\(\\begin{cases}\\frac{\\nu}{\\nu-2} &amp; \\nu &gt; 2\\\\ \\infty &amp; 1 &lt; \\nu \\leq 2 \\\\ \\text{undefined} &amp; \\text{otherwise}\\end{cases}\\) Does Not Exist References "],["statistics.html", "Chapter 7 Statistics 7.1 Table of Sufficient Statistics 7.2 Moments of the Sufficient Statistic", " Chapter 7 Statistics Proving sufficient, complete, and ancillary statistics Pitman-Koopman-Darmois Theorem 7.1 Table of Sufficient Statistics Translated from https://en.wikipedia.org/wiki/Exponential_family Distribution Parameter Sufficient Statistic S.S. Distribution Bernoulli p \\(\\sum_{i=1}^n x_i\\) \\(Binomial(n, p)\\) Binomial p \\(\\sum_{i=1}^n x_i\\) \\(Binomial(nm, p)\\) Poisson \\(\\lambda\\) \\(\\sum_{i=1}^n x_i\\) \\(Poisson(n\\lambda)\\) Negative Binomial p \\(\\sum_{i=1}^n x_i\\) Exponential \\(\\lambda\\) \\(\\sum_{i=1}^n x_i\\) \\(Gamma(n, \\lambda)\\) Normal (known \\(\\sigma^2\\)) \\(\\mu\\) \\(\\frac{1}{\\sigma}\\sum_{i=1}^n x_i\\) Normal \\(\\mu\\), \\(\\sigma^2\\) \\((\\sum_{i=1}^n x_i, \\sum_{i=1}^n x_i^2)\\) Chi-Squared \\(\\nu\\) \\(\\sum_{i=1}^n \\log(x_i)\\) Pareto (known min \\(x_m\\)) \\(\\alpha\\) \\(\\sum_{i=1}^n \\log(x_i)\\) Gamma \\(\\alpha, \\beta\\) \\((\\sum_{i=1}^n\\log(x_i), \\sum_{i=1}^n x_i)\\) Beta \\(\\alpha, \\beta\\) \\((\\sum_{i=1}^n\\log(x_i), \\sum_{i=1}^n\\log(1 - x_i))\\) Weibull (known shape \\(k\\)) \\(\\lambda\\) \\(\\sum_{i=1}^n x^k\\) 7.1.1 A Note on Distributions of Sufficient Statistics Recall a convenient property of exponential families: that their maximum likelihood estimate is a function of their sufficient statistic. Because of this, in order to prove Finite Sample Properties of an estimator or construct Hypothesis Tests, it is often useful to understand their distributions. This is why the distributions of each of the sufficient statistics are included in the fourth column of the table above. These distributions are mostly derived from additive, location-scale, and other properties in Chapter 4 - Known Distributions 7.2 Moments of the Sufficient Statistic As stated by Casella and Berger (1990), if \\(X\\) is an exponential family, the moments of its exponential family can be easily computed using certain properties. ::: {.theorem name = “I Don’t Know What To Call This”} If \\(X\\) is an exponential family, then \\[E\\Big(\\sum_{i=1}^k \\frac{\\partial w_i(\\theta)}{\\partial \\theta_j}t_i(X)\\Big) = -\\frac{\\partial}{\\partial\\theta_j} \\log c(\\theta)\\] and \\[Var\\Big(\\sum_{i=1}^k \\frac{\\partial w_i(\\theta)}{\\partial \\theta_j}t_i(X)\\Big) = -\\frac{\\partial}{\\partial\\theta_j} \\log c(\\theta) - E\\Big(\\sum_{i=1}^k \\frac{\\partial^2w_i(\\theta)}{\\partial \\theta_j^2}t_i(X)\\Big)\\] ::: If \\(X\\) is a natural exponential family, then these identities simplify even further. ::: {.theorem name = “I Don’t Know What To Call This 2”} If \\(X\\) is a natural exponential family, then$ \\[E(t_j(X)) = -\\frac{\\partial}{\\partial\\eta_j}\\log(c^*(\\eta))\\] and \\[Var(t_j(X)) = -\\frac{\\partial^2}{\\partial\\eta_j^2}\\log(c^*(\\eta))\\] ::: Using these theorems, we can calculate the moments of the sufficient statistics of exponential families directly. This is because if \\(T(X)\\) is a sufficient statistic, then \\(T(X) = \\Big(\\sum_{i=1}^nt_1(X), ..., \\sum_{i=1}^nt_k(X)\\Big)\\). Suppose, for simplicity, that \\(T(X)\\) is one-dimensional and \\(X_i\\) are iid. Then, if \\(X_i\\) is a natural exponential family, \\[E(T(X)) = E\\Big(\\sum_{i=1}^nt(X_i)\\Big) = nE(t(X_i)) = n\\cdot-\\frac{\\partial}{\\partial \\eta}\\log(c^*(\\eta))\\] In fact, letting \\(A(\\eta) = -\\log(c^*(\\eta))\\), we can obtain all of the moments of \\(T(X)\\) by simply differentiating \\(A(\\eta)\\) References "],["point-estimators-finite-samples.html", "Chapter 8 Point Estimators: Finite Samples 8.1 Method of Moments 8.2 MLE Theory 8.3 Unbiasedness 8.4 Minimum Variance (Cramer-Rao Lower Bound) 8.5 Uniform Minimum Variance Unbiased Estimators (UMVUEs) 8.6 Inferential Properties of Exponential Families Distributions", " Chapter 8 Point Estimators: Finite Samples Finding point estimators and evaluating their finite sample properties. 8.1 Method of Moments 8.2 MLE Theory Definition 8.1 (Likelihood) Let \\(X\\) be a random variable or random vector, and \\(\\theta\\) a parameter or vector of parameters describing the distribution of \\(X\\). Then, the likelihood is \\[\\mathcal{L}(\\theta | X) = f_X(x|\\theta)\\] Definition 8.2 (Log-Likelihood) Let \\(X\\) be a random variable or random vector, and \\(\\theta\\) a parameter or vector of parameters describing the distribution of \\(X\\). Then, the log-likelihood is \\[\\ell(\\theta|X) = \\log\\mathcal{L}(\\theta | X) = \\log f_X(x|\\theta)\\] The log-likelihood is often easier to use. Furthermore, When \\(X\\) is an iid sample \\(X_1, X_2, ... X_n\\), by the properties of logarithms, \\(\\ell(\\theta|X) = \\log\\Big(\\prod_{i=1}^nf_{X_i}(x|\\theta)\\Big) = \\sum_{i=1}^n \\log f_{X_i}(x|\\theta)\\). Working with a sum is much easier than with a product. Of course, the whole reason this is permissible in finding a maximum likelihood estimate is that the \\(\\log\\) function is monotonic - finding the maximum of a function is equivalent to finding the maximum of its logarithm. Maximizing this likelihood often requires calculus - specifically, the First Derivative Test. In statistics, the gradient of the log-likelihood has a special name: the score. Definition 8.3 (Score Equations) If \\(\\theta\\) is a vector of parameters, the score equations are defined as the gradient of the log-likelihood (as described above). That is, \\[U(\\theta | X) = \\begin{bmatrix}\\frac{\\partial}{\\partial\\theta_1}\\ell(\\theta_1|X) &amp; ... &amp; \\frac{\\partial}{\\partial\\theta_k}\\ell(\\theta_k|X) \\end{bmatrix}\\] Of course, if \\(\\theta\\) is a single parameter, then \\(U(\\theta|X) = \\frac{d}{d\\theta}\\ell(\\theta|X)\\) Therefore, the first step in finding an MLE is to set the score to 0, and solve for the desired parameter. Let’s see an example. Example 8.1 (MLE of _______) example The score has two useful properties in MLE theory. First, under regularity conditions, \\(E(U(\\theta|X)) = 0\\) - its mean is 0. This is because the regularity conditions imply Leibniz’s rule; that is, \\[\\begin{align} E(U(\\theta|X)) = \\int_{\\mathcal{X}}\\frac{\\partial}{\\partial\\theta}\\log\\mathcal{L}(\\theta|X)f(x|\\theta)dx \\\\ = \\int_{\\mathcal{X}}\\frac{1}{f(x|\\theta)}\\frac{\\partial}{\\partial\\theta}f(x|\\theta)f(x|\\theta)dx \\\\ = \\frac{\\partial}{\\partial\\theta}\\frac{f(x|\\theta)}{f(x|\\theta)}f(x|\\theta)dx = \\frac{\\partial}{\\partial\\theta}1 = 0 \\end{align}\\] The second useful property is that, under the same regularity conditions, the variance of the score is \\[\\begin{align} Var(U(\\theta|X)) = E(U(\\theta|X)U(\\theta|X)^\\top) \\\\ = -E\\Big(\\frac{\\partial^2}{\\partial\\partial^\\top}\\ell(\\theta|X)\\Big) \\end{align}\\] If \\(\\theta\\) is one-dimensional, \\(Var(U(\\theta|X)) = E(U(\\theta|X)^2) = -E\\Big(\\frac{\\partial^2}{\\partial\\theta^2}\\ell(\\theta|X)\\Big)\\). The rather insightful derivation of this is available here. This variance has a special name, the information (or sometimes, “Fisher Information”) Definition 8.4 (Information) In statistics, information refers to the amount of information that a random variable \\(X\\) contains about a parameter \\(\\theta\\). It is defined mathematically as the variance of the score, which is \\[I(\\theta) = E\\Big((\\frac{\\partial}{\\partial\\theta}\\log f(X|\\theta))^2\\Big|\\theta\\Big)\\] Under MLE regularity conditions, \\[I(\\theta) = -E\\Big(\\frac{\\partial^2}{\\partial\\theta^2}\\log f(X|\\theta) \\Big|\\theta\\Big)\\] As the mean value of a second derivative, the information measures the curve of \\(\\ell(\\theta|X)\\) (expand?) The negative second derivative of the log-likelihood also has a special name itself: the observed information. Definition 8.5 (Observed Information) The observed information is defined as \\[\\mathcal{J}(\\theta|X) = -\\frac{\\partial^2}{\\partial\\theta^2}\\ell(\\theta|X)\\] When \\(\\theta\\) is a \\(k\\)-dimensional vector, the observed information is the Hessian of the log-likelihood: \\[\\mathcal{J}(\\theta|X) = -\\nabla\\nabla^\\top\\ell(\\theta|X) = \\begin{bmatrix}\\frac{\\partial^2}{\\partial\\theta_1^2} &amp; ... &amp; \\frac{\\partial^2}{\\partial\\theta_1\\partial\\theta_k}\\\\ \\vdots &amp; &amp; \\vdots\\\\ \\frac{\\partial^2}{\\partial\\theta_k\\partial\\theta_1} &amp; ... &amp; \\frac{\\partial^2}{\\partial\\theta_k^2}\\end{bmatrix}\\] The observed information can be used to estimate the Fisher information for a given sample \\(X\\). 8.3 Unbiasedness 8.4 Minimum Variance (Cramer-Rao Lower Bound) 8.5 Uniform Minimum Variance Unbiased Estimators (UMVUEs) Definition 8.6 (Lehmann-Scheffe Theorem) This theorem 8.6 Inferential Properties of Exponential Families Distributions Suppose we draw a sample of \\(n\\) iid random variables \\(X_1,...,X_n\\) following one of the distributions below. The proceeding tables list the inferential properties of this sample. 8.6.1 Bernoulli Log-likelihood \\(\\ell(\\theta|X) = n\\log(1-p) + \\log(\\frac{p}{1-p})\\sum_{i=1}^n x_i\\) Score Equations \\(U_n(\\theta|X) = -\\frac{n}{1-p} + \\frac{1}{p(1-p)}\\sum_{i=1}^n x_i\\) Observed Information \\(-\\frac{\\partial^2}{\\partial\\theta^2}\\ell(\\theta|X) = \\frac{n}{(1-p)^2} + \\frac{1-2p}{p^2(1-p)^2}\\sum_{i=1}^n x_i\\) Fisher Information \\(I(\\theta)= \\frac{n}{p(1-p)}\\) MLE \\(\\frac{1}{n}\\sum_{i=1}^n x_i\\) 8.6.2 Binomial Since the Binomial has \\(n\\) as a parameter, notation in problems that involve a sample of \\(n\\) iid Binomial random variables can be tricky. To clarify, in the following table let \\(X_i \\sim \\text{Binomial}(m, p)\\), and let \\(n\\) represent the number of samples. Log-likelihood \\(\\ell(\\theta|X) = nm\\log(1-p) + \\log(\\frac{p}{1-p})\\sum_{i=1}^n x_i + \\log({m\\choose x_i})\\) Score Equations \\(U_n(\\theta|X) = -\\frac{nm}{1-p} + \\frac{1}{p(1-p)}\\sum_{i=1}^n x_i\\) Observed Information \\(-\\frac{\\partial^2}{\\partial\\theta^2}\\ell(\\theta|X) = \\frac{nm}{(1-p)^2} + \\frac{1-2p}{p^2(1-p)^2}\\sum_{i=1}^n x_i\\) Fisher Information \\(I(\\theta)= \\frac{nm}{p(1-p)}\\) MLE \\(\\frac{1}{nm}\\sum_{i=1}^n x_i\\) 8.6.3 Geometric Log-likelihood Score Equations Fisher Information MLE \\(n\\log(p) + \\log(1-p)\\sum_{i=1}^n x_i\\) \\(\\frac{n}{p} - \\frac{1}{1-p}\\sum_{i=1}^n x_i\\) \\(\\frac{n}{\\sum_{i=1}^n x_i}\\) 8.6.4 Negative Binomial Log-likelihood Score Equations Fisher Information MLE \\(nr\\log(\\frac{p}{1-p}) + \\sum_{i=1}^n x_i\\log(1-p) + \\log{x_i + r - 1\\choose x_i}\\) \\(-\\frac{nr}{p} - \\frac{1}{1-p}\\sum_{i=1}^n x_i\\) \\(\\frac{r}{(1-p)^2p}\\) \\(\\frac{1}{1 - \\frac{1}{nr}\\sum_{i=1}^nx_i}\\) 8.6.5 Poisson Log-likelihood Score Equations Fisher Information MLE \\(n\\lambda + \\sum_{i=1}^n x_i\\log(\\lambda) - \\log(x_i!)\\) \\(-n + \\frac{1}{\\lambda}x_i\\) \\(\\frac{1}{\\lambda}\\) \\(\\frac{1}{n}\\sum_{i=1}^nx_i\\) 8.6.6 Normal Log-likelihood Score Equations Fisher Information MLE \\(-\\frac{n}{2}\\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2}\\sum_{i=1}^n (x_i - \\mu)^2\\) \\(U(\\mu | x, \\sigma^2) = -n\\mu -\\frac{1}{\\sigma^2}\\sum_{i=1}^n x_i \\\\ U(\\sigma^2 | x, \\mu) = -\\frac{n}{2\\sigma^2} + \\frac{1}{2(\\sigma^2)^2}\\sum_{i=1}^n(x_i-\\mu)^2\\) \\(\\begin{bmatrix}\\frac{1}{\\sigma^2} &amp; 0 \\\\ 0 &amp; \\frac{1}{2\\sigma^4}\\end{bmatrix}\\) \\(\\frac{1}{n}\\sum_{i=1}^nx_i\\) 8.6.7 Exponential Log-likelihood Score Equations Fisher Information MLE \\(-n\\log(\\lambda) - \\frac{1}{\\lambda}\\sum_{i=1}^n x_i\\) \\(-\\frac{n}{\\lambda} + \\frac{1}{\\lambda^2}\\sum_{i=1}^n x_i\\) \\(\\lambda^2\\) \\(\\frac{1}{n}\\sum_{i=1}^nx_i\\) 8.6.8 Gamma Log-likelihood Score Equations Fisher Information MLE \\(-n\\log(\\Gamma(k)) - nk\\log(\\lambda) + (k - 1 - \\frac{1}{\\lambda})\\sum_{i=1}^n x_i\\) \\(-\\frac{nk}{\\lambda} + \\frac{1}{\\lambda^2}\\sum_{i=1}^n x_i\\) with \\(k\\) known (otherwise, requires differentiating \\(\\Gamma(k)\\)) \\(\\begin{bmatrix}\\psi^{(1)}(k) &amp; \\frac{1}{\\lambda}\\\\ \\frac{1}{\\lambda} &amp; \\frac{k}{\\lambda^2}\\end{bmatrix}\\) 8.6.9 Pareto Log-likelihood Score Equations Fisher Information MLE \\(n\\log{\\alpha} + n\\alpha\\log(x_m) - (\\alpha + 1)\\sum_{i=1}^n\\log(x_i)\\) \\(U(\\alpha | x_i) = \\frac{n}{\\alpha} + n\\log(x_m) - \\sum_{i=1}^n \\log(x_i)\\) \\(\\frac{n}{\\alpha^2}\\) \\(\\frac{n}{\\sum_{i=1}^n\\log(x_i)}\\) "],["point-estimators-asymptotics.html", "Chapter 9 Point Estimators: Asymptotics 9.1 Consistency 9.2 Asymptotic Efficiency 9.3 Asymptotic Properties of MLEs 9.4 Variance Stabilizing Transformations 9.5 Asymptotic Confidence Intervals", " Chapter 9 Point Estimators: Asymptotics Evaluating the asymptotic properties of point estimators. 9.1 Consistency An important asymptotic property of an estimator is that it converges in probability to the true value being estimated as \\(n \\rightarrow \\infty\\). This is called consistency. The most common strategy for proving consistency is to use the Weak Law of Large Numbers. Theorem 9.1 (Weak Law of Large Numbers) iid version: If \\(Z_i\\) are iid with finite mean, then \\[\\bar{X} = \\frac{1}{n}\\sum_{i=1}^nX_i\\overset{p}{\\rightarrow}E(X_i)\\] Non-iid version: Alternatively, we can drop the iid assumption; if \\(X_i\\) have finite mean and variance, \\(Cov(X_i, X_j) = 0\\), and \\(\\lim_{n\\rightarrow\\infty}\\sum_{i=1}^n\\frac{\\sigma_i^2}{n^2} = 0\\), then \\[\\frac{1}{n}\\sum_{i=1}^nX_i - \\frac{1}{n}\\sum_{i=1}^nE(X_i) \\overset{\\mathcal{p}}{\\rightarrow} 0\\] The WLLN can be combined with the Continuous Mapping Theorem to show a variety of estimators are consistent. 9.2 Asymptotic Efficiency In addition to checking that an estimator converges to the correct value, we are also often concerned with the estimator’s variance as \\(n \\rightarrow \\infty\\). Often, estimators converge asymptotically to a normal distribution. If an estimator \\(T_n(X)\\) has the property \\(k_n(T_n(X) - \\tau(\\theta)) \\overset{\\mathcal{D}}{\\rightarrow} N(0,\\sigma^2)\\), then \\(\\sigma^2\\) is called the asymptotic variance (Casella and Berger 1990). Furthermore, \\(T_n(X)\\) is asymptotically efficient* if \\(\\sigma^2\\) achieves the Cramer-Rao Lower Bound. If \\(T_n(X)\\) is one-dimensional, then the asymptotic efficiency** of \\(T_n(X)\\) can be computed as the ratio \\[AE(\\theta, T_n) = \\frac{(\\tau&#39;(\\theta))^2}{\\mathcal{I}(\\theta)\\sigma^2}\\] If \\(T_n(X)\\) is \\(k\\)-dimensional, let \\(d = \\begin{bmatrix}\\frac{\\partial}{\\partial\\theta_1}\\tau(\\theta),...,\\frac{\\partial}{\\partial\\theta_k}\\tau(\\theta)\\end{bmatrix}\\). Then, the asymptotic efficiency is \\[AE(\\theta, T_n) = \\frac{d&#39;\\mathcal{I}(\\theta)^{-1}d}{\\sigma^2}\\] We can also compare estimators via their asymptotic relative efficiency (ARE), which for estimators \\(S_n\\) and \\(T_n\\) is \\[AE(\\theta, S_n, T_n) = \\frac{\\sigma_T^2}{\\sigma_S^2}\\] Let’s explore first how we might prove that an estimator converges asymptotically to a normal distribution to show this directly. 9.2.1 Central Limit Theorems These theorems are used to show asymptotic normality. There are many different types; let us focus on the three most common. Theorem 9.2 (Central Limit Theorem (iid)) If \\(X_i\\) are iid with finite first and second moments (\\(E(X_i), Var(X_i) &lt; \\infty\\)) then \\[\\sqrt{n}\\Big(\\frac{1}{n}\\sum_{i=1}^nX_i - E(X_i)\\Big) \\overset{\\mathcal{D}}{\\rightarrow} N(0,Var(X_i))\\] Theorem 9.2 (Lyapunov's Central Limit Theorem (non-iid)) Consider \\(X_i\\) that are independent with finite first and second moments (\\(E(X_i), Var(X_i) &lt; \\infty\\)). Let \\(S_n^2 = \\sum_{i=1}^n Var(X_i)\\). Then, for some \\(\\delta &gt; 0\\), if the condition \\[\\lim_{n\\rightarrow \\infty} \\frac{1}{S_n^{2 + \\delta}}\\sum_{i=1}^n E(|X_i - E(X_i)|^{2 + \\delta}) = 0\\] holds, then we know \\[\\frac{1}{S_n}\\sum_{i=1}^n (X_i - E(X_i)) \\overset{\\mathcal{D}}{\\rightarrow} N(0,1)\\] In practice, to prove that Lyapunov’s CLT holds true, we typically take \\(\\delta = 1\\) and compute the third moments contained in the Lyapunov condition. Alternatively, we can use Lindeberg’s CLT for non-iid data: Theorem 9.2 (Lindeberg's Central Limit Theorem (non-iid)) Like the Lyapunov CLT, consider \\(X_i\\) that are independent with finite first and second moments (\\(E(X_i), Var(X_i) &lt; \\infty\\)). Let \\(S_n^2 = \\sum_{i=1}^n Var(X_i)\\). Then if \\[\\lim_{n\\rightarrow \\infty} \\frac{1}{S_n^{2}}\\sum_{i=1}^n E((X_i - E(X_i))^{2})\\cdot I(|X_i - E(X_i) &gt; \\varepsilon S_n) = 0\\] holds, then we know \\[\\frac{1}{S_n}\\sum_{i=1}^n (X_i - E(X_i)) \\overset{\\mathcal{D}}{\\rightarrow} N(0,1)\\] 9.2.2 The Delta Method What if our estimator is not a sample mean? In this case, if it is a function of a sample mean, we can still use the Delta Method to prove that it converges asymptotically to either a Normal or a \\(\\chi^2\\) distribution. Theorem 9.3 (Delta Method) If \\(\\sqrt{n}(\\hat{\\theta}_n - \\theta) \\overset{\\mathcal{D}}{\\rightarrow} N(0,\\sigma^2)\\), then for a continuous \\(g\\) with continuous nonzero derivative in an interval containing \\(\\theta\\)… \\(\\sqrt{n}\\Big(g(\\hat{\\theta}_n) - g(\\theta)\\Big) \\overset{\\mathcal{D}}{\\rightarrow} N\\Big(0, \\sigma^2\\cdot (\\frac{d}{d\\theta}g(\\theta))^2\\Big)\\) (First-Order Delta Method) \\(n\\Big(g(\\hat{\\theta}_n) - g(\\theta)\\Big) \\overset{\\mathcal{D}}{\\rightarrow} \\frac{1}{2}\\sigma^2\\cdot g&#39;&#39;(\\theta)\\cdot\\chi^2(1)\\) (Second-Order Delta Method) Generally, the Second-Order Delta Method is required if \\(g&#39;(\\theta) = 0\\). Theorem 9.4 (Multivariate Delta Method) If \\(\\sqrt{n}(\\hat{\\theta}_n - \\theta) \\overset{\\mathcal{D}}{\\rightarrow} N(0,V)\\), where \\(V\\) is a \\(k \\times k\\) matrix, then for a continuous real-valued function \\(g(x)\\) of \\(k\\) variables with continuous nonzero first partial derivatives, \\[\\sqrt{n}\\Big(g(\\hat{\\theta}_n) - g(\\theta)\\Big) \\overset{\\mathcal{D}}{\\rightarrow} MVN_k\\Big(0, u&#39;Vu\\Big)\\] where \\(u = \\begin{bmatrix}\\frac{\\partial}{\\partial \\theta_1}g(\\theta),...,\\frac{\\partial}{\\partial \\theta_k}g(\\theta)\\end{bmatrix}\\) 9.2.3 Cramer-Wold Device One final technique for proving convergence in distribution is to use the Cramer-Wold Device. Theorem 9.5 (Cramer Wold Device) For a sequence of random vectors \\(X_n\\), a random vector \\(X\\), and a vector \\(a\\) of constants, \\[X_n \\overset{\\mathcal{D}}{\\rightarrow} \\iff a&#39;X_n \\overset{\\mathcal{D}}{\\rightarrow}a&#39;X, \\forall a\\] In other words, we can show convergence in distribution of a random vector by showing that every linear combination of that random vector converges in distribution to the same linear combination of \\(X\\). The Cramer-Wold Device allows us to convert a problem involving convergence of random vectors into a problem involving convergence of a single random variable. 9.2.4 Asymptotic Distribution in Practice. Statistical problems are often more complicated than simply applying the CLT or Delta Method. Often, we may need to consider the convergences of other random variables. To do this, we can combine the CLT/Delta Method with Slutsky’s Theorem or the Continuous Mapping Theorem to prove desired results. 9.3 Asymptotic Properties of MLEs While the CLT and Delta Method are extremely useful, if you are working with MLEs, it can often be faster to rely on their known properties. For \\(X_i\\) satisfying a certain set of regularity conditions (where? better link), the MLE $_n has the two import properties discussed above: Consistency: \\(\\hat{\\theta}_n \\overset{p}{\\rightarrow} \\theta\\) Asymptotic Efficiency: \\(\\sqrt{n}(\\hat{\\theta}_n - \\theta) \\overset{\\mathcal{D}}{\\rightarrow} N(0, \\mathcal{I}(\\theta_0)^{-1})\\), the Cramer-Rao Lower Bound This holds true even for multi-parameter MLEs. Note that the regularity conditions are met by the MLE of all exponential families for which \\(\\nu \\in \\Theta \\subset \\mathbb{R}\\) is an open set, which can be useful in problem-solving. Estimating \\(\\mathcal{I}(\\theta_0)\\) can be performed in two ways: (is this correct?) \\(-\\frac{1}{n} \\frac{\\partial^2}{\\partial\\theta^2}\\ell(\\hat{\\theta}_n | X_n) \\overset{p}{\\rightarrow} \\mathcal{I}(\\theta_0)\\) \\(\\frac{1}{n} I_n(\\hat{\\theta}_n) \\overset{p}{\\rightarrow} \\mathcal{I}(\\theta_0)\\) 9.4 Variance Stabilizing Transformations Sometimes, the asymptotic variance of a distribution may depend on the value of a particular parameter. This may be undesirable because the parameter might be unknown. In this case, we can perform a variance-stabilizing transformation to remove the variance. This is done by setting \\(f&#39;(\\theta) = \\frac{c}{\\tau(\\theta)}\\). Then, for \\(T_n(X)\\) such that \\(\\sqrt{n}(T_n - \\theta) \\overset{\\mathcal{D}}{\\rightarrow} N(0, \\tau^2(\\theta))\\), by the Delta Method we get \\[\\sqrt{n}(f(T_n) - f(\\theta)) \\overset{\\mathcal{D}}{\\rightarrow} N(0, f&#39;(\\theta)^2 \\cdot\\tau^2(\\theta)) = N(0, c)\\] 9.5 Asymptotic Confidence Intervals References "],["hypothesis-tests-finite-samples.html", "Chapter 10 Hypothesis Tests: Finite Samples", " Chapter 10 Hypothesis Tests: Finite Samples Constructing finite-sample hypothesis tests and evaluating their properties. "],["hypothesis-tests-asymptotics.html", "Chapter 11 Hypothesis Tests: Asymptotics 11.1 Wald Test 11.2 Score Test 11.3 Likelihood Ratio Test 11.4 Composite Null Hypotheses", " Chapter 11 Hypothesis Tests: Asymptotics Constructing a finite-sample hypothesis test requires deriving the full distribution of the test statistic, which may be difficult. Oftentimes, however, we can use the Central Limit Theorem and other asymptotic tools to prove that, as \\(n \\rightarrow \\infty\\), a test statistic converges to either a standard normal or chi-squared distribution. This permits the construction of asymptotic hypothesis tests. This section will focus on constructing and evaluating tests based on the MLE \\(\\hat{\\theta}_n\\) when regularity conditions are met under both the null hypothesis \\(H_0\\) and the alternative \\(H_1\\).. In this scenario, there are three possible tests that may be constructed: Wald Test Score Test Likelihood Ratio Test All of these tests are asymptotically equivalent. One-sided tests are generally based on a Normal approximation, while two-sided are based on a \\(\\chi^2\\) approximation. Let’s discuss each in detail, including their strengths and weaknesses. 11.1 Wald Test Definition 11.1 (Wald Test) The one-sided, one-dimensional Wald test is constructed based on \\[W_n = (\\hat{\\theta}_n - \\theta_0)(I_n(\\theta_0))^\\frac{1}{2} \\approx N(0,1)\\] The two-sided one-dimensional Wald test is constructed based on the square of this; that is, \\[W_n = (\\hat{\\theta}_n - \\theta_0)^2(I_n(\\theta_0)) \\approx \\chi^2(1)\\] It can be extended to the multidimensional setting by representing \\(\\hat{\\theta}_n\\) and \\(\\theta\\) as \\(k\\)-dimensional vectors and \\(I_n(\\hat{\\theta}_0)\\) a \\(k \\times k\\) matrix. \\[W_n = (\\hat{\\theta}_n - \\theta_0)^\\top(I_n(\\theta_0))(\\hat{\\theta}_n - \\theta_0) \\approx \\chi^2_k\\] which follows from the fact that \\(\\hat{\\theta}_n \\approx MVN_k(\\theta_0, (I_n(\\theta_0))^{-1})\\) To obtain \\(I_n(\\theta_0)\\), we can estimate it using either… \\(I_n(\\hat{\\theta}_n)\\); simply plug the MLE into the expected information. \\(-\\frac{\\partial^2}{\\partial\\theta^2}\\ell(\\hat{\\theta}_n | X_n)\\); plug the MLE into the observed information However, \\(I_n(\\hat{\\theta}_0)\\) is generally preferred, because it is more efficient. Advantages: - Simple to compute if you know the form of the MLE - Constructing confidence intervals is easy Disadvantages: - Requires knowing the form of the MLE - Not invariant if you transform the MLE - Approximation may not be as accurate as the other two tests 11.2 Score Test Definition 11.2 (Score Test) The one-sided, one-dimensional Score Test is constructed by \\[S_n = \\Big(\\frac{\\partial}{\\partial \\theta}\\ell(\\theta_0|X_n)\\Big)(I_n(\\theta_0))^{-\\frac{1}{2}}\\] The two-sided, one-dimensional Score test is constructed based on its square: \\[S_n = \\Big(\\frac{\\partial}{\\partial\\theta}\\ell(\\theta_0 | X_n)\\Big)^2(I_n(\\theta_0))^{-1}\\] It can be extended to the multivariate setting by representing \\(U_n(\\theta_0) = \\begin{bmatrix}\\frac{\\partial}{\\partial\\theta_1}\\ell(\\theta|X_n) &amp; ... &amp; \\frac{\\partial}{\\partial\\theta_k}\\ell(\\theta|X_n) \\end{bmatrix}\\) and \\(I_n(\\hat{\\theta}_0)\\) a \\(k \\times k\\) matrix: \\[(U_n(\\theta_0))^\\top(I_n(\\theta_0))^{-1}(U_n(\\theta_0)) \\approx \\chi^2_k\\] which follows from the fact that \\(U_n(\\theta_0) \\approx MVN_k(0, I_n(\\theta_0))\\) Advantages: - Not actually necessary to know the form of the MLE to construct the test - all that is needed is the score equation and information under \\(H_0\\) - More computationally efficient as a result Disadvantages: - Constructing confidence intervals is more complicated, since inverting the Score test statistic may be challenging - Not invariant to under reparametrization 11.3 Likelihood Ratio Test Definition 11.3 (Likelihood Ratio Test) The one-dimensional asymptotic Likelihood Ratio Test (LRT) is constructed based on \\[Q_n = 2\\log\\Big(\\frac{\\mathcal{L}(\\hat{\\theta}_n)}{\\mathcal{L}(\\theta_0)}\\Big) = 2(\\ell(\\hat{\\theta}_n|X_n) - \\ell(\\theta_0|X_n)) \\approx \\chi^2(1)\\] This can be extended to multiple dimensions by letting \\(\\ell(\\theta|X_n)\\) by a \\(k\\)-dimensional vector. Then, \\[Q_n = 2(\\ell(\\hat{\\theta}_n|X_n) - \\ell(\\theta_0|X_n)) \\approx\\chi^2(k)\\] Advantages: - No derivatives need to be calculated - Invariant to transformations of the MLE - Provides the most accurate approximation, especially if we decide to reparametrize the model, in which case the derivative in the Wald/Score tests would need to be recomputed. Disadvantages: - Inverting the test to construct a confidence interval is difficult - Requires knowledge of the log-likelihood under both the null hypothesis \\(H_0\\) and the alternative \\(H_1\\). 11.4 Composite Null Hypotheses A composite null hypothesis involves multiple parameters, some of which are nuisance parameters. Suppose we have a null hypothesis \\(H_0: \\alpha = \\alpha_0\\), where \\(\\alpha\\) is a vector of length \\(k\\), and we have a vector \\(\\beta\\) of nuisance parameters. In a composite null, we can partition \\(\\theta^\\top = (\\alpha^\\top, \\beta^\\top)\\). ::: {#adjusted-information .definition name=“Adjusted Information} To use a normal asymptotic approximation in this situation, it is necessary to compute the”adjusted information” \\(I_{n,\\alpha\\alpha|\\beta}(\\alpha, \\beta)\\) - the information for \\(\\alpha\\), conditional on our \\(\\beta\\) estimate: \\[I_{n,\\alpha\\alpha|\\beta}(\\alpha, \\beta) = I_{n,\\alpha\\alpha}(\\alpha, \\beta) - I_{n,\\alpha\\beta}(\\alpha, \\beta)(I_{n,\\beta\\beta}(\\alpha, \\beta))^{-1}I_{n,\\beta\\alpha}(\\alpha, \\beta)\\] :: The adjusted information is derived from the block-matrix partition formula for the partition \\[I_n(\\alpha, \\beta) = \\begin{bmatrix} I_{n,\\alpha\\alpha}(\\alpha, \\beta) &amp; I_{n,\\alpha\\beta}(\\alpha, \\beta) \\\\ I_{n,\\beta\\alpha}(\\alpha, \\beta) &amp; I_{n,\\beta\\beta}(\\alpha, \\beta) \\\\\\end{bmatrix}\\] With the adjusted information computed, the previously-discussed asymptotic tests become the following: Wald: \\(W_n = (\\hat{\\alpha}_n - \\alpha_0)^\\top I_{n,\\alpha\\alpha|\\beta}(\\hat{\\alpha}_n, \\hat{\\beta}_n)(\\hat{\\alpha}_n - \\alpha_0)\\) Score: \\(S_n = U_{n,\\alpha}(\\alpha_0, \\hat{\\beta}_n)^\\top I_{n,\\alpha\\alpha|\\beta}(\\alpha_0, \\hat{\\beta}_n)U_{n,\\alpha}(\\alpha_0, \\hat{\\beta}_n)\\) Likelihood Ratio: \\(Q_n = 2(\\ell(\\hat{\\alpha}_n, \\hat{\\beta}_n) - \\ell(\\alpha_0, \\hat{\\beta}_n))\\) all of which follow a \\(\\chi^2(k)\\) distribution, where \\(k\\) is the number of parameters being tested in \\(\\alpha\\). "],["generating-random-variables.html", "Chapter 12 Generating Random Variables", " Chapter 12 Generating Random Variables What the title says. "],["random-processes.html", "Chapter 13 Random Processes 13.1 Poisson Processes 13.2 Branching Processes", " Chapter 13 Random Processes This section elaborates on two types of random processes: poisson processes and branching processes. 13.1 Poisson Processes A Poisson Process is a model for a series of discrete events where the average time between events is known, but the exact timing of events is random. A poisson process has the following properties: Events are independent of each other. The average rate (events per time period) is constant. Two events cannot occur at the same time. 13.1.1 Memorylessness of the Exponential Recall that the exponential distribution is memoryless, meaning \\(P(X&gt;x+a \\mid X&gt;a)=P(X&gt;x)\\). The memoryless property of the exponential distribution applies to waiting times in a Poisson process. It means that the time between events remains independent of past events, allowing us to predict future waiting times solely based on the average rate of event occurrences (i.e., the interarrival times between events are i.i.d.). 13.1.2 Count-Time Duality \\[ \\{T_n&gt;t\\}=\\{N_t&lt;n\\} \\] In words, the following 2 statements are equivalent: \\(T_n\\) (time to the \\(n^{th}\\) event) is greater than some fixed time \\(t\\) \\(N_t\\) (number of events up to time \\(t\\)) is less than some fixed number \\(n\\) \\[ \\int_t^{\\infty} \\underbrace{\\frac{1}{\\Gamma(n)\\lambda^{-n}} x^{n-1} e^{-\\lambda x}}_{T_n \\sim \\text{Gamma}(n,\\lambda)} d x \\quad = \\quad \\sum_{x=0}^{n-1} \\underbrace{\\frac{e^{-\\lambda t} (\\lambda t)^x}{x!}}_{N_t \\sim \\text{Poisson}(\\lambda t)} \\] 13.1.3 Poisson Distribution Suppose that we are interested in the expected number of events that will occur over a particular interval. The probability of observing a particular number of events can be modeled using the (discrete) poisson distribution: \\(X=\\) Discrete number of events occurring over a finite interval Moments: \\(\\mathbb{E}[X]=\\lambda\\), \\(\\operatorname{Var}(X)=\\lambda\\) \\(\\lambda=\\) Expected number of events over interval \\(=\\underbrace{\\frac{Events}{Time}}_{Rate}\\times Time\\)   13.1.4 Exponential Distribution Suppose that we are interested in the expected time before the next event. The probability of observing a particular time before the next event can be modeled using the (continuous) exponential distribution: \\(X=\\) Continuous time between events Moments: \\(\\mathbb{E}[X]=\\frac{1}{\\lambda}\\), \\(\\operatorname{Var}(X)=\\frac{1}{\\lambda^2}\\) \\(\\lambda=\\) Rate of events \\(=\\underbrace{\\frac{Events}{Time}}_{Rate}\\) Note: There is an inverse relationship between the rate of events (\\(\\lambda\\)) and expected time before the next event (\\(x\\)). As the rate of events (\\(\\lambda\\)) increases, the time before the next event (\\(x\\)) decreases.     13.1.5 Example Consider a Poisson process \\((\\lambda)\\) with a twist: After every event there is a guaranteed period of length \\(\\nu\\) during which no event can occur. Typical Poisson process: Distribution of time between events: \\(T_n-T_{n-1} \\sim \\operatorname{Exp}(\\lambda)\\) Distribution of time to the \\(n^{th}\\) event: \\(\\sum_{i=1}^n T_i \\sim \\operatorname{Gamma}(n, \\lambda)\\) Poisson process with a twist: Distribution of time between events: \\(T_n-T_{n-1} \\sim \\operatorname{Exp}(\\lambda)+\\nu\\) \\[ f_{T_n-T_{n-1}}(x)=\\lambda e^{-\\lambda(x-\\nu)} \\] Distribution of time to the \\(n^{th}\\) event: \\(\\sum_{i=1}^n T_i \\sim \\operatorname{Gamma}(n, \\lambda)+n \\nu\\) \\[ f_{T_n}(x)=\\frac{1}{\\Gamma(n) \\lambda^{-n}}(x-n \\nu)^{n-1} e^{-\\lambda(x-n \\nu)} \\] Further, by count time duality, we can write: \\[ P(T_n&gt;t)=P(N_t&lt;n) \\] \\[ \\int_t^{\\infty} \\frac{1}{\\Gamma(n)\\lambda^{-n}} (x-n \\nu)^{n-1} e^{-\\lambda (x-n \\nu)} d x \\quad = \\quad \\sum_{x=0}^{n-1} \\frac{e^{-\\lambda (t-n \\nu)} (\\lambda (t-n \\nu))^x}{x!} \\] 13.2 Branching Processes 13.2.1 Random Variables We’ll start by defining the random variables that characterize a branching process. \\(X_n\\) = size of the population at time (or generation) \\(n\\) \\(Y_{i,n-1}\\) = number of offspring produced by the \\(i^{th}\\) individual in generation \\(n-1\\)   How are random variables \\(X_n\\) and \\(Y_{i,n-1}\\) related? Intuitively, we can think of the population size of the current generation (\\(n\\)) as the sum of the offspring produced by each individual in the previous generation (\\(n-1\\)): \\[ \\begin{aligned} X_n &amp;= \\sum_{i=1}^{X_{n-1}} Y_{i,n-1}\\\\ &amp;= Y_{1,n-1} + Y_{2,n-1} +...+Y_{X_{n-1},n-1} \\end{aligned} \\] 13.2.2 Probability Generating Function Now we’ll define the Probability Generating Function (PGF), which allows us to compute probabilities from branching processes. \\[ g_X(t)=\\mathbb{E}_X\\left(t^X\\right)=\\sum_{n=0}^{\\infty} t^n P(X=n) \\] Similar to MGFs, independent random variables can be convoluted by multiplying their PGFs. For i.i.d. \\(Y_{i,j}\\), the PGF gives the first two moments: \\[ \\begin{aligned} \\mathbb{E}\\left(X_n\\right) &amp; =[\\mathbb{E}(Y)]^n \\\\ \\operatorname{Var}\\left(X_n\\right) &amp; =\\operatorname{Var}(Y)\\left(\\sum_{i=n-1}^{2(n-1)}[\\mathbb{E}(Y)]^i\\right) \\end{aligned} \\]   Asymptotics exists for Branching Processes on i.i.d. \\(\\left\\{Y_{i,j}\\right\\}\\) with finite variance: \\[ \\begin{gathered} \\text{Population-Level} \\quad \\quad \\text{Individual-Level} \\\\ \\\\ \\mathrm{E}\\left(X_n\\right) \\rightarrow \\begin{cases}0 &amp; \\mathrm{E}(Y)&lt;1 \\\\ 1 &amp; \\mathrm{E}(Y)=1 \\\\ \\infty &amp; \\mathrm{E}(Y)&gt;1\\end{cases} \\\\ \\\\ \\operatorname{Var}\\left(X_n\\right) \\rightarrow \\begin{cases}0 &amp; \\mathrm{E}(Y)&lt;1 \\\\ \\infty &amp; \\mathrm{E}(Y) \\geq 1\\end{cases} \\end{gathered} \\] 13.2.3 Criticality Theorem Criticality Theorem states that the probability of ultimate extinction of a branching process is the smallest solution to \\(\\eta=g_Y(\\eta)\\). In general, we can solve for \\(\\eta\\) using the quadratic equation: \\[ \\begin{aligned} &amp;0=a\\eta^2+b\\eta+c\\\\ \\\\ &amp;\\eta=\\frac{-b \\pm \\sqrt{b^2-4 a c}}{2 a} \\end{aligned} \\] 13.2.4 Example Consider a Branching Process where individuals duplicate with probability \\(p\\) and die with probability \\(q\\). Describe the mean and variance over time for this branching process. For what values of \\(p\\) will the process go extinct with probability 1? Establish the probability of eventual extinction for arbitrary \\(p\\).   Step 1: Define the pmf of \\(Y\\) based on the given reproduction probabilities. Does \\(Y\\) follow a known distribution? \\(f_y(y)=\\left\\{\\begin{array}{ll}y=0 &amp; \\text { w.p. } q=1-p \\\\ y=2 &amp; \\text { w.p. } p\\end{array} \\quad \\Rightarrow \\quad y \\sim 2\\right.\\) Bernoulli\\((p)\\)   Step 2: Calculate the moments of \\(Y\\), which can be used to calculate the moments of \\(X_n\\). \\[ \\begin{aligned} \\mathbb{E}[Y]=2 p \\quad \\quad \\quad \\quad \\mathbb{E}\\left[X_n\\right] &amp; =(2 p)^n \\\\ \\operatorname{Var}(Y)=4 p q \\quad \\quad \\operatorname{Var}\\left(X_n\\right) &amp; =\\operatorname{Var}(y) \\sum_{i=n-1}^{2(n-1)} \\mathbb{E}[Y]^i \\\\ &amp; =4 p q \\sum_{i=n-1}^{2(n-1)}(2 p)^i \\end{aligned} \\] Based on our asymptotic results, we know that the process \\(X_n\\) will go extinct with probability 1 if \\(\\mathbb{E}[Y]&lt;1\\) \\[ \\mathbb{E}[Y]=2 p \\Rightarrow \\text { process will go extinct with probability 1 if } p&lt; \\frac{1}{2} \\]   Step 3: Find the probability of ultimate extinction using Criticality Theorem. First, find the PGF of \\(Y\\): \\[ \\begin{aligned} g_Y(\\eta) =\\mathbb{E}\\left[\\eta^Y\\right]&amp;=\\sum_Y \\eta^Y f_Y(y) \\\\ &amp; =\\eta^2 P(Y=2)+\\eta^0 P(Y=0) \\\\ &amp; =\\eta^2 p+q \\end{aligned} \\] Second, find the probability of ultimate extinction, which is the smallest solution to \\(\\eta=g_Y(\\eta)\\): \\[ \\begin{aligned} \\eta&amp;=\\eta^2 p+q \\\\ 0&amp;=\\eta^2 p-\\eta+q\\\\ \\\\ a&amp;=p, b=-1, c=q\\\\ \\\\ \\eta&amp;= \\frac{1 \\pm \\sqrt{1-4pq}}{2p} \\\\&amp;= \\frac{1 \\pm \\sqrt{1-4p(1-p)}}{2p} \\\\&amp;= \\frac{1 \\pm \\sqrt{1-4p+4p^2}}{2p} \\\\&amp;= \\frac{1 \\pm \\sqrt{(2p-1)^2}}{2p} \\\\&amp;= \\frac{1 \\pm (2p-1)}{2p}\\\\ &amp;=\\text{min}\\left(1 \\quad \\text{or} \\quad \\frac{1}{p}-1\\right) \\end{aligned} \\] For \\(p \\leq \\frac{1}{2}, 1\\) is the minimum. Thus, \\(\\eta=P(\\text{Extinction})=1.\\) "],["references.html", "References", " References "]]
