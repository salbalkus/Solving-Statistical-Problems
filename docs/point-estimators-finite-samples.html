<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 8 Point Estimators: Finite Samples | Solving Statistical Problems</title>
  <meta name="description" content="Chapter 8 Point Estimators: Finite Samples | Solving Statistical Problems" />
  <meta name="generator" content="bookdown 0.34 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 8 Point Estimators: Finite Samples | Solving Statistical Problems" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="Chapter 8 Point Estimators: Finite Samples | Solving Statistical Problems" />
  <meta name="github-repo" content="salbalkus/Solving-Statistical-Problems" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 8 Point Estimators: Finite Samples | Solving Statistical Problems" />
  
  <meta name="twitter:description" content="Chapter 8 Point Estimators: Finite Samples | Solving Statistical Problems" />
  

<meta name="author" content="Salvador Balkus, Kimberly Greco, and Mónica Robles Fontán" />


<meta name="date" content="2023-07-17" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="statistics.html"/>
<link rel="next" href="point-estimators-asymptotics.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>
<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Solving Statistical Problems</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Introduction</a></li>
<li class="chapter" data-level="2" data-path="math-tricks.html"><a href="math-tricks.html"><i class="fa fa-check"></i><b>2</b> Math Tricks</a>
<ul>
<li class="chapter" data-level="2.1" data-path="math-tricks.html"><a href="math-tricks.html#combinatorics"><i class="fa fa-check"></i><b>2.1</b> Combinatorics</a></li>
<li class="chapter" data-level="2.2" data-path="math-tricks.html"><a href="math-tricks.html#binomial-and-multinomial-theorems"><i class="fa fa-check"></i><b>2.2</b> Binomial and Multinomial Theorems</a></li>
<li class="chapter" data-level="2.3" data-path="math-tricks.html"><a href="math-tricks.html#geometric-series"><i class="fa fa-check"></i><b>2.3</b> Geometric Series</a></li>
<li class="chapter" data-level="2.4" data-path="math-tricks.html"><a href="math-tricks.html#exponential-taylor"><i class="fa fa-check"></i><b>2.4</b> Taylor Series for Exponential Function</a></li>
<li class="chapter" data-level="2.5" data-path="math-tricks.html"><a href="math-tricks.html#taylors-formula"><i class="fa fa-check"></i><b>2.5</b> Taylor’s Formula</a></li>
<li class="chapter" data-level="2.6" data-path="math-tricks.html"><a href="math-tricks.html#exponential-limit"><i class="fa fa-check"></i><b>2.6</b> Exponential Limit</a></li>
<li class="chapter" data-level="2.7" data-path="math-tricks.html"><a href="math-tricks.html#ibp"><i class="fa fa-check"></i><b>2.7</b> Integration by Parts</a></li>
<li class="chapter" data-level="2.8" data-path="math-tricks.html"><a href="math-tricks.html#leibniz-rule"><i class="fa fa-check"></i><b>2.8</b> Leibniz’s Rule</a></li>
<li class="chapter" data-level="2.9" data-path="math-tricks.html"><a href="math-tricks.html#fubinis-theorem"><i class="fa fa-check"></i><b>2.9</b> Fubini’s Theorem</a></li>
<li class="chapter" data-level="2.10" data-path="math-tricks.html"><a href="math-tricks.html#gamma-function"><i class="fa fa-check"></i><b>2.10</b> Gamma Function</a></li>
<li class="chapter" data-level="2.11" data-path="math-tricks.html"><a href="math-tricks.html#triangle-inequality"><i class="fa fa-check"></i><b>2.11</b> Triangle Inequality</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="probability.html"><a href="probability.html"><i class="fa fa-check"></i><b>3</b> Probability</a>
<ul>
<li class="chapter" data-level="3.1" data-path="probability.html"><a href="probability.html#basic-axioms"><i class="fa fa-check"></i><b>3.1</b> Basic Axioms</a></li>
<li class="chapter" data-level="3.2" data-path="probability.html"><a href="probability.html#basic-probability-solving-techniques"><i class="fa fa-check"></i><b>3.2</b> Basic Probability Solving Techniques</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="probability.html"><a href="probability.html#disjointify"><i class="fa fa-check"></i><b>3.2.1</b> Disjointification</a></li>
<li class="chapter" data-level="3.2.2" data-path="probability.html"><a href="probability.html#demorgan"><i class="fa fa-check"></i><b>3.2.2</b> DeMorgan’s Laws</a></li>
<li class="chapter" data-level="3.2.3" data-path="probability.html"><a href="probability.html#proving-inequalities-subsetting"><i class="fa fa-check"></i><b>3.2.3</b> Proving Inequalities: Subsetting</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="probability.html"><a href="probability.html#conditional-probability"><i class="fa fa-check"></i><b>3.3</b> Conditional Probability</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="probability.html"><a href="probability.html#conditional-probability-in-practice"><i class="fa fa-check"></i><b>3.3.1</b> Conditional Probability in Practice</a></li>
<li class="chapter" data-level="3.6.3" data-path="probability.html"><a href="probability.html#important-theorems"><i class="fa fa-check"></i><b>3.6.3</b> Important Theorems</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="known-distributions.html"><a href="known-distributions.html"><i class="fa fa-check"></i><b>4</b> Known Distributions</a>
<ul>
<li class="chapter" data-level="4.1" data-path="known-distributions.html"><a href="known-distributions.html#families-of-distributions"><i class="fa fa-check"></i><b>4.1</b> Families of Distributions</a></li>
<li class="chapter" data-level="4.2" data-path="known-distributions.html"><a href="known-distributions.html#location-scale"><i class="fa fa-check"></i><b>4.2</b> Location and Scale Families</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="known-distributions.html"><a href="known-distributions.html#location-families"><i class="fa fa-check"></i><b>4.2.1</b> Location Families</a></li>
<li class="chapter" data-level="4.2.2" data-path="known-distributions.html"><a href="known-distributions.html#scale-families"><i class="fa fa-check"></i><b>4.2.2</b> Scale Families</a></li>
<li class="chapter" data-level="4.2.3" data-path="known-distributions.html"><a href="known-distributions.html#properties-of-location-scale-families"><i class="fa fa-check"></i><b>4.2.3</b> Properties of Location-Scale Families</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="known-distributions.html"><a href="known-distributions.html#exponential-family"><i class="fa fa-check"></i><b>4.3</b> Exponential Families</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="known-distributions.html"><a href="known-distributions.html#properties"><i class="fa fa-check"></i><b>4.3.1</b> Properties</a></li>
<li class="chapter" data-level="4.3.2" data-path="known-distributions.html"><a href="known-distributions.html#natural-exponential-family"><i class="fa fa-check"></i><b>4.3.2</b> Natural Exponential Families</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="known-distributions.html"><a href="known-distributions.html#known-univariate-exponential-families"><i class="fa fa-check"></i><b>4.4</b> Known Univariate Exponential Families</a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="known-distributions.html"><a href="known-distributions.html#bernoulli"><i class="fa fa-check"></i><b>4.4.1</b> Bernoulli</a></li>
<li class="chapter" data-level="4.4.2" data-path="known-distributions.html"><a href="known-distributions.html#binomial"><i class="fa fa-check"></i><b>4.4.2</b> Binomial</a></li>
<li class="chapter" data-level="4.4.3" data-path="known-distributions.html"><a href="known-distributions.html#geometric-distribution"><i class="fa fa-check"></i><b>4.4.3</b> Geometric</a></li>
<li class="chapter" data-level="4.4.4" data-path="known-distributions.html"><a href="known-distributions.html#negative-binomial"><i class="fa fa-check"></i><b>4.4.4</b> Negative Binomial</a></li>
<li class="chapter" data-level="4.4.5" data-path="known-distributions.html"><a href="known-distributions.html#poisson"><i class="fa fa-check"></i><b>4.4.5</b> Poisson</a></li>
<li class="chapter" data-level="4.4.6" data-path="known-distributions.html"><a href="known-distributions.html#normal"><i class="fa fa-check"></i><b>4.4.6</b> Normal</a></li>
<li class="chapter" data-level="4.4.7" data-path="known-distributions.html"><a href="known-distributions.html#exponential"><i class="fa fa-check"></i><b>4.4.7</b> Exponential</a></li>
<li class="chapter" data-level="4.4.8" data-path="known-distributions.html"><a href="known-distributions.html#gamma"><i class="fa fa-check"></i><b>4.4.8</b> Gamma</a></li>
<li class="chapter" data-level="4.4.9" data-path="known-distributions.html"><a href="known-distributions.html#beta"><i class="fa fa-check"></i><b>4.4.9</b> Beta</a></li>
<li class="chapter" data-level="4.4.10" data-path="known-distributions.html"><a href="known-distributions.html#chi-squared"><i class="fa fa-check"></i><b>4.4.10</b> Chi-squared</a></li>
<li class="chapter" data-level="4.4.11" data-path="known-distributions.html"><a href="known-distributions.html#weibull"><i class="fa fa-check"></i><b>4.4.11</b> Weibull</a></li>
<li class="chapter" data-level="4.4.12" data-path="known-distributions.html"><a href="known-distributions.html#pareto"><i class="fa fa-check"></i><b>4.4.12</b> Pareto</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="known-distributions.html"><a href="known-distributions.html#non-exponential-families"><i class="fa fa-check"></i><b>4.5</b> Non-exponential families</a>
<ul>
<li class="chapter" data-level="4.5.1" data-path="known-distributions.html"><a href="known-distributions.html#uniform"><i class="fa fa-check"></i><b>4.5.1</b> Uniform</a></li>
<li class="chapter" data-level="4.5.2" data-path="known-distributions.html"><a href="known-distributions.html#cauchy"><i class="fa fa-check"></i><b>4.5.2</b> Cauchy</a></li>
<li class="chapter" data-level="4.5.3" data-path="known-distributions.html"><a href="known-distributions.html#studentst"><i class="fa fa-check"></i><b>4.5.3</b> t-distribution</a></li>
<li class="chapter" data-level="4.5.4" data-path="known-distributions.html"><a href="known-distributions.html#f-distribution"><i class="fa fa-check"></i><b>4.5.4</b> F-distribution</a></li>
<li class="chapter" data-level="4.5.5" data-path="known-distributions.html"><a href="known-distributions.html#hypergeometric"><i class="fa fa-check"></i><b>4.5.5</b> Hypergeometric</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="known-distributions.html"><a href="known-distributions.html#multivariate-distributions"><i class="fa fa-check"></i><b>4.6</b> Multivariate Distributions</a>
<ul>
<li class="chapter" data-level="4.6.1" data-path="known-distributions.html"><a href="known-distributions.html#bivariate-normal"><i class="fa fa-check"></i><b>4.6.1</b> Bivariate Normal</a></li>
<li class="chapter" data-level="4.6.2" data-path="known-distributions.html"><a href="known-distributions.html#multivariate-normal"><i class="fa fa-check"></i><b>4.6.2</b> Multivariate Normal</a></li>
<li class="chapter" data-level="4.6.3" data-path="known-distributions.html"><a href="known-distributions.html#multinomial"><i class="fa fa-check"></i><b>4.6.3</b> Multinomial</a></li>
</ul></li>
<li class="chapter" data-level="4.7" data-path="known-distributions.html"><a href="known-distributions.html#medians-and-other-functionals-of-a-distribution"><i class="fa fa-check"></i><b>4.7</b> Medians and Other Functionals of a Distribution</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="new-distributions.html"><a href="new-distributions.html"><i class="fa fa-check"></i><b>5</b> New Distributions</a>
<ul>
<li class="chapter" data-level="5.1" data-path="new-distributions.html"><a href="new-distributions.html#transformations"><i class="fa fa-check"></i><b>5.1</b> Transformations</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="new-distributions.html"><a href="new-distributions.html#theorems"><i class="fa fa-check"></i><b>5.1.1</b> Theorems</a></li>
<li class="chapter" data-level="5.1.2" data-path="new-distributions.html"><a href="new-distributions.html#practical-strategy"><i class="fa fa-check"></i><b>5.1.2</b> Practical Strategy</a></li>
<li class="chapter" data-level="5.1.3" data-path="new-distributions.html"><a href="new-distributions.html#proving-independence-from-a-joint-transformation"><i class="fa fa-check"></i><b>5.1.3</b> Proving Independence From a Joint Transformation</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="new-distributions.html"><a href="new-distributions.html#computing-joint-probabilities"><i class="fa fa-check"></i><b>5.2</b> Computing Joint Probabilities</a></li>
<li class="chapter" data-level="5.3" data-path="new-distributions.html"><a href="new-distributions.html#probability-integral-transform"><i class="fa fa-check"></i><b>5.3</b> Probability Integral Transform</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="new-distributions.html"><a href="new-distributions.html#hiearchical-models-iterated-moments"><i class="fa fa-check"></i><b>5.3.1</b> Hiearchical Models (Iterated Moments)</a></li>
<li class="chapter" data-level="5.3.2" data-path="new-distributions.html"><a href="new-distributions.html#convolutions"><i class="fa fa-check"></i><b>5.3.2</b> Convolutions</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="moments.html"><a href="moments.html"><i class="fa fa-check"></i><b>6</b> Moments</a>
<ul>
<li class="chapter" data-level="6.1" data-path="moments.html"><a href="moments.html#basic-definitions"><i class="fa fa-check"></i><b>6.1</b> Basic Definitions</a></li>
<li class="chapter" data-level="6.2" data-path="moments.html"><a href="moments.html#expected-value"><i class="fa fa-check"></i><b>6.2</b> <span class="math inline">\(E(X)\)</span> Properties</a></li>
<li class="chapter" data-level="6.3" data-path="moments.html"><a href="moments.html#varx-properties"><i class="fa fa-check"></i><b>6.3</b> <span class="math inline">\(Var(X)\)</span> Properties</a></li>
<li class="chapter" data-level="6.4" data-path="moments.html"><a href="moments.html#covariance-and-correlation"><i class="fa fa-check"></i><b>6.4</b> Covariance and Correlation</a></li>
<li class="chapter" data-level="6.5" data-path="moments.html"><a href="moments.html#conditional-expectation"><i class="fa fa-check"></i><b>6.5</b> Conditional Expectation</a></li>
<li class="chapter" data-level="6.6" data-path="moments.html"><a href="moments.html#mgf"><i class="fa fa-check"></i><b>6.6</b> Moment Generating Functions</a></li>
<li class="chapter" data-level="6.7" data-path="moments.html"><a href="moments.html#moment-bounds"><i class="fa fa-check"></i><b>6.7</b> Moment Inequalities</a></li>
<li class="chapter" data-level="6.8" data-path="moments.html"><a href="moments.html#techniques-for-deriving-moments"><i class="fa fa-check"></i><b>6.8</b> Techniques for Deriving Moments</a>
<ul>
<li class="chapter" data-level="6.8.1" data-path="moments.html"><a href="moments.html#bernoulli-direct-summation"><i class="fa fa-check"></i><b>6.8.1</b> Bernoulli: Direct Summation</a></li>
<li class="chapter" data-level="6.8.2" data-path="moments.html"><a href="moments.html#uniform-direct-integration"><i class="fa fa-check"></i><b>6.8.2</b> Uniform: Direct Integration</a></li>
<li class="chapter" data-level="6.8.3" data-path="moments.html"><a href="moments.html#geometric-series-convergence"><i class="fa fa-check"></i><b>6.8.3</b> Geometric: Series Convergence</a></li>
<li class="chapter" data-level="6.8.4" data-path="moments.html"><a href="moments.html#binomial-kernel-technique-series-version"><i class="fa fa-check"></i><b>6.8.4</b> Binomial: Kernel Technique, Series Version</a></li>
<li class="chapter" data-level="6.8.5" data-path="moments.html"><a href="moments.html#negative-binomial-and-hypergeometric-computing-exx-1"><i class="fa fa-check"></i><b>6.8.5</b> Negative Binomial and Hypergeometric: Computing <span class="math inline">\(E(X(X-1))\)</span></a></li>
<li class="chapter" data-level="6.8.6" data-path="moments.html"><a href="moments.html#poisson-exponential-taylor-series"><i class="fa fa-check"></i><b>6.8.6</b> Poisson: Exponential Taylor Series</a></li>
<li class="chapter" data-level="6.8.7" data-path="moments.html"><a href="moments.html#exponential-integration-by-parts"><i class="fa fa-check"></i><b>6.8.7</b> Exponential: Integration By Parts</a></li>
<li class="chapter" data-level="6.8.8" data-path="moments.html"><a href="moments.html#gamma-and-beta-kernel-technique-integration-version"><i class="fa fa-check"></i><b>6.8.8</b> Gamma and Beta: Kernel Technique, Integration Version</a></li>
<li class="chapter" data-level="6.8.9" data-path="moments.html"><a href="moments.html#normal-location-scale-trick-and-polar-integration"><i class="fa fa-check"></i><b>6.8.9</b> Normal: Location-Scale Trick and Polar Integration</a></li>
</ul></li>
<li class="chapter" data-level="6.9" data-path="moments.html"><a href="moments.html#other-moments-for-reference"><i class="fa fa-check"></i><b>6.9</b> Other Moments (for reference)</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="statistics.html"><a href="statistics.html"><i class="fa fa-check"></i><b>7</b> Statistics</a>
<ul>
<li class="chapter" data-level="7.1" data-path="statistics.html"><a href="statistics.html#sufficient-stats"><i class="fa fa-check"></i><b>7.1</b> Sufficient Statistics</a>
<ul>
<li class="chapter" data-level="7.1.1" data-path="statistics.html"><a href="statistics.html#techniques-for-finding-sufficient-statistics"><i class="fa fa-check"></i><b>7.1.1</b> Techniques for Finding Sufficient Statistics</a></li>
<li class="chapter" data-level="7.1.2" data-path="statistics.html"><a href="statistics.html#exp-fam-ss"><i class="fa fa-check"></i><b>7.1.2</b> Exponential Family Sufficient Statistics</a></li>
<li class="chapter" data-level="7.1.3" data-path="statistics.html"><a href="statistics.html#a-note-on-distributions-of-sufficient-statistics"><i class="fa fa-check"></i><b>7.1.3</b> A Note on Distributions of Sufficient Statistics</a></li>
<li class="chapter" data-level="7.1.4" data-path="statistics.html"><a href="statistics.html#moments-of-the-sufficient-statistic"><i class="fa fa-check"></i><b>7.1.4</b> Moments of the Sufficient Statistic</a></li>
<li class="chapter" data-level="7.1.5" data-path="statistics.html"><a href="statistics.html#table-ss"><i class="fa fa-check"></i><b>7.1.5</b> Table of Sufficient Statistics</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="statistics.html"><a href="statistics.html#minimal-sufficiency"><i class="fa fa-check"></i><b>7.2</b> Minimal Sufficiency</a></li>
<li class="chapter" data-level="7.3" data-path="statistics.html"><a href="statistics.html#ancillary-stats"><i class="fa fa-check"></i><b>7.3</b> Ancillary Statistics</a>
<ul>
<li class="chapter" data-level="7.3.1" data-path="statistics.html"><a href="statistics.html#why-are-we-interested-in-ancillary-statistics"><i class="fa fa-check"></i><b>7.3.1</b> Why are we interested in ancillary statistics?</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="statistics.html"><a href="statistics.html#complete-stats"><i class="fa fa-check"></i><b>7.4</b> Complete Statistics</a>
<ul>
<li class="chapter" data-level="7.4.1" data-path="statistics.html"><a href="statistics.html#techniques-for-finding-css"><i class="fa fa-check"></i><b>7.4.1</b> Techniques for Finding CSS</a></li>
<li class="chapter" data-level="7.4.2" data-path="statistics.html"><a href="statistics.html#basus-theorem"><i class="fa fa-check"></i><b>7.4.2</b> Basu’s Theorem</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="point-estimators-finite-samples.html"><a href="point-estimators-finite-samples.html"><i class="fa fa-check"></i><b>8</b> Point Estimators: Finite Samples</a>
<ul>
<li class="chapter" data-level="8.1" data-path="point-estimators-finite-samples.html"><a href="point-estimators-finite-samples.html#identifiability"><i class="fa fa-check"></i><b>8.1</b> Identifiability</a></li>
<li class="chapter" data-level="8.2" data-path="point-estimators-finite-samples.html"><a href="point-estimators-finite-samples.html#finding-estimators"><i class="fa fa-check"></i><b>8.2</b> Finding estimators</a>
<ul>
<li class="chapter" data-level="8.2.1" data-path="point-estimators-finite-samples.html"><a href="point-estimators-finite-samples.html#method-of-moments"><i class="fa fa-check"></i><b>8.2.1</b> Method of Moments</a></li>
<li class="chapter" data-level="8.2.2" data-path="point-estimators-finite-samples.html"><a href="point-estimators-finite-samples.html#maximum-likelihood-estimation"><i class="fa fa-check"></i><b>8.2.2</b> Maximum Likelihood Estimation</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="point-estimators-finite-samples.html"><a href="point-estimators-finite-samples.html#properties-of-estimators"><i class="fa fa-check"></i><b>8.3</b> Properties of Estimators</a></li>
<li class="chapter" data-level="8.4" data-path="point-estimators-finite-samples.html"><a href="point-estimators-finite-samples.html#uniform-minimum-variance-unbiased-estimators-umvues"><i class="fa fa-check"></i><b>8.4</b> Uniform Minimum Variance Unbiased Estimators (UMVUEs)</a>
<ul>
<li class="chapter" data-level="8.4.1" data-path="point-estimators-finite-samples.html"><a href="point-estimators-finite-samples.html#finding-umvues-i-cramér-rao-bound"><i class="fa fa-check"></i><b>8.4.1</b> Finding UMVUEs I: Cramér-Rao Bound</a></li>
<li class="chapter" data-level="8.4.2" data-path="point-estimators-finite-samples.html"><a href="point-estimators-finite-samples.html#rao-blackwell-and-lehmann-scheffé-theorem"><i class="fa fa-check"></i><b>8.4.2</b> Rao-Blackwell and Lehmann-Scheffé Theorem</a></li>
<li class="chapter" data-level="8.4.3" data-path="point-estimators-finite-samples.html"><a href="point-estimators-finite-samples.html#finding-umvues-ii-using-the-lehmann-scheffé-theorem"><i class="fa fa-check"></i><b>8.4.3</b> Finding UMVUEs II: Using the Lehmann-Scheffé Theorem</a></li>
<li class="chapter" data-level="8.4.4" data-path="point-estimators-finite-samples.html"><a href="point-estimators-finite-samples.html#finding-umvues-iii-lehmann-scheffé-corollary"><i class="fa fa-check"></i><b>8.4.4</b> Finding UMVUEs III: Lehmann-Scheffé Corollary</a></li>
<li class="chapter" data-level="8.4.5" data-path="point-estimators-finite-samples.html"><a href="point-estimators-finite-samples.html#proving-an-umvue-does-not-exist"><i class="fa fa-check"></i><b>8.4.5</b> Proving an UMVUE Does Not Exist</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="point-estimators-finite-samples.html"><a href="point-estimators-finite-samples.html#inferential-properties-of-exponential-families-distributions"><i class="fa fa-check"></i><b>8.5</b> Inferential Properties of Exponential Families Distributions</a>
<ul>
<li class="chapter" data-level="8.5.1" data-path="point-estimators-finite-samples.html"><a href="point-estimators-finite-samples.html#bernoulli-1"><i class="fa fa-check"></i><b>8.5.1</b> Bernoulli</a></li>
<li class="chapter" data-level="8.5.2" data-path="point-estimators-finite-samples.html"><a href="point-estimators-finite-samples.html#binomial-1"><i class="fa fa-check"></i><b>8.5.2</b> Binomial</a></li>
<li class="chapter" data-level="8.5.3" data-path="point-estimators-finite-samples.html"><a href="point-estimators-finite-samples.html#geometric"><i class="fa fa-check"></i><b>8.5.3</b> Geometric</a></li>
<li class="chapter" data-level="8.5.4" data-path="point-estimators-finite-samples.html"><a href="point-estimators-finite-samples.html#negative-binomial-1"><i class="fa fa-check"></i><b>8.5.4</b> Negative Binomial</a></li>
<li class="chapter" data-level="8.5.5" data-path="point-estimators-finite-samples.html"><a href="point-estimators-finite-samples.html#poisson-1"><i class="fa fa-check"></i><b>8.5.5</b> Poisson</a></li>
<li class="chapter" data-level="8.5.6" data-path="point-estimators-finite-samples.html"><a href="point-estimators-finite-samples.html#normal-1"><i class="fa fa-check"></i><b>8.5.6</b> Normal</a></li>
<li class="chapter" data-level="8.5.7" data-path="point-estimators-finite-samples.html"><a href="point-estimators-finite-samples.html#exponential-1"><i class="fa fa-check"></i><b>8.5.7</b> Exponential</a></li>
<li class="chapter" data-level="8.5.8" data-path="point-estimators-finite-samples.html"><a href="point-estimators-finite-samples.html#gamma-1"><i class="fa fa-check"></i><b>8.5.8</b> Gamma</a></li>
<li class="chapter" data-level="8.5.9" data-path="point-estimators-finite-samples.html"><a href="point-estimators-finite-samples.html#pareto-1"><i class="fa fa-check"></i><b>8.5.9</b> Pareto</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="point-estimators-asymptotics.html"><a href="point-estimators-asymptotics.html"><i class="fa fa-check"></i><b>9</b> Point Estimators: Asymptotics</a>
<ul>
<li class="chapter" data-level="9.1" data-path="point-estimators-asymptotics.html"><a href="point-estimators-asymptotics.html#consistency"><i class="fa fa-check"></i><b>9.1</b> Consistency</a>
<ul>
<li class="chapter" data-level="9.1.1" data-path="point-estimators-asymptotics.html"><a href="point-estimators-asymptotics.html#technique-weak-law-of-large-numbers"><i class="fa fa-check"></i><b>9.1.1</b> Technique: Weak Law of Large Numbers</a></li>
<li class="chapter" data-level="9.1.2" data-path="point-estimators-asymptotics.html"><a href="point-estimators-asymptotics.html#technique-direct-proof-via-convergence-in-probability"><i class="fa fa-check"></i><b>9.1.2</b> Technique: Direct Proof via Convergence in Probability</a></li>
<li class="chapter" data-level="9.1.3" data-path="point-estimators-asymptotics.html"><a href="point-estimators-asymptotics.html#technique-continuous-mapping-theorem."><i class="fa fa-check"></i><b>9.1.3</b> Technique: Continuous Mapping Theorem.</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="point-estimators-asymptotics.html"><a href="point-estimators-asymptotics.html#asymptotic-efficiency"><i class="fa fa-check"></i><b>9.2</b> Asymptotic Efficiency</a>
<ul>
<li class="chapter" data-level="9.2.1" data-path="point-estimators-asymptotics.html"><a href="point-estimators-asymptotics.html#central-limit-theorems"><i class="fa fa-check"></i><b>9.2.1</b> Central Limit Theorems</a></li>
<li class="chapter" data-level="9.2.2" data-path="point-estimators-asymptotics.html"><a href="point-estimators-asymptotics.html#the-delta-method"><i class="fa fa-check"></i><b>9.2.2</b> The Delta Method</a></li>
<li class="chapter" data-level="9.2.3" data-path="point-estimators-asymptotics.html"><a href="point-estimators-asymptotics.html#cramer-wold-device"><i class="fa fa-check"></i><b>9.2.3</b> Cramer-Wold Device</a></li>
<li class="chapter" data-level="9.2.4" data-path="point-estimators-asymptotics.html"><a href="point-estimators-asymptotics.html#asymptotic-distribution-in-practice."><i class="fa fa-check"></i><b>9.2.4</b> Asymptotic Distribution in Practice.</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="point-estimators-asymptotics.html"><a href="point-estimators-asymptotics.html#asymptotic-properties-of-mles"><i class="fa fa-check"></i><b>9.3</b> Asymptotic Properties of MLEs</a></li>
<li class="chapter" data-level="9.4" data-path="point-estimators-asymptotics.html"><a href="point-estimators-asymptotics.html#variance-stabilizing-transformations"><i class="fa fa-check"></i><b>9.4</b> Variance Stabilizing Transformations</a></li>
<li class="chapter" data-level="9.5" data-path="point-estimators-asymptotics.html"><a href="point-estimators-asymptotics.html#asymptotic-confidence-intervals"><i class="fa fa-check"></i><b>9.5</b> Asymptotic Confidence Intervals</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="hypothesis-tests-finite-samples.html"><a href="hypothesis-tests-finite-samples.html"><i class="fa fa-check"></i><b>10</b> Hypothesis Tests: Finite Samples</a>
<ul>
<li class="chapter" data-level="10.1" data-path="hypothesis-tests-finite-samples.html"><a href="hypothesis-tests-finite-samples.html#constructing-a-test"><i class="fa fa-check"></i><b>10.1</b> Constructing a Test</a></li>
<li class="chapter" data-level="10.2" data-path="hypothesis-tests-finite-samples.html"><a href="hypothesis-tests-finite-samples.html#lrt"><i class="fa fa-check"></i><b>10.2</b> Likelihood Ratio Tests</a></li>
<li class="chapter" data-level="10.3" data-path="hypothesis-tests-finite-samples.html"><a href="hypothesis-tests-finite-samples.html#power"><i class="fa fa-check"></i><b>10.3</b> Power</a></li>
<li class="chapter" data-level="10.4" data-path="hypothesis-tests-finite-samples.html"><a href="hypothesis-tests-finite-samples.html#how-to-find-optimal-tests"><i class="fa fa-check"></i><b>10.4</b> How to Find Optimal Tests</a>
<ul>
<li class="chapter" data-level="10.4.1" data-path="hypothesis-tests-finite-samples.html"><a href="hypothesis-tests-finite-samples.html#properties-1"><i class="fa fa-check"></i><b>10.4.1</b> Properties</a></li>
<li class="chapter" data-level="10.4.2" data-path="hypothesis-tests-finite-samples.html"><a href="hypothesis-tests-finite-samples.html#optimality-for-simple-hypotheses-the-neyman-pearson-lemma"><i class="fa fa-check"></i><b>10.4.2</b> Optimality for Simple Hypotheses: The Neyman-Pearson Lemma</a></li>
<li class="chapter" data-level="10.4.3" data-path="hypothesis-tests-finite-samples.html"><a href="hypothesis-tests-finite-samples.html#optimality-for-one-sided-hypotheses-the-karlin-rubin-theorem"><i class="fa fa-check"></i><b>10.4.3</b> Optimality for One-Sided Hypotheses: The Karlin-Rubin Theorem</a></li>
<li class="chapter" data-level="10.4.4" data-path="hypothesis-tests-finite-samples.html"><a href="hypothesis-tests-finite-samples.html#optimality-for-two-sided-hypotheses."><i class="fa fa-check"></i><b>10.4.4</b> Optimality for Two-Sided Hypotheses.</a></li>
</ul></li>
<li class="chapter" data-level="10.5" data-path="hypothesis-tests-finite-samples.html"><a href="hypothesis-tests-finite-samples.html#nuisance-parameters"><i class="fa fa-check"></i><b>10.5</b> Nuisance Parameters</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="hypothesis-tests-asymptotics.html"><a href="hypothesis-tests-asymptotics.html"><i class="fa fa-check"></i><b>11</b> Hypothesis Tests: Asymptotics</a>
<ul>
<li class="chapter" data-level="11.1" data-path="hypothesis-tests-asymptotics.html"><a href="hypothesis-tests-asymptotics.html#wald-test"><i class="fa fa-check"></i><b>11.1</b> Wald Test</a></li>
<li class="chapter" data-level="11.2" data-path="hypothesis-tests-asymptotics.html"><a href="hypothesis-tests-asymptotics.html#score-test"><i class="fa fa-check"></i><b>11.2</b> Score Test</a></li>
<li class="chapter" data-level="11.3" data-path="hypothesis-tests-asymptotics.html"><a href="hypothesis-tests-asymptotics.html#likelihood-ratio-test"><i class="fa fa-check"></i><b>11.3</b> Likelihood Ratio Test</a></li>
<li class="chapter" data-level="11.4" data-path="hypothesis-tests-asymptotics.html"><a href="hypothesis-tests-asymptotics.html#composite-null-hypotheses"><i class="fa fa-check"></i><b>11.4</b> Composite Null Hypotheses</a>
<ul>
<li class="chapter" data-level="11.4.1" data-path="hypothesis-tests-asymptotics.html"><a href="hypothesis-tests-asymptotics.html#multiple-parameters"><i class="fa fa-check"></i><b>11.4.1</b> Multiple Parameters</a></li>
<li class="chapter" data-level="11.4.2" data-path="hypothesis-tests-asymptotics.html"><a href="hypothesis-tests-asymptotics.html#nuisance-parameters-1"><i class="fa fa-check"></i><b>11.4.2</b> Nuisance Parameters</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="12" data-path="generating-random-variables.html"><a href="generating-random-variables.html"><i class="fa fa-check"></i><b>12</b> Generating Random Variables</a></li>
<li class="chapter" data-level="13" data-path="random-processes.html"><a href="random-processes.html"><i class="fa fa-check"></i><b>13</b> Random Processes</a>
<ul>
<li class="chapter" data-level="13.1" data-path="random-processes.html"><a href="random-processes.html#branching-process"><i class="fa fa-check"></i><b>13.1</b> Branching Processes</a>
<ul>
<li class="chapter" data-level="13.1.1" data-path="random-processes.html"><a href="random-processes.html#random-variables"><i class="fa fa-check"></i><b>13.1.1</b> Random Variables</a></li>
<li class="chapter" data-level="13.1.2" data-path="random-processes.html"><a href="random-processes.html#probability-generating-function"><i class="fa fa-check"></i><b>13.1.2</b> Probability Generating Function</a></li>
<li class="chapter" data-level="13.1.3" data-path="random-processes.html"><a href="random-processes.html#finding-the-pgf-of-a-branching-process"><i class="fa fa-check"></i><b>13.1.3</b> Finding the PGF of a Branching Process</a></li>
<li class="chapter" data-level="13.1.4" data-path="random-processes.html"><a href="random-processes.html#finding-the-probability-of-extinction-criticality-theorem"><i class="fa fa-check"></i><b>13.1.4</b> Finding the Probability of Extinction: Criticality Theorem</a></li>
<li class="chapter" data-level="13.1.5" data-path="random-processes.html"><a href="random-processes.html#example"><i class="fa fa-check"></i><b>13.1.5</b> Example</a></li>
</ul></li>
<li class="chapter" data-level="13.2" data-path="random-processes.html"><a href="random-processes.html#poisson-processes"><i class="fa fa-check"></i><b>13.2</b> Poisson Processes</a>
<ul>
<li class="chapter" data-level="13.2.1" data-path="random-processes.html"><a href="random-processes.html#memorylessness-of-the-exponential"><i class="fa fa-check"></i><b>13.2.1</b> Memorylessness of the Exponential</a></li>
<li class="chapter" data-level="13.2.2" data-path="random-processes.html"><a href="random-processes.html#count-time-duality"><i class="fa fa-check"></i><b>13.2.2</b> Count-Time Duality</a></li>
<li class="chapter" data-level="13.2.3" data-path="random-processes.html"><a href="random-processes.html#poisson-distribution"><i class="fa fa-check"></i><b>13.2.3</b> Poisson Distribution</a></li>
<li class="chapter" data-level="13.2.4" data-path="random-processes.html"><a href="random-processes.html#exponential-distribution"><i class="fa fa-check"></i><b>13.2.4</b> Exponential Distribution</a></li>
<li class="chapter" data-level="13.2.5" data-path="random-processes.html"><a href="random-processes.html#example-1"><i class="fa fa-check"></i><b>13.2.5</b> Example</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Solving Statistical Problems</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="point-estimators-finite-samples" class="section level1 hasAnchor" number="8">
<h1><span class="header-section-number">Chapter 8</span> Point Estimators: Finite Samples<a href="point-estimators-finite-samples.html#point-estimators-finite-samples" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>The goal of parametric statistical inference is to estimate a parameter - or a function of said parameter - of a given distribution using a random sample. To do this, we must construct an <a href="https://en.wikipedia.org/wiki/Estimator">estimator</a>:</p>
<div class="definition">
<p><span id="def:unlabeled-div-88" class="definition"><strong>Definition 8.1  (Estimator) </strong></span>An <strong>estimator</strong> is a <a href="statistics.html#statistics">statistic</a> <span class="math inline">\(T(X)\)</span> - a function of data - used to obtain an estimate of a parameter <span class="math inline">\(\theta\)</span> or function of a parameter <span class="math inline">\(\tau(\theta)\)</span> in a distribution.</p>
<ul>
<li>An <strong>estimate</strong> is the value of an estimator when computed on a set of data - that is, <span class="math inline">\(T(X)\)</span> for a particular realization of <span class="math inline">\(X\)</span>.</li>
<li>An <strong>estimand</strong> is the parameter <span class="math inline">\(\theta\)</span> or function <span class="math inline">\(\tau(\theta)\)</span> needed to be estimated.</li>
</ul>
</div>
<p>Point estimators estimate scalar or vector real values - as opposed to interval estimators, which estimate intervals. Estimators have various qualities - some are good, and some are bad. This chapter will discuss how to construct and evaluate the quality of estimators.</p>
<div id="identifiability" class="section level2 hasAnchor" number="8.1">
<h2><span class="header-section-number">8.1</span> Identifiability<a href="point-estimators-finite-samples.html#identifiability" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>First of all, how do we know if it even possible to estimate a given parameter in a distribution? Such a question gives rise to the concept of <a href="https://en.wikipedia.org/wiki/Identifiability">identifiability</a>:</p>
<div class="definition">
<p><span id="def:unlabeled-div-89" class="definition"><strong>Definition 8.2  (Identifiability) </strong></span>For a random variable <span class="math inline">\(X\)</span>, a parameter <span class="math inline">\(\theta\)</span> in a parameter space <span class="math inline">\(\Theta\)</span> is said to be <strong>identifiable</strong> if, for all <span class="math inline">\(\theta, \theta&#39; \in \Theta\)</span>,</p>
<p><span class="math display">\[\theta \neq \theta&#39; \implies F_X(x|\theta) \neq F_X(x|\theta&#39;)\]</span>
Alternatively, <span class="math inline">\(\theta\)</span> is <em>not</em> identifiable if there <em>exists</em> <span class="math inline">\(\theta, \theta&#39; \in \Theta\)</span> such that, for all <span class="math inline">\(x\)</span>,</p>
<p><span class="math display">\[F_X(x|\theta) = F_X(x|\theta&#39;)\]</span></p>
</div>
<p>If <span class="math inline">\(\theta\)</span> is non-identifiable, that means two distributions from the same family will be identical under different <span class="math inline">\(\theta\)</span> values. Hence, even if we knew the true underlying distribution of <span class="math inline">\(X\)</span>, it would be impossible to know its parameters. Given a random sample, it would be impossible to estimate the parameter accurately.</p>
<p>Non-identifiability usually arises when a distribution is parameterized by some function of multiple parameters. For instance, if <span class="math inline">\(\lambda = (\lambda_1, \lambda_2)\)</span>, and <span class="math inline">\(X \sim \text{Exp}(\lambda_1 + \lambda_2)\)</span>, it would be impossible to estimate <span class="math inline">\(\lambda_1\)</span> or <span class="math inline">\(\lambda_2\)</span> given <span class="math inline">\(X\)</span> alone since <span class="math inline">\(\lambda_1 + \lambda_2\)</span> can take on the same value for different individual values of <span class="math inline">\(\lambda_1\)</span> and <span class="math inline">\(\lambda_2\)</span>.</p>
<p>Generally, we can assume parameters are identifiable. But how might we show a parameter is not identifiable? One clue to look for is linear dependence.</p>
<div class="example">
<p><span id="exm:unlabeled-div-90" class="example"><strong>Example 8.1  (Non-Identifiability Via Linear Dependence) </strong></span>Non-identifiability can arise in a natural exponential family. Suppose we’ve parametrized a <a href="known-distributions.html#exponential-family">natural exponential family</a> into the form</p>
<p><span class="math display">\[f(x | \eta) = h(x)c(\theta)\exp(\sum_{i=1}^k \eta_it_i)\]</span>
where the vector <span class="math inline">\(t = (t_1, ..., t_k)\)</span> is linearly dependent. Linear dependence implies that there exists <span class="math inline">\(\alpha = (\alpha_1, ..., \alpha_k)\)</span> not equal to the 0-vector such that <span class="math inline">\(\alpha t = 0\)</span>. Then, <span class="math inline">\(\eta\)</span> is not identified?</p>
<p>How do we know? Since <span class="math inline">\(\alpha t = \sum_{i=1}^k \alpha_it_i = 0\)</span>, we can add it on the interior of the exponential function:</p>
<p><span class="math display">\[f(x | \eta) = h(x)c(\theta)\exp(\sum_{i=1}^k \eta_it_i + \sum_{i=1}^k \alpha_it_i)\\
= h(x)c(\theta)\exp(\sum_{i=1}^k (\eta_i + \alpha_i)t_i)\]</span></p>
<p>Letting <span class="math inline">\(\eta = (\eta_1 + \alpha_1,..., \eta_k + \alpha_k)\)</span>, clearly <span class="math inline">\(f(x|\eta) = f(x|\eta&#39;)\)</span> and <span class="math inline">\(\nu\)</span> is not identified.</p>
</div>
<p>Non-identifiability can also arise in more practical situations, such as the following:</p>
<div class="example">
<p><span id="exm:unlabeled-div-91" class="example"><strong>Example 8.2  (Non-Identifiability with Measurement Error) </strong></span>Suppose we want to construct a simple linear regression model <span class="math inline">\(Y = \beta X + \varepsilon\)</span> - but there’s a twist. Both <span class="math inline">\(Y\)</span> and <span class="math inline">\(X\)</span> are measured with unknown error, so we only observe</p>
<p><span class="math display">\[
Y^* = Y + \epsilon_1\\
X^* = X + \epsilon_2
\]</span></p>
<p>If we try to fit this linear regression on <span class="math inline">\(Y^*\)</span> and <span class="math inline">\(X^*\)</span> instead, we get</p>
<p><span class="math display">\[
Y + \epsilon_1 = \beta(X + \epsilon_2) + \varepsilon\\
\implies Y = \beta X + (\beta\epsilon_2 - \varepsilon_1 + \varepsilon)
\]</span></p>
<p>Now, since <span class="math inline">\(\epsilon_1\)</span> and <span class="math inline">\(\epsilon_2\)</span> are unknown, <span class="math inline">\(\beta\)</span> has become unidentifiable. Consider <span class="math inline">\((\beta&#39;, \epsilon_2&#39;) \neq (\beta, \epsilon_2)\)</span>. Subtracting <span class="math inline">\(Y = \beta&#39; X + (\beta&#39;\epsilon_2&#39; - \varepsilon_1 + \varepsilon)\)</span> from <span class="math inline">\(Y = \beta X + (\beta\epsilon_2 - \varepsilon_1 + \varepsilon)\)</span>, we get</p>
<p><span class="math display">\[0 = (\beta - \beta&#39;)X + (\beta\epsilon_2 - \beta&#39;\epsilon_2&#39;)\]</span>
Multiple values of <span class="math inline">\(\beta\)</span> and <span class="math inline">\(\beta&#39;\)</span> can be chosen to satisfy this equation (since <span class="math inline">\(\epsilon \neq \epsilon&#39;\)</span>) meaning that <span class="math inline">\(F_X(x|\beta) = F_X(x|\beta&#39;)\)</span> for <span class="math inline">\(\beta \neq \beta&#39;\)</span>, proving <span class="math inline">\(\beta\)</span> is not identifiable.</p>
</div>
<p>(can we use the MGF to show this as well?)</p>
</div>
<div id="finding-estimators" class="section level2 hasAnchor" number="8.2">
<h2><span class="header-section-number">8.2</span> Finding estimators<a href="point-estimators-finite-samples.html#finding-estimators" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>How do we take a guess at what a good estimator might be? There are several methods used to find parameter estimators. In this chapter we will discuss two very popular methods, the method of moments and the maximum likelihood method.</p>
<div id="method-of-moments" class="section level3 hasAnchor" number="8.2.1">
<h3><span class="header-section-number">8.2.1</span> Method of Moments<a href="point-estimators-finite-samples.html#method-of-moments" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Recall that the moments of a distribution are given by <span class="math inline">\(\mu^k = E[X^k]\)</span>, where <span class="math inline">\(k \in \mathbb{N}_{+}\)</span>. The idea of the method of moments is to equate the population quantity to the sample quantity, i.e.</p>
<p><span class="math display">\[\mu^k = E[X^k] = \frac{1}{n} \sum_{i=1}^{n} X_i^k\]</span>
where <span class="math inline">\(k=1,2,...\)</span> and <span class="math inline">\(\textbf{X}=(X_1,..., X_n)\)</span> is the sample from a distribution <span class="math inline">\(f(x|\theta)\)</span> where <span class="math inline">\(\theta = (\theta_1,..., \theta_k)\)</span>. Usually we use find <span class="math inline">\(k\)</span> equations of sample average and moments to estimate <span class="math inline">\(k\)</span> parameters.</p>
<div class="example">
<p><span id="exm:unlabeled-div-92" class="example"><strong>Example 8.3  </strong></span>Suppose we want to find MOM estimators for <span class="math inline">\(\alpha, \beta\)</span> from a <span class="math inline">\(X\sim\Gamma(\alpha,\beta)\)</span> random variable. Let <span class="math inline">\(\textbf{X}=(X_1,...,X_n)\)</span> be and iid sample from the gamma distribution. In this case we know that <span class="math inline">\(E[X]=\alpha/\beta\)</span> and <span class="math inline">\(Var(X)= \alpha/\beta^2\)</span>. Since we are estimating two parameters, we will be using the first two moments, namely, <span class="math inline">\(E[X], E[X^2]\)</span>. Observe that <span class="math inline">\(Var(X) = E[X^2] - E[X]^2 \Rightarrow E[X^2] = \alpha/\beta^2 + (\alpha/\beta)^2\)</span>. Now, we can proceed with the method of moments estimation with the following two equations.</p>
<p><span class="math display">\[ \bar{X} = \alpha/\beta\\ \bar{X^2} = \alpha/\beta^2 + (\alpha/\beta)^2\]</span>
Solving for <span class="math inline">\(\alpha, \beta\)</span> we obtain that</p>
<p><span class="math display">\[ \hat{\beta} = \frac{\bar{X}}{\bar{X^2} - \bar{X}^2},\\ \hat{\alpha} = \hat{\beta}\bar{X} = \frac{\bar{X}^2}{\bar{X^2} - \bar{X}^2}.\]</span></p>
</div>
<p><strong>Issues with the MoM Estimators:</strong></p>
<ul>
<li><p>MOM estimators may not exist. Suppose <span class="math inline">\(Y_1,...,Y_n\)</span> is a random sample from a <span class="math inline">\(Unif(-\theta, \theta)\)</span>. Then <span class="math inline">\(E[X] = 0\)</span> and a MOM estimator would solve the following equation, <span class="math inline">\(\bar{Y} = 0\)</span>, which has no solutions in <span class="math inline">\(\theta\)</span>. Thus a MOM estimator of <span class="math inline">\(\theta\)</span> does not exist.</p></li>
<li><p>MOM estimators may be outside of the parameter space.</p></li>
<li><p>MOM estimators are not invariant to transformations.</p></li>
</ul>
<p><strong>Advantages of MoM Estimators:</strong></p>
<ul>
<li><p>They are guaranteed to be <a href="#unbiasedness">unbiased</a> and <a href="point-estimators-asymptotics.html#consistency">consistent</a> for the parameter they estimate.</p></li>
<li><p>MOM estimators can offer a starting point for iterative methods used to find solutions of likelihood equations.</p></li>
</ul>
</div>
<div id="maximum-likelihood-estimation" class="section level3 hasAnchor" number="8.2.2">
<h3><span class="header-section-number">8.2.2</span> Maximum Likelihood Estimation<a href="point-estimators-finite-samples.html#maximum-likelihood-estimation" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Rather than simply matching moments, often a more effective strategy for finding an estimator is by finding the value which maximizes the <em>entire</em> likelihood of the sample. That is, for a given <span class="math inline">\(X = X_1, X_2, ... X_n\)</span>, we choose the estimator for which this sample is most likely. This is called the <em>maximum likelihood estimator</em>. Because of how common this approach is, we introduce several terms to describe the components of MLE.</p>
<div class="definition">
<p><span id="def:unlabeled-div-93" class="definition"><strong>Definition 8.3  (Likelihood) </strong></span>Let <span class="math inline">\(X\)</span> be a random variable or random vector, and <span class="math inline">\(\theta\)</span> a parameter or vector of parameters describing the distribution of <span class="math inline">\(X\)</span>. Then, the <strong>likelihood</strong> is</p>
<p><span class="math display">\[\mathcal{L}(\theta | X) = f_X(x|\theta)\]</span></p>
</div>
<div class="definition">
<p><span id="def:unlabeled-div-94" class="definition"><strong>Definition 8.4  (Log-Likelihood) </strong></span>Let <span class="math inline">\(X\)</span> be a random variable or random vector, and <span class="math inline">\(\theta\)</span> a parameter or vector of parameters describing the distribution of <span class="math inline">\(X\)</span>. Then, the <strong>log-likelihood</strong> is
<span class="math display">\[\ell(\theta|X) = \log\mathcal{L}(\theta | X) = \log f_X(x|\theta)\]</span></p>
</div>
<p>The log-likelihood is often easier to use. Furthermore, When <span class="math inline">\(X\)</span> is an iid sample <span class="math inline">\(X_1, X_2, ... X_n\)</span>, by the properties of logarithms, <span class="math inline">\(\ell(\theta|X) = \log\Big(\prod_{i=1}^nf_{X_i}(x|\theta)\Big) = \sum_{i=1}^n \log f_{X_i}(x|\theta)\)</span>. Working with a sum is much easier than with a product.</p>
<p>Of course, the whole reason this is permissible in finding a maximum likelihood estimate is that the <span class="math inline">\(\log\)</span> function is monotonic - finding the maximum of a function is equivalent to finding the maximum of its logarithm.</p>
<p>Maximizing this likelihood often requires calculus - specifically, the First Derivative Test. In statistics, the gradient of the log-likelihood has a special name: the <strong>score</strong>.</p>
<div class="definition">
<p><span id="def:unlabeled-div-95" class="definition"><strong>Definition 8.5  (Score Equations) </strong></span>If <span class="math inline">\(\theta\)</span> is a vector of parameters, the <strong>score equations</strong> are defined as the gradient of the log-likelihood (as described above). That is,</p>
<p><span class="math display">\[U(\theta | X) = \begin{bmatrix}\frac{\partial}{\partial\theta_1}\ell(\theta_1|X) &amp; ... &amp; \frac{\partial}{\partial\theta_k}\ell(\theta_k|X) \end{bmatrix}\]</span>
Of course, if <span class="math inline">\(\theta\)</span> is a single parameter, then <span class="math inline">\(U(\theta|X) = \frac{d}{d\theta}\ell(\theta|X)\)</span></p>
</div>
<p>Therefore, the first step in finding an MLE is to set the score to 0, and solve for the desired parameter. Let’s see an example.</p>
<div class="example">
<p><span id="exm:unlabeled-div-96" class="example"><strong>Example 8.4  </strong></span>Let <span class="math inline">\(\textbf{X} = (X_1,..., X_n)\)</span> be an iid sample from a <span class="math inline">\(Bern(p)\)</span> distribution. To find the MLE for <span class="math inline">\(p\)</span>, let us find maximize wrt <span class="math inline">\(p\)</span> the log likelihood function. The likelihood function is <span class="math inline">\(\mathcal{L}(p|\textbf{X}) = \prod_{i=1}^{n} p^X_i(1-p)^{1-X_i} = p^{\sum X_i}(1-p)^{n-\sum X_i}\)</span>. Now, the log-likehood is given by <span class="math inline">\(\mathcal{l}(p|\textbf{X}) = log(p) \sum X_i + log(1-p)(n-\sum X_i)\)</span>. The score equation goes as follows.</p>
<p><span class="math display">\[ U(p|\textbf{X}) = \frac{\sum X_i}{p} - \frac{(n-\sum X_i)}{1-p} \overset{set}= 0\]</span>
and the solution wrt <span class="math inline">\(p\)</span> is <span class="math inline">\(\bar{X}\)</span>. Now, the second derivative test is always negative which implies that <span class="math inline">\(\hat{p}_{MLE} = \bar{X}\)</span>.</p>
</div>
<p>The score has two useful properties in MLE theory. First, under <a href="#regularity-conditions">regularity conditions</a>, <span class="math inline">\(E(U(\theta|X)) = 0\)</span> - its mean is 0. This is because the regularity conditions imply <a href="math-tricks.html#leibniz-rule">Leibniz’s rule</a>; that is,</p>
<p><span class="math display">\[\begin{align}
E(U(\theta|X)) = \int_{\mathcal{X}}\frac{\partial}{\partial\theta}\log\mathcal{L}(\theta|X)f(x|\theta)dx \\
= \int_{\mathcal{X}}\frac{1}{f(x|\theta)}\frac{\partial}{\partial\theta}f(x|\theta)f(x|\theta)dx \\
= \frac{\partial}{\partial\theta}\frac{f(x|\theta)}{f(x|\theta)}f(x|\theta)dx = \frac{\partial}{\partial\theta}1 = 0
\end{align}\]</span></p>
<p>The second useful property is that, under the same regularity conditions, the variance of the score is</p>
<p><span class="math display">\[\begin{align}
Var(U(\theta|X)) = E(U(\theta|X)U(\theta|X)^\top) \\
= -E\Big(\frac{\partial^2}{\partial\partial^\top}\ell(\theta|X)\Big)
\end{align}\]</span></p>
<p>If <span class="math inline">\(\theta\)</span> is one-dimensional, <span class="math inline">\(Var(U(\theta|X)) = E(U(\theta|X)^2) = -E\Big(\frac{\partial^2}{\partial\theta^2}\ell(\theta|X)\Big)\)</span>. The rather insightful derivation of this is available <a href="https://en.wikipedia.org/wiki/Score_(statistics)#Variance">here</a>. This variance has a special name, the <strong>information</strong> (or sometimes, “Fisher Information”)</p>
<div class="definition">
<p><span id="def:information" class="definition"><strong>Definition 8.6  (Information) </strong></span>In statistics, <strong>information</strong> refers to the amount of information that a random variable <span class="math inline">\(X\)</span> contains about a parameter <span class="math inline">\(\theta\)</span>. It is defined mathematically as the variance of the score, which is</p>
<p><span class="math display">\[I(\theta) = E\Big((\frac{\partial}{\partial\theta}\log f(X|\theta))^2\Big|\theta\Big)\]</span>
Under MLE regularity conditions,</p>
<p><span class="math display">\[I(\theta) = -E\Big(\frac{\partial^2}{\partial\theta^2}\log f(X|\theta) \Big|\theta\Big)\]</span></p>
</div>
<p>As the mean value of a second derivative, the information measures the curve of <span class="math inline">\(\ell(\theta|X)\)</span> <em>(expand?)</em></p>
<p>The negative second derivative of the log-likelihood also has a special name itself: the <strong>observed information</strong>.</p>
<div class="definition">
<p><span id="def:unlabeled-div-97" class="definition"><strong>Definition 8.7  (Observed Information) </strong></span>The observed information is defined as</p>
<p><span class="math display">\[\mathcal{J}(\theta|X) = -\frac{\partial^2}{\partial\theta^2}\ell(\theta|X)\]</span>
When <span class="math inline">\(\theta\)</span> is a <span class="math inline">\(k\)</span>-dimensional vector, the observed information is the Hessian of the log-likelihood:</p>
<p><span class="math display">\[\mathcal{J}(\theta|X) = -\nabla\nabla^\top\ell(\theta|X) = \begin{bmatrix}\frac{\partial^2}{\partial\theta_1^2} &amp; ... &amp; \frac{\partial^2}{\partial\theta_1\partial\theta_k}\\ \vdots &amp; &amp; \vdots\\ \frac{\partial^2}{\partial\theta_k\partial\theta_1} &amp; ... &amp; \frac{\partial^2}{\partial\theta_k^2}\end{bmatrix}\]</span></p>
</div>
<p>The observed information can be used to estimate the Fisher information for a given sample <span class="math inline">\(X\)</span>.</p>
<p>The MLE theory is essential is the task of finding estimators for a given parameter. By definition, the solution that maximizes the likelihood function is called the maximum likelihood estimator for the parameter. This estimator has very properties, namely, it is a statistic that respects the parameter space. We previously saw an example in which we found an MLE for a one-dimensional parameter. However, we can find multidimensional MLEs for multidimensional parameters. There are two main approaches.</p>
<p>Let <span class="math inline">\(\theta = (\theta_1, \theta_2)\)</span>, a two-dimensional parameter.
1. Find MLE candidates for <span class="math inline">\(\theta_1, \theta_2\)</span> by setting partial derivative wrt <span class="math inline">\(\theta_1, \theta_2\)</span> respectively to zero and solving.
2. Find second partial derivatives and show that at least one of them is negative.
3. Find the second order partial derivative matrix and show that the determinant is positive.
If 2. and 3. are shown to be true, then then candidates from 1. are MLE’s for <span class="math inline">\(\theta_1, \theta_2\)</span>.</p>
<p>Let <span class="math inline">\(\theta\)</span> be a <span class="math inline">\(k\)</span>-dimensional parameter, ie, <span class="math inline">\(k\)</span> parameters. Then we can employ the profile-likelihood approach to find an MLE candidate. The profile-likelihood approach consists of maximizing with respect to a parameter of interest and profiling out the rest of the parameters (which can be considered nuisance parameters). In the case in which we want to find MLE for all <span class="math inline">\(k\)</span> parameters, then we would have to maximize with respect to each parameter and then substitute back our solutions until we prove that the candidates maximize each of the profile likelihoods for each parameter. This method is necessary when we want to find the MLE of an unknown, but fixed, number of parameters.</p>
<p>EXAMPLE FORTHCOMING
<!-- EXPLAIN BETTER -->
<!-- https://web.stat.tamu.edu/~suhasini/teaching613/chapter3.pdf --></p>
<p><strong>Remarks:</strong> Suppose <span class="math inline">\(X \sim f(x|\eta)\)</span> is an exponential family with natural parametrization. Then MOM estimators correspond to MLE.</p>
<div class="theorem">
<p><span id="thm:unlabeled-div-98" class="theorem"><strong>Theorem 8.1  </strong></span>If an MLE is unique and an MSS exists, then the MLE is a function of the MSS.</p>
</div>
<div class="theorem">
<p><span id="thm:unlabeled-div-99" class="theorem"><strong>Theorem 8.2  (Invariance property of MLE) </strong></span>If <span class="math inline">\(\hat{\theta}\)</span> is MLE for <span class="math inline">\(\theta\)</span>, then <span class="math inline">\(\tau(\hat{\theta})\)</span> is MLE for <span class="math inline">\(\tau(\theta)\)</span>.</p>
</div>
<div class="example">
<p><span id="exm:unlabeled-div-100" class="example"><strong>Example 8.5  </strong></span>invariance of mle example</p>
</div>
</div>
</div>
<div id="properties-of-estimators" class="section level2 hasAnchor" number="8.3">
<h2><span class="header-section-number">8.3</span> Properties of Estimators<a href="point-estimators-finite-samples.html#properties-of-estimators" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>How do we know if a particular estimator is considered “good”? There are two main properties by which we measure the quality of an estimator: <strong>bias</strong>, which measures the <em>accuracy</em> of the estimator, and <strong>variance</strong>, which measures the <em>precision</em>. Let <span class="math inline">\(\textbf{X} = (X_1,..., X_n), X_i \sim f(x|\theta), \theta \in \Theta\)</span> and let <span class="math inline">\(\hat{\theta}\)</span> be an estimator of <span class="math inline">\(\theta\)</span>. Then, we have the following:</p>
<div class="definition">
<p><span id="def:unlabeled-div-101" class="definition"><strong>Definition 8.8  (Bias of an estimator) </strong></span><span class="math display">\[bias(\hat{\theta}) = E[\hat{\theta}] - \theta\]</span></p>
</div>
<p>If <span class="math inline">\(bias(\hat{\theta})=0\)</span> then <span class="math inline">\(E[\hat{\theta}] = \theta\)</span> and we say that <span class="math inline">\(\hat{\theta}\)</span> is an <strong>unbiased</strong> estimator of <span class="math inline">\(\theta\)</span>.</p>
<p><strong>Note:</strong> Bias is a measure of accuracy.</p>
<div class="definition">
<p><span id="def:unlabeled-div-102" class="definition"><strong>Definition 8.9  (Variance of an estimator) </strong></span><span class="math display">\[Var(\hat{\theta}) = E[(\hat{\theta} - E[\hat{\theta}])^2] = E[\hat{\theta}^2]- E[\hat{\theta}]^2 \]</span></p>
</div>
<p><strong>Note:</strong> Variance is a measure of precision</p>
<div class="definition">
<p><span id="def:unlabeled-div-103" class="definition"><strong>Definition 8.10  (Mean squared error, MSE) </strong></span><span class="math display">\[MSE(\hat{\theta}) = Var(\hat{\theta}) + bias^2(\hat{\theta})\]</span></p>
</div>
<p><strong>Note:</strong> MSE is a measure of precision and accuracy.</p>
<p>Ideally we want an unbiased estimator with minimal variance. Often we have a <a href="https://en.wikipedia.org/wiki/Bias–variance_tradeoff">variance-bias tradeoff</a>, a situation in which when bias decreases, variance increases and viceversa. For example, for any distribution we have the following facts:</p>
<ul>
<li><span class="math inline">\(S^2\)</span> is unbiased</li>
<li><span class="math inline">\(\hat{\sigma^2}\)</span> is biased</li>
<li><span class="math inline">\(Var(\hat{\sigma^2}) &lt; Var(S^2)\)</span></li>
</ul>
<div class="definition">
<p><span id="def:unlabeled-div-104" class="definition"><strong>Definition 8.11  </strong></span>Let <span class="math inline">\(\hat{\theta}, \tilde{\theta}\)</span> be two estimators of <span class="math inline">\(\theta\)</span>. If <span class="math inline">\(MSE(\hat{\theta}) \le MSE (\tilde{\theta}) \forall \theta\)</span> and <span class="math inline">\(MSE(\hat{\theta}) &lt; MSE (\tilde{\theta})\)</span> for at least one <span class="math inline">\(\theta\)</span>, then we say that <span class="math inline">\(\tilde{\theta}\)</span> is inadmissible.</p>
</div>
<div class="definition">
<p><span id="def:unlabeled-div-105" class="definition"><strong>Definition 8.12  </strong></span>Let <span class="math inline">\(\hat{\theta}, \tilde{\theta}\)</span> be any unbiased estimators of <span class="math inline">\(\theta\)</span> whose variances exist. Then,</p>
<p><span class="math display">\[RE(\hat{\theta};\tilde{\theta}) = \frac{Var(\tilde{\theta})}{Var(\hat{\theta})}\]</span></p>
<p>is the <em>relative efficiency</em> of <span class="math inline">\(\hat{\theta}\)</span> to <span class="math inline">\(\tilde{\theta}\)</span>. This allows use to compare variance between estimators</p>
</div>
</div>
<div id="uniform-minimum-variance-unbiased-estimators-umvues" class="section level2 hasAnchor" number="8.4">
<h2><span class="header-section-number">8.4</span> Uniform Minimum Variance Unbiased Estimators (UMVUEs)<a href="point-estimators-finite-samples.html#uniform-minimum-variance-unbiased-estimators-umvues" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Is it possible for there to be a “best” estimator with respect to MSE? If we restrict consideration only to unbiased estimators, then the answer may be yes. An unbiased estimator with the minimum possible variance among all possible <span class="math inline">\(\theta\)</span> is known as a <em>Uniform Minimum Variance Unbiased Estimator</em>, or UMVUE for short. Let’s define this mathematically.</p>
<div class="definition">
<p><span id="def:unlabeled-div-106" class="definition"><strong>Definition 8.13  (Uniform minimum variance unbiased estimator (UMVUE)) </strong></span>An estimator <span class="math inline">\(W\)</span> of a parameter <span class="math inline">\(\tau(\theta)\)</span> is called <em>uniform minimum variance unbiased estimator (UMVUE)</em> of <span class="math inline">\(\tau(\theta)\)</span> if <span class="math inline">\(E_{\theta}(W)= \tau(\theta)\)</span> and <span class="math inline">\(Var_{\theta}(W) \le Var_{\theta}(Q)\)</span> for any other unbiased estimator, Q, of <span class="math inline">\(\tau(\theta)\)</span>.</p>
</div>
<p>The UMVUE is the unbiased estimator of <span class="math inline">\(\tau(\theta)\)</span> with the minimum variance among all other unbiased estimators of <span class="math inline">\(\tau(\theta)\)</span>. First, let’s consider when the UMVUE exists - in this case, there are generally two strategies to construct the UMVUE.</p>
<div id="finding-umvues-i-cramér-rao-bound" class="section level3 hasAnchor" number="8.4.1">
<h3><span class="header-section-number">8.4.1</span> Finding UMVUEs I: Cramér-Rao Bound<a href="point-estimators-finite-samples.html#finding-umvues-i-cramér-rao-bound" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>If you can find an unbiased estimator of <span class="math inline">\(\tau(\theta)\)</span>, one way to show that it is an UMVUE is by showing that it attains the minimum possible variance among all estimators. The Cramér-Rao Bound provides such a <em>lower</em> bound for the variance of all estimators of a parameter <span class="math inline">\(\tau(\theta)\)</span>. This result is extremely powerful because if we can show that an <em>unbiased</em> estimator attains the <em>Cramér-Rao Lower Bound</em> (CRLB), then the estimator is UMVUE.</p>
<div class="definition">
<p><span id="def:unlabeled-div-107" class="definition"><strong>Definition 8.14  (Cramér-Rao Bound) </strong></span>Let <span class="math inline">\(\textbf{X}=(X_1,..., X_n), \textbf{X} \sim f_{\textbf{X}}(x|\theta), \theta \in \Theta\)</span>. If <span class="math inline">\(W=W(\textbf{X})\)</span> is some estimator such that</p>
<p><span class="math display" id="eq:reg-cond-1">\[\begin{equation}
\frac{\partial}{\partial\theta} E_{\theta}(W)=\int \frac{\partial}{\partial\theta} \bigg[ W(\textbf{X})f_{\textbf{X}}(x|\theta)\bigg]dx
\tag{8.1}
\end{equation}\]</span></p>
<p>then,</p>
<p><span class="math display">\[Var(W) \ge \frac{[\frac{\partial}{\partial\theta} E_{\theta}(W)]^2}{E_{\theta}([\frac{\partial}{\partial\theta} \log(f_{\textbf{X}}(x|\theta))]^2)}.\]</span></p>
</div>
<p><strong>Remark:</strong> If the sample is iid and</p>
<p><span class="math display" id="eq:reg-cond-2">\[
\begin{equation}
\frac{\partial^2}{\partial\theta^2} \int f(x|\theta) dx =  \int \frac{\partial^2}{\partial\theta^2} f(x|\theta) dx
\tag{8.2}
\end{equation}
\]</span>
then,
<span class="math display">\[Var(W) \ge \frac{[\frac{\partial}{\partial\theta} E_{\theta}(W)]^2}{nE_{\theta}[-\frac{\partial^2}{\partial\theta^2} ln(f_{X_i}(x|\theta))]}.\]</span></p>
<p>Before continuing, let’s discuss conditions <a href="point-estimators-finite-samples.html#eq:reg-cond-1">(8.1)</a> and <a href="point-estimators-finite-samples.html#eq:reg-cond-2">(8.2)</a>. First, if <a href="point-estimators-finite-samples.html#eq:reg-cond-1">(8.1)</a> is satisfied then
<span class="math display">\[E_{\theta}\bigg[\frac{\partial}{\partial\theta}\log(f_{\textbf{X}}(X|\theta))\bigg] = 0,  \forall \theta \in \Theta.\]</span>
Furthermore, if <a href="point-estimators-finite-samples.html#eq:reg-cond-2">(8.2)</a> is satisfied, then</p>
<p><span class="math display">\[E_{\theta}\bigg[\frac{\partial^2}{\partial\theta^2} \log(f(x|\theta))\bigg] = -E_{\theta}\bigg[\bigg(\frac{\partial}{\partial\theta} \log(f(x|\theta))\bigg)^2\bigg]. \]</span>
<strong>Remarks:</strong></p>
<ul>
<li><a href="point-estimators-finite-samples.html#eq:reg-cond-2">(8.2)</a> is satisfied for exponential families.</li>
<li>If <a href="point-estimators-finite-samples.html#eq:reg-cond-1">(8.1)</a> and <a href="point-estimators-finite-samples.html#eq:reg-cond-2">(8.2)</a> are not satisfied, then the CRLB might not be the lower bound for the variance.</li>
<li>Sometimes, especially for multiple parameters, we can also write the CRLB as <span class="math inline">\(Var(W) \geq I(\theta)^{-1}\)</span>, the inverse of of the <a href="#information">information</a> mentioned previously. Thus, if one is computing the MLE, the CRLB follows from the same calculations used for the MLE, which can save time, especially for problems involving common families of distributions.</li>
</ul>
<div class="example">
<p><span id="exm:unlabeled-div-108" class="example"><strong>Example 8.6  </strong></span>CRLB example</p>
</div>
<p>Since the CRLB is a lower bound, if the variance of an unbiased estimator <span class="math inline">\(Var(\hat{\theta})\)</span> equals the CRLB, then it must be the UMVUE.</p>
<p>While it is certainly possible to compute the CRLB directly, and then compute the variance of the estimator <span class="math inline">\(Var(\hat{\theta})\)</span>, and check if these values are the same, there is also a simpler method. The <em>Attainment Theorem</em> tells us whether a specific estimator attains the CRLB without requiring that the bound actually be computed:</p>
<div class="theorem">
<p><span id="thm:unlabeled-div-109" class="theorem"><strong>Theorem 8.3  (Attainment Theorem) </strong></span>Let <span class="math inline">\(W(X)\)</span> be an unbiased <span class="math inline">\(\tau(\theta)\)</span> and suppose that <a href="point-estimators-finite-samples.html#eq:reg-cond-1">(8.1)</a> holds. Then <span class="math inline">\(W\)</span> attains CRLB if and only if</p>
<p><span class="math display">\[\frac{\partial}{\partial \theta} \log (\mathcal{L}(\theta|X)) = a(\theta)[W-\tau(\theta)]\]</span></p>
<p>for some <span class="math inline">\(a(\theta)\)</span></p>
</div>
<p>As a result, to prove whether an estimator does <em>or does not</em> attain the CRLB, we simply must show that the <a href="#score">score</a> of the sample can or cannot be factorized as above, which is often faster than computing the variance of the estimator.</p>
<div class="example">
<p><span id="exm:unlabeled-div-110" class="example"><strong>Example 8.7  </strong></span>attainment theorem example</p>
</div>
<p><strong>Note:</strong> An estimator that does not attain CRLB can still be UMVUE! For some parameters, CRLB might not be achieved.</p>
</div>
<div id="rao-blackwell-and-lehmann-scheffé-theorem" class="section level3 hasAnchor" number="8.4.2">
<h3><span class="header-section-number">8.4.2</span> Rao-Blackwell and Lehmann-Scheffé Theorem<a href="point-estimators-finite-samples.html#rao-blackwell-and-lehmann-scheffé-theorem" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The more common approach to finding an UMVUE is to use the Lehmann-Scheffé theorem, which describes specifically how to construct an UMVUE using a complete sufficient statistic (as discussed in <a href="statistics.html#statistics">Chapter 7</a>).</p>
<p>Before introducing the Lehmann-Scheffé theorem, first let us state the more general theorem upon which its proof is based:</p>
<div class="theorem">
<p><span id="thm:unlabeled-div-111" class="theorem"><strong>Theorem 8.4  (Rao-Blackwell) </strong></span>Suppose <span class="math inline">\(W\)</span> is unbiased for <span class="math inline">\(\tau(\theta)\)</span> and <span class="math inline">\(T\)</span> is a SS for <span class="math inline">\(\tau(\theta)\)</span>. Let <span class="math inline">\(Y:=E(W|T)\)</span>. Then,</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(Y\)</span> is a statistic (not a function of <span class="math inline">\(\theta\)</span>)</li>
<li><span class="math inline">\(E[Y] = \tau(\theta), \forall \theta\)</span></li>
<li><span class="math inline">\(Var(Y)\le Var(W), \forall \theta\)</span>.</li>
</ol>
</div>
<p>The Rao-Blackwell theorem provides a method to improve upon any unbiased estimator, i.e. provides a <em>better</em> unbiased estimator, by conditioning on a sufficient statistic. However, if this statistic is <em>also</em> complete, then the Lehmann-Scheffé provides a method to find an UMVUE. This is because of the following theorem:</p>
<div class="theorem">
<p><span id="thm:unlabeled-div-112" class="theorem"><strong>Theorem 8.5  </strong></span>If <span class="math inline">\(E_{\theta}(W(X))= \tau(\theta)\)</span>, then <span class="math inline">\(W(X)\)</span> is best unbiased if and only if <span class="math inline">\(W(X)\)</span> is uncorrelated with all unbiased estimators of 0.</p>
</div>
<p>How do we guarantee that <span class="math inline">\(W\)</span> is uncorrelated with all unbiased estimators of 0? A complete statistic contains no unbiased estimators of 0 (other than 0 itself). This is by definition; <span class="math inline">\(E(g(T) = 0 \implies g(T) = 0\)</span> with probability 1, as established in <a href="statistics.html#complete-stats">Complete Statistics</a>. Therefore, if we condition on a complete statistic, then the resulting statistic must be uncorrelated with all unbiased estimators of 0, and therefore be an UMVUE. This principle forms the foundation of the Lehmann-Scheffé Theorem:</p>
<div class="theorem">
<p><span id="thm:unlabeled-div-113" class="theorem"><strong>Theorem 8.6  (Lehmann-Scheffé) </strong></span>Suppose <span class="math inline">\(W\)</span> is unbiased for <span class="math inline">\(\tau(\theta)\)</span> and <span class="math inline">\(T\)</span> is a <em>complete</em> and <em>sufficient</em> for <span class="math inline">\(\tau(\theta)\)</span>. Then <span class="math inline">\(Y:=E(W|T)\)</span> is UMVUE of <span class="math inline">\(\tau(\theta)\)</span>.</p>
<p>In addition, let <span class="math inline">\(W\)</span> be a statistic such that <span class="math inline">\(Var(W)&lt;\infty\)</span>, and <span class="math inline">\(W\)</span> is UMVUE for <span class="math inline">\(\tau(\theta)\)</span>. Then <span class="math inline">\(W\)</span> is the unique UMVUE of <span class="math inline">\(\tau(\theta)\)</span>.</p>
</div>
</div>
<div id="finding-umvues-ii-using-the-lehmann-scheffé-theorem" class="section level3 hasAnchor" number="8.4.3">
<h3><span class="header-section-number">8.4.3</span> Finding UMVUEs II: Using the Lehmann-Scheffé Theorem<a href="point-estimators-finite-samples.html#finding-umvues-ii-using-the-lehmann-scheffé-theorem" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>How would one use the Lehmann-Scheffé Theorem to construct an UMVUE in practice? Let’s consider an example.</p>
<div class="example">
<p><span id="exm:unlabeled-div-114" class="example"><strong>Example 8.8  (Conditioning on a Complete Statistic) </strong></span>Let <span class="math inline">\(X_i \overset{iid}{\sim} \text{Bernoulli}(p)\)</span>, and our goal is to find an UMVUE of <span class="math inline">\(p^2\)</span>.</p>
<p>First, we need to find an unbiased estimator of <span class="math inline">\(p^2\)</span>. Since all observations are iid, <span class="math inline">\(X_1\cdot X_2\)</span> should suffice, as <span class="math inline">\(E(X_1X_2) = E(X_1) \cdot E(X_2) = p^2\)</span>.</p>
<p>Since the Bernoulli is an exponential family with <span class="math inline">\(w(\theta) = \log(\frac{p}{1-p})\)</span> an open set, we know its CSS is <span class="math inline">\(\sum_{i=1}^n X_i\)</span> by Lehmann-Scheffé, the UMVUE is</p>
<p><span class="math display">\[E(X_1X_2 | \sum_{i=1}^n X_i = t)\]</span>
Since the expected value of a Bernoulli can be written as a probability, this equals</p>
<p><span class="math display">\[
P(X_1X_2 = 1 | \sum_{i=1}^n X_i = t) = \frac{P(X_1 = 1, X_2 = 1, \sum_{i=3}^n X_i = t - 2)}{P(\sum_{i=1}^n X_i = t)} \\
= \frac{P(X_1=1) \cdot P(X_2=1) \cdot P(\sum_{i=3}^n X_i = t-2)}{{n\choose t}p^x(1-p)^{n-t}}\\
= \frac{p^2{n-2\choose t-2}p^{t-2}(1-p)^{n-2 - t+2}}{{n\choose t}p^t(1-p)^{n-t}}\\
= \frac{{n-2\choose t-2}}{{n\choose t}} = \frac{t(t-1)}{n(n-1)}\\
\]</span>
using the fact that <span class="math inline">\(\sum_{i=1}^nX_i \sim \text{Binomial}(n,p)\)</span>. Therefore, the UMVUE is <span class="math inline">\(\frac{t(t-1)}{n(n-1)}\)</span></p>
</div>
</div>
<div id="finding-umvues-iii-lehmann-scheffé-corollary" class="section level3 hasAnchor" number="8.4.4">
<h3><span class="header-section-number">8.4.4</span> Finding UMVUEs III: Lehmann-Scheffé Corollary<a href="point-estimators-finite-samples.html#finding-umvues-iii-lehmann-scheffé-corollary" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div class="corollary">
<p><span id="cor:unlabeled-div-115" class="corollary"><strong>Corollary 8.1  (Lehmann-Scheffé) </strong></span>Suppose <span class="math inline">\(T\)</span> is a complete sufficient statistic for <span class="math inline">\(\theta\)</span>. Let <span class="math inline">\(g(T)\)</span> be an estimator based solely on <span class="math inline">\(T\)</span>. Then, <span class="math inline">\(g(T)\)</span> is the UMVUE of its expected value - that is, <span class="math inline">\(E(g(T))\)</span> is an UMVUE.</p>
</div>
<p>Rather than playing around with conditional probabilities to construct an UMVUE using Lehmann-Scheffé, it can often be much easier to “work backwards” based on what you know about the CSS and find a function <span class="math inline">\(g\)</span> such that <span class="math inline">\(E(g(T)) = \tau(\theta)\)</span>. Here is an algorithm for using the Lehmann-Scheffé corollary for a function of a parameter <span class="math inline">\(\tau(\theta)\)</span> given <span class="math inline">\(X_1, ..., X_n \sim f_{X_i}(x|\theta)\)</span></p>
<ol style="list-style-type: decimal">
<li>Find the CSS of the distribution <span class="math inline">\(f_{X_i}(x|\theta)\)</span>. Call it <span class="math inline">\(T\)</span>.</li>
<li>Find the distribution of <span class="math inline">\(T\)</span>. Often, these are known based on convolution formulas. See Chapter 7 for a <a href="#id_#table-ss">table of sufficient statistic distributions</a>.</li>
<li>Set <span class="math inline">\(E(g(T)) = \tau(\theta)\)</span>, and rearrange to get <span class="math inline">\(E\Big(\frac{g(T)}{\tau(\theta)}\Big) = 1\)</span>.</li>
<li>(Optional) Use the linearity of expectation to split the above into multiple integration problems. For example, if <span class="math inline">\(\tau(\theta) = \tau_1(\theta) + \tau_2(\theta) + ... + \tau_k(\theta)\)</span>, then we can let <span class="math inline">\(E(g(T)) = E(g_1(T)) + E(g_2(T)) + ... + E(g_k(T))\)</span> with each <span class="math inline">\(E(g_1(T)) = \tau_i(\theta)\)</span></li>
<li>Because <span class="math inline">\(E\Big(\frac{g(T)}{\tau(\theta)}\Big)\)</span> must integrate to 1, <span class="math inline">\(\frac{g(T)}{\tau(\theta)}\)</span> must be a pdf. Therefore, you can use the <a href="#kernel-technique">Kernel Technique</a> to solve for the <span class="math inline">\(g(T)\)</span> that converts the rest of the integrand into a pdf.</li>
</ol>
<p>That last step probably sounds a bit complicated. Let’s look at an example.</p>
<div class="example">
<p><span id="exm:unlabeled-div-116" class="example"><strong>Example 8.9  (Constructing an UMVUE using a function of a CSS) </strong></span>Suppose <span class="math inline">\(X_i \sim \text{Exp}(\lambda)\)</span>. Find an UMVUE for <span class="math inline">\(\tau(\lambda) = \lambda^k\)</span> with <span class="math inline">\(k \in \{1, ... n\}\)</span> using the algorithm above.</p>
<p>Let us proceed step-by-step.</p>
<ol style="list-style-type: decimal">
<li>Since the exponential distribution is (trivially) a single-parameter exponential family, <span class="math inline">\(T = \sum_{i=1}^nt_1(X) = \sum_{i=1}^nX_i\)</span> is a sufficient statistic.</li>
<li>It is known, <a href="#table-SS">by convolution properties</a>, that <span class="math inline">\(T = \sum_{i=1}^nX_i \sim \text{Gamma}(n, \lambda)\)</span>. Hence, we’ve found the distribution of <span class="math inline">\(T\)</span></li>
<li>Let <span class="math inline">\(E\Big(\frac{g(T)}{\tau(\theta)}\Big) = 1\)</span>, which implies</li>
</ol>
<p><span class="math display">\[
E\Big(\frac{g(T)}{\tau(\theta)}\Big) = \int_0^\infty\frac{g(t)}{\lambda^k}\cdot\frac{1}{\Gamma(n)\lambda^n}t^{n-1}\exp(-\frac{t}{\lambda})dt = 1\\
\implies \int_0^\infty g(t)\cdot\frac{1}{\Gamma(n)\lambda^{n+k}}t^{n-1}\exp(-\frac{t}{\lambda})dt = 1
\]</span></p>
<ol start="5" style="list-style-type: decimal">
<li><p>Clearly this can be converted into a <span class="math inline">\(\text{Gamma}(n+k, \lambda)\)</span> pdf by setting <span class="math inline">\(g(t)\)</span> appropriately. Let’s algebraically manipulate to insert <span class="math inline">\(n+k\)</span> where it is needed, and keep track of our transformations.</p></li>
<li><p>Since <span class="math inline">\(n\)</span> and <span class="math inline">\(k\)</span> are integers, <span class="math inline">\(\frac{1}{\Gamma(n+k)} = \frac{1}{(n+k)(n+k-1)...(n+1)\Gamma(n)}\)</span></p></li>
<li><p><span class="math inline">\(t^{n + k -1} = t^{n - 1}\cdot t^k\)</span></p></li>
</ol>
<p>These transformations give us the <span class="math inline">\(\text{Gamma}(n+k, \lambda)\)</span> pdf we need. Let <span class="math inline">\(g(T) = \frac{T^k}{(n+k)(n+k-1)...(n+1)}\)</span>. Then, by the Lehmann Scheffé Corollary, <span class="math inline">\(g(T)\)</span> must be an UMVUE for <span class="math inline">\(\tau(\lambda)\)</span>, since <span class="math inline">\(E(g(T)) = \lambda^k\)</span> and <span class="math inline">\(T\)</span> is a complete sufficient statistic.</p>
</div>
</div>
<div id="proving-an-umvue-does-not-exist" class="section level3 hasAnchor" number="8.4.5">
<h3><span class="header-section-number">8.4.5</span> Proving an UMVUE Does Not Exist<a href="point-estimators-finite-samples.html#proving-an-umvue-does-not-exist" class="anchor-section" aria-label="Anchor link to header"></a></h3>
</div>
</div>
<div id="inferential-properties-of-exponential-families-distributions" class="section level2 hasAnchor" number="8.5">
<h2><span class="header-section-number">8.5</span> Inferential Properties of Exponential Families Distributions<a href="point-estimators-finite-samples.html#inferential-properties-of-exponential-families-distributions" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Suppose we draw a sample of <span class="math inline">\(n\)</span> iid random variables <span class="math inline">\(X_1,...,X_n\)</span> following one of the distributions below. The proceeding tables list the inferential properties of this sample.</p>
<div id="bernoulli-1" class="section level3 hasAnchor" number="8.5.1">
<h3><span class="header-section-number">8.5.1</span> Bernoulli<a href="point-estimators-finite-samples.html#bernoulli-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<table>
<colgroup>
<col width="50%" />
<col width="50%" />
</colgroup>
<tbody>
<tr class="odd">
<td align="center">Log-likelihood</td>
<td align="center"><span class="math inline">\(\ell(\theta|X) = n\log(1-p) + \log(\frac{p}{1-p})\sum_{i=1}^n x_i\)</span></td>
</tr>
<tr class="even">
<td align="center">Score Equations</td>
<td align="center"><span class="math inline">\(U_n(\theta|X) = -\frac{n}{1-p} + \frac{1}{p(1-p)}\sum_{i=1}^n x_i\)</span></td>
</tr>
<tr class="odd">
<td align="center">Observed Information</td>
<td align="center"><span class="math inline">\(-\frac{\partial^2}{\partial\theta^2}\ell(\theta|X) = \frac{n}{(1-p)^2} + \frac{1-2p}{p^2(1-p)^2}\sum_{i=1}^n x_i\)</span></td>
</tr>
<tr class="even">
<td align="center">Fisher Information</td>
<td align="center"><span class="math inline">\(I(\theta)= \frac{n}{p(1-p)}\)</span></td>
</tr>
<tr class="odd">
<td align="center">MLE</td>
<td align="center"><span class="math inline">\(\frac{1}{n}\sum_{i=1}^n x_i\)</span></td>
</tr>
</tbody>
</table>
</div>
<div id="binomial-1" class="section level3 hasAnchor" number="8.5.2">
<h3><span class="header-section-number">8.5.2</span> Binomial<a href="point-estimators-finite-samples.html#binomial-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Since the Binomial has <span class="math inline">\(n\)</span> as a parameter, notation in problems that involve a <em>sample</em> of <span class="math inline">\(n\)</span> iid Binomial random variables can be tricky. To clarify, in the following table let <span class="math inline">\(X_i \sim \text{Binomial}(m, p)\)</span>, and let <span class="math inline">\(n\)</span> represent the number of samples.</p>
<table>
<colgroup>
<col width="50%" />
<col width="50%" />
</colgroup>
<tbody>
<tr class="odd">
<td align="center">Log-likelihood</td>
<td align="center"><span class="math inline">\(\ell(\theta|X) = nm\log(1-p) + \log(\frac{p}{1-p})\sum_{i=1}^n x_i + \log({m\choose x_i})\)</span></td>
</tr>
<tr class="even">
<td align="center">Score Equations</td>
<td align="center"><span class="math inline">\(U_n(\theta|X) = -\frac{nm}{1-p} + \frac{1}{p(1-p)}\sum_{i=1}^n x_i\)</span></td>
</tr>
<tr class="odd">
<td align="center">Observed Information</td>
<td align="center"><span class="math inline">\(-\frac{\partial^2}{\partial\theta^2}\ell(\theta|X) = \frac{nm}{(1-p)^2} + \frac{1-2p}{p^2(1-p)^2}\sum_{i=1}^n x_i\)</span></td>
</tr>
<tr class="even">
<td align="center">Fisher Information</td>
<td align="center"><span class="math inline">\(I(\theta)= \frac{nm}{p(1-p)}\)</span></td>
</tr>
<tr class="odd">
<td align="center">MLE</td>
<td align="center"><span class="math inline">\(\frac{1}{nm}\sum_{i=1}^n x_i\)</span></td>
</tr>
</tbody>
</table>
</div>
<div id="geometric" class="section level3 hasAnchor" number="8.5.3">
<h3><span class="header-section-number">8.5.3</span> Geometric<a href="point-estimators-finite-samples.html#geometric" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<table>
<colgroup>
<col width="50%" />
<col width="50%" />
</colgroup>
<tbody>
<tr class="odd">
<td align="center">Log-likelihood</td>
<td align="center"><span class="math inline">\(\ell(\theta|X) = n\log(p) + \log(1-p)\sum_{i=1}^n x_i\)</span></td>
</tr>
<tr class="even">
<td align="center">Score Equations</td>
<td align="center"><span class="math inline">\(U_n(\theta|X) = \frac{n}{p} - \frac{1}{1-p}\sum_{i=1}^n x_i\)</span></td>
</tr>
<tr class="odd">
<td align="center">Observed Information</td>
<td align="center"></td>
</tr>
<tr class="even">
<td align="center">Fisher Information</td>
<td align="center"></td>
</tr>
<tr class="odd">
<td align="center">MLE</td>
<td align="center"><span class="math inline">\(\frac{n}{\sum_{i=1}^n x_i}\)</span></td>
</tr>
</tbody>
</table>
</div>
<div id="negative-binomial-1" class="section level3 hasAnchor" number="8.5.4">
<h3><span class="header-section-number">8.5.4</span> Negative Binomial<a href="point-estimators-finite-samples.html#negative-binomial-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<table>
<colgroup>
<col width="50%" />
<col width="50%" />
</colgroup>
<tbody>
<tr class="odd">
<td align="center">Log-likelihood</td>
<td align="center"><span class="math inline">\(\ell(\theta|X) = nr\log(\frac{p}{1-p}) + \sum_{i=1}^n x_i\log(1-p) + \log{x_i + r - 1\choose x_i}\)</span></td>
</tr>
<tr class="even">
<td align="center">Score Equations</td>
<td align="center"><span class="math inline">\(U_n(\theta|X) = -\frac{nr}{p} - \frac{1}{1-p}\sum_{i=1}^n x_i\)</span></td>
</tr>
<tr class="odd">
<td align="center">Observed Information</td>
<td align="center"></td>
</tr>
<tr class="even">
<td align="center">Fisher Information</td>
<td align="center"><span class="math inline">\(\frac{r}{(1-p)^2p}\)</span></td>
</tr>
<tr class="odd">
<td align="center">MLE</td>
<td align="center"><span class="math inline">\(\frac{1}{1 - \frac{1}{nr}\sum_{i=1}^nx_i}\)</span></td>
</tr>
</tbody>
</table>
</div>
<div id="poisson-1" class="section level3 hasAnchor" number="8.5.5">
<h3><span class="header-section-number">8.5.5</span> Poisson<a href="point-estimators-finite-samples.html#poisson-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<table>
<colgroup>
<col width="50%" />
<col width="50%" />
</colgroup>
<tbody>
<tr class="odd">
<td align="center">Log-likelihood</td>
<td align="center"><span class="math inline">\(\ell(\theta|X) = n\lambda + \sum_{i=1}^n x_i\log(\lambda) - \log(x_i!)\)</span></td>
</tr>
<tr class="even">
<td align="center">Score Equations</td>
<td align="center"><span class="math inline">\(U_n(\theta|X) = -n + \frac{1}{\lambda}x_i\)</span></td>
</tr>
<tr class="odd">
<td align="center">Observed Information</td>
<td align="center"></td>
</tr>
<tr class="even">
<td align="center">Fisher Information</td>
<td align="center"><span class="math inline">\(\frac{1}{\lambda}\)</span></td>
</tr>
<tr class="odd">
<td align="center">MLE</td>
<td align="center"><span class="math inline">\(\frac{1}{n}\sum_{i=1}^nx_i\)</span></td>
</tr>
</tbody>
</table>
</div>
<div id="normal-1" class="section level3 hasAnchor" number="8.5.6">
<h3><span class="header-section-number">8.5.6</span> Normal<a href="point-estimators-finite-samples.html#normal-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<table>
<colgroup>
<col width="50%" />
<col width="50%" />
</colgroup>
<tbody>
<tr class="odd">
<td align="center">Log-likelihood</td>
<td align="center"><span class="math inline">\(\ell(\theta|X) = -\frac{n}{2}\log(2\pi\sigma^2) - \frac{1}{2\sigma^2}\sum_{i=1}^n (x_i - \mu)^2\)</span></td>
</tr>
<tr class="even">
<td align="center">Score Equations</td>
<td align="center"><span class="math inline">\(U(\mu | x, \sigma^2) = -n\mu -\frac{1}{\sigma^2}\sum_{i=1}^n x_i \\ U(\sigma^2 | x, \mu) = -\frac{n}{2\sigma^2} + \frac{1}{2(\sigma^2)^2}\sum_{i=1}^n(x_i-\mu)^2\)</span></td>
</tr>
<tr class="odd">
<td align="center">Observed Information</td>
<td align="center"></td>
</tr>
<tr class="even">
<td align="center">Fisher Information</td>
<td align="center"><span class="math inline">\(\begin{bmatrix}\frac{1}{\sigma^2} &amp; 0 \\ 0 &amp; \frac{1}{2\sigma^4}\end{bmatrix}\)</span></td>
</tr>
<tr class="odd">
<td align="center">MLE</td>
<td align="center"><span class="math inline">\(\frac{1}{n}\sum_{i=1}^nx_i\)</span></td>
</tr>
</tbody>
</table>
</div>
<div id="exponential-1" class="section level3 hasAnchor" number="8.5.7">
<h3><span class="header-section-number">8.5.7</span> Exponential<a href="point-estimators-finite-samples.html#exponential-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<table>
<colgroup>
<col width="50%" />
<col width="50%" />
</colgroup>
<tbody>
<tr class="odd">
<td align="center">Log-likelihood</td>
<td align="center"><span class="math inline">\(\ell(\theta|X) = -n\log(\lambda) - \frac{1}{\lambda}\sum_{i=1}^n x_i\)</span></td>
</tr>
<tr class="even">
<td align="center">Score Equations</td>
<td align="center"><span class="math inline">\(U(\mu | x, \sigma^2) = -\frac{n}{\lambda} + \frac{1}{\lambda^2}\sum_{i=1}^n x_i\)</span></td>
</tr>
<tr class="odd">
<td align="center">Observed Information</td>
<td align="center"></td>
</tr>
<tr class="even">
<td align="center">Fisher Information</td>
<td align="center"><span class="math inline">\(\lambda^2\)</span></td>
</tr>
<tr class="odd">
<td align="center">MLE</td>
<td align="center"><span class="math inline">\(\frac{1}{n}\sum_{i=1}^nx_i\)</span></td>
</tr>
</tbody>
</table>
</div>
<div id="gamma-1" class="section level3 hasAnchor" number="8.5.8">
<h3><span class="header-section-number">8.5.8</span> Gamma<a href="point-estimators-finite-samples.html#gamma-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<table>
<colgroup>
<col width="50%" />
<col width="50%" />
</colgroup>
<tbody>
<tr class="odd">
<td align="center">Log-likelihood</td>
<td align="center"><span class="math inline">\(\ell(\theta|X) = -n\log(\Gamma(k)) - nk\log(\lambda) + (k - 1 - \frac{1}{\lambda})\sum_{i=1}^n x_i\)</span></td>
</tr>
<tr class="even">
<td align="center">Score Equations</td>
<td align="center"><span class="math inline">\(U(\mu | x, \sigma^2) = -\frac{nk}{\lambda} + \frac{1}{\lambda^2}\sum_{i=1}^n x_i\)</span> with <span class="math inline">\(k\)</span> known (otherwise, requires differentiating <span class="math inline">\(\Gamma(k)\)</span>)</td>
</tr>
<tr class="odd">
<td align="center">Observed Information</td>
<td align="center"></td>
</tr>
<tr class="even">
<td align="center">Fisher Information</td>
<td align="center"><span class="math inline">\(\begin{bmatrix}\psi^{(1)}(k) &amp; \frac{1}{\lambda}\\ \frac{1}{\lambda} &amp; \frac{k}{\lambda^2}\end{bmatrix}\)</span></td>
</tr>
<tr class="odd">
<td align="center">MLE</td>
<td align="center"></td>
</tr>
</tbody>
</table>
</div>
<div id="pareto-1" class="section level3 hasAnchor" number="8.5.9">
<h3><span class="header-section-number">8.5.9</span> Pareto<a href="point-estimators-finite-samples.html#pareto-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<table>
<colgroup>
<col width="50%" />
<col width="50%" />
</colgroup>
<tbody>
<tr class="odd">
<td align="center">Log-likelihood</td>
<td align="center"><span class="math inline">\(\ell(\theta|X) = n\log{\alpha} + n\alpha\log(x_m) - (\alpha + 1)\sum_{i=1}^n\log(x_i)\)</span></td>
</tr>
<tr class="even">
<td align="center">Score Equations</td>
<td align="center"><span class="math inline">\(U(\mu | x, \sigma^2) = \frac{n}{\alpha} + n\log(x_m) - \sum_{i=1}^n \log(x_i)\)</span></td>
</tr>
<tr class="odd">
<td align="center">Observed Information</td>
<td align="center"></td>
</tr>
<tr class="even">
<td align="center">Fisher Information</td>
<td align="center"><span class="math inline">\(\frac{n}{\alpha^2}\)</span></td>
</tr>
<tr class="odd">
<td align="center">MLE</td>
<td align="center"></td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="statistics.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="point-estimators-asymptotics.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown-demo/edit/master/08-point-estimators-finite-samples.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["Solving-Statistical-Problems.pdf", "Solving-Statistical-Problems.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
