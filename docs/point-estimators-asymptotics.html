<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 9 Point Estimators: Asymptotics | Solving Statistical Problems</title>
  <meta name="description" content="Chapter 9 Point Estimators: Asymptotics | Solving Statistical Problems" />
  <meta name="generator" content="bookdown 0.34 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 9 Point Estimators: Asymptotics | Solving Statistical Problems" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="Chapter 9 Point Estimators: Asymptotics | Solving Statistical Problems" />
  <meta name="github-repo" content="salbalkus/Solving-Statistical-Problems" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 9 Point Estimators: Asymptotics | Solving Statistical Problems" />
  
  <meta name="twitter:description" content="Chapter 9 Point Estimators: Asymptotics | Solving Statistical Problems" />
  

<meta name="author" content="Salvador Balkus, Kimberly Greco, and Mónica Robles Fontán" />


<meta name="date" content="2023-07-02" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="point-estimators-finite-samples.html"/>
<link rel="next" href="hypothesis-tests-finite-samples.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>
<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Solving Statistical Problems</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Introduction</a></li>
<li class="chapter" data-level="2" data-path="math-tricks.html"><a href="math-tricks.html"><i class="fa fa-check"></i><b>2</b> Math Tricks</a>
<ul>
<li class="chapter" data-level="2.1" data-path="math-tricks.html"><a href="math-tricks.html#combinatorics"><i class="fa fa-check"></i><b>2.1</b> Combinatorics</a></li>
<li class="chapter" data-level="2.2" data-path="math-tricks.html"><a href="math-tricks.html#geometric-series"><i class="fa fa-check"></i><b>2.2</b> Geometric Series</a></li>
<li class="chapter" data-level="2.3" data-path="math-tricks.html"><a href="math-tricks.html#exponential-taylor"><i class="fa fa-check"></i><b>2.3</b> Exponential Taylor Series</a></li>
<li class="chapter" data-level="2.4" data-path="math-tricks.html"><a href="math-tricks.html#exponential-limit"><i class="fa fa-check"></i><b>2.4</b> Exponential Limit</a></li>
<li class="chapter" data-level="2.5" data-path="math-tricks.html"><a href="math-tricks.html#ibp"><i class="fa fa-check"></i><b>2.5</b> Integration by Parts</a></li>
<li class="chapter" data-level="2.6" data-path="math-tricks.html"><a href="math-tricks.html#leibniz-rule"><i class="fa fa-check"></i><b>2.6</b> Leibniz’s Rule</a></li>
<li class="chapter" data-level="2.7" data-path="math-tricks.html"><a href="math-tricks.html#gamma-function"><i class="fa fa-check"></i><b>2.7</b> Gamma Function</a></li>
<li class="chapter" data-level="2.8" data-path="math-tricks.html"><a href="math-tricks.html#triangle-inequality"><i class="fa fa-check"></i><b>2.8</b> Triangle Inequality</a></li>
<li class="chapter" data-level="2.9" data-path="math-tricks.html"><a href="math-tricks.html#fubinis-theorem"><i class="fa fa-check"></i><b>2.9</b> Fubini’s Theorem</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="probability.html"><a href="probability.html"><i class="fa fa-check"></i><b>3</b> Probability</a>
<ul>
<li class="chapter" data-level="3.1" data-path="probability.html"><a href="probability.html#basic-axioms"><i class="fa fa-check"></i><b>3.1</b> Basic Axioms</a></li>
<li class="chapter" data-level="3.2" data-path="probability.html"><a href="probability.html#basic-probability-solving-techniques"><i class="fa fa-check"></i><b>3.2</b> Basic Probability Solving Techniques</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="probability.html"><a href="probability.html#disjointify"><i class="fa fa-check"></i><b>3.2.1</b> Disjointification</a></li>
<li class="chapter" data-level="3.2.2" data-path="probability.html"><a href="probability.html#demorgan"><i class="fa fa-check"></i><b>3.2.2</b> DeMorgan’s Laws</a></li>
<li class="chapter" data-level="3.2.3" data-path="probability.html"><a href="probability.html#proving-inequalities-subsetting"><i class="fa fa-check"></i><b>3.2.3</b> Proving Inequalities: Subsetting</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="probability.html"><a href="probability.html#conditional-probability"><i class="fa fa-check"></i><b>3.3</b> Conditional Probability</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="probability.html"><a href="probability.html#conditional-probability-in-practice"><i class="fa fa-check"></i><b>3.3.1</b> Conditional Probability in Practice</a></li>
<li class="chapter" data-level="3.6.3" data-path="probability.html"><a href="probability.html#important-theorems"><i class="fa fa-check"></i><b>3.6.3</b> Important Theorems</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="known-distributions.html"><a href="known-distributions.html"><i class="fa fa-check"></i><b>4</b> Known Distributions</a>
<ul>
<li class="chapter" data-level="4.1" data-path="known-distributions.html"><a href="known-distributions.html#families-of-distributions"><i class="fa fa-check"></i><b>4.1</b> Families of Distributions</a></li>
<li class="chapter" data-level="4.2" data-path="known-distributions.html"><a href="known-distributions.html#location-scale"><i class="fa fa-check"></i><b>4.2</b> Location and Scale Families</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="known-distributions.html"><a href="known-distributions.html#location-families"><i class="fa fa-check"></i><b>4.2.1</b> Location Families</a></li>
<li class="chapter" data-level="4.2.2" data-path="known-distributions.html"><a href="known-distributions.html#scale-families"><i class="fa fa-check"></i><b>4.2.2</b> Scale Families</a></li>
<li class="chapter" data-level="4.2.3" data-path="known-distributions.html"><a href="known-distributions.html#properties-of-location-scale-families"><i class="fa fa-check"></i><b>4.2.3</b> Properties of Location-Scale Families</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="known-distributions.html"><a href="known-distributions.html#exponential-family"><i class="fa fa-check"></i><b>4.3</b> Exponential Families</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="known-distributions.html"><a href="known-distributions.html#properties"><i class="fa fa-check"></i><b>4.3.1</b> Properties</a></li>
<li class="chapter" data-level="4.3.2" data-path="known-distributions.html"><a href="known-distributions.html#natural-exponential-family"><i class="fa fa-check"></i><b>4.3.2</b> Natural Exponential Families</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="known-distributions.html"><a href="known-distributions.html#known-univariate-exponential-families"><i class="fa fa-check"></i><b>4.4</b> Known Univariate Exponential Families</a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="known-distributions.html"><a href="known-distributions.html#bernoulli"><i class="fa fa-check"></i><b>4.4.1</b> Bernoulli</a></li>
<li class="chapter" data-level="4.4.2" data-path="known-distributions.html"><a href="known-distributions.html#binomial"><i class="fa fa-check"></i><b>4.4.2</b> Binomial</a></li>
<li class="chapter" data-level="4.4.3" data-path="known-distributions.html"><a href="known-distributions.html#geometric-distribution"><i class="fa fa-check"></i><b>4.4.3</b> Geometric</a></li>
<li class="chapter" data-level="4.4.4" data-path="known-distributions.html"><a href="known-distributions.html#negative-binomial"><i class="fa fa-check"></i><b>4.4.4</b> Negative Binomial</a></li>
<li class="chapter" data-level="4.4.5" data-path="known-distributions.html"><a href="known-distributions.html#poisson"><i class="fa fa-check"></i><b>4.4.5</b> Poisson</a></li>
<li class="chapter" data-level="4.4.6" data-path="known-distributions.html"><a href="known-distributions.html#normal"><i class="fa fa-check"></i><b>4.4.6</b> Normal</a></li>
<li class="chapter" data-level="4.4.7" data-path="known-distributions.html"><a href="known-distributions.html#exponential"><i class="fa fa-check"></i><b>4.4.7</b> Exponential</a></li>
<li class="chapter" data-level="4.4.8" data-path="known-distributions.html"><a href="known-distributions.html#gamma"><i class="fa fa-check"></i><b>4.4.8</b> Gamma</a></li>
<li class="chapter" data-level="4.4.9" data-path="known-distributions.html"><a href="known-distributions.html#beta"><i class="fa fa-check"></i><b>4.4.9</b> Beta</a></li>
<li class="chapter" data-level="4.4.10" data-path="known-distributions.html"><a href="known-distributions.html#chi-squared"><i class="fa fa-check"></i><b>4.4.10</b> Chi-squared</a></li>
<li class="chapter" data-level="4.4.11" data-path="known-distributions.html"><a href="known-distributions.html#weibull"><i class="fa fa-check"></i><b>4.4.11</b> Weibull</a></li>
<li class="chapter" data-level="4.4.12" data-path="known-distributions.html"><a href="known-distributions.html#pareto"><i class="fa fa-check"></i><b>4.4.12</b> Pareto</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="known-distributions.html"><a href="known-distributions.html#non-exponential-families"><i class="fa fa-check"></i><b>4.5</b> Non-exponential families</a>
<ul>
<li class="chapter" data-level="4.5.1" data-path="known-distributions.html"><a href="known-distributions.html#uniform"><i class="fa fa-check"></i><b>4.5.1</b> Uniform</a></li>
<li class="chapter" data-level="4.5.2" data-path="known-distributions.html"><a href="known-distributions.html#cauchy"><i class="fa fa-check"></i><b>4.5.2</b> Cauchy</a></li>
<li class="chapter" data-level="4.5.3" data-path="known-distributions.html"><a href="known-distributions.html#studentst"><i class="fa fa-check"></i><b>4.5.3</b> t-distribution</a></li>
<li class="chapter" data-level="4.5.4" data-path="known-distributions.html"><a href="known-distributions.html#f-distribution"><i class="fa fa-check"></i><b>4.5.4</b> F-distribution</a></li>
<li class="chapter" data-level="4.5.5" data-path="known-distributions.html"><a href="known-distributions.html#hypergeometric"><i class="fa fa-check"></i><b>4.5.5</b> Hypergeometric</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="known-distributions.html"><a href="known-distributions.html#multivariate-distributions"><i class="fa fa-check"></i><b>4.6</b> Multivariate Distributions</a>
<ul>
<li class="chapter" data-level="4.6.1" data-path="known-distributions.html"><a href="known-distributions.html#bivariate-normal"><i class="fa fa-check"></i><b>4.6.1</b> Bivariate Normal</a></li>
<li class="chapter" data-level="4.6.2" data-path="known-distributions.html"><a href="known-distributions.html#multivariate-normal"><i class="fa fa-check"></i><b>4.6.2</b> Multivariate Normal</a></li>
<li class="chapter" data-level="4.6.3" data-path="known-distributions.html"><a href="known-distributions.html#multinomial"><i class="fa fa-check"></i><b>4.6.3</b> Multinomial</a></li>
</ul></li>
<li class="chapter" data-level="4.7" data-path="known-distributions.html"><a href="known-distributions.html#medians-and-other-functionals-of-a-distribution"><i class="fa fa-check"></i><b>4.7</b> Medians and Other Functionals of a Distribution</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="new-distributions.html"><a href="new-distributions.html"><i class="fa fa-check"></i><b>5</b> New Distributions</a>
<ul>
<li class="chapter" data-level="5.1" data-path="new-distributions.html"><a href="new-distributions.html#transformations"><i class="fa fa-check"></i><b>5.1</b> Transformations</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="new-distributions.html"><a href="new-distributions.html#theorems"><i class="fa fa-check"></i><b>5.1.1</b> Theorems</a></li>
<li class="chapter" data-level="5.1.2" data-path="new-distributions.html"><a href="new-distributions.html#practical-strategy"><i class="fa fa-check"></i><b>5.1.2</b> Practical Strategy</a></li>
<li class="chapter" data-level="5.1.3" data-path="new-distributions.html"><a href="new-distributions.html#proving-independence-from-a-joint-transformation"><i class="fa fa-check"></i><b>5.1.3</b> Proving Independence From a Joint Transformation</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="new-distributions.html"><a href="new-distributions.html#computing-joint-probabilities"><i class="fa fa-check"></i><b>5.2</b> Computing Joint Probabilities</a></li>
<li class="chapter" data-level="5.3" data-path="new-distributions.html"><a href="new-distributions.html#probability-integral-transform"><i class="fa fa-check"></i><b>5.3</b> Probability Integral Transform</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="new-distributions.html"><a href="new-distributions.html#hiearchical-models-iterated-moments"><i class="fa fa-check"></i><b>5.3.1</b> Hiearchical Models (Iterated Moments)</a></li>
<li class="chapter" data-level="5.3.2" data-path="new-distributions.html"><a href="new-distributions.html#convolutions"><i class="fa fa-check"></i><b>5.3.2</b> Convolutions</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="moments.html"><a href="moments.html"><i class="fa fa-check"></i><b>6</b> Moments</a>
<ul>
<li class="chapter" data-level="6.1" data-path="moments.html"><a href="moments.html#basic-definitions"><i class="fa fa-check"></i><b>6.1</b> Basic Definitions</a></li>
<li class="chapter" data-level="6.2" data-path="moments.html"><a href="moments.html#expected-value"><i class="fa fa-check"></i><b>6.2</b> <span class="math inline">\(E(X)\)</span> Properties</a></li>
<li class="chapter" data-level="6.3" data-path="moments.html"><a href="moments.html#varx-properties"><i class="fa fa-check"></i><b>6.3</b> <span class="math inline">\(Var(X)\)</span> Properties</a></li>
<li class="chapter" data-level="6.4" data-path="moments.html"><a href="moments.html#covariance-and-correlation"><i class="fa fa-check"></i><b>6.4</b> Covariance and Correlation</a></li>
<li class="chapter" data-level="6.5" data-path="moments.html"><a href="moments.html#conditional-expectation"><i class="fa fa-check"></i><b>6.5</b> Conditional Expectation</a></li>
<li class="chapter" data-level="6.6" data-path="moments.html"><a href="moments.html#mgf"><i class="fa fa-check"></i><b>6.6</b> Moment Generating Functions</a></li>
<li class="chapter" data-level="6.7" data-path="moments.html"><a href="moments.html#moment-bounds"><i class="fa fa-check"></i><b>6.7</b> Moment Inequalities</a></li>
<li class="chapter" data-level="6.8" data-path="moments.html"><a href="moments.html#techniques-for-deriving-moments"><i class="fa fa-check"></i><b>6.8</b> Techniques for Deriving Moments</a>
<ul>
<li class="chapter" data-level="6.8.1" data-path="moments.html"><a href="moments.html#bernoulli-direct-summation"><i class="fa fa-check"></i><b>6.8.1</b> Bernoulli: Direct Summation</a></li>
<li class="chapter" data-level="6.8.2" data-path="moments.html"><a href="moments.html#uniform-direct-integration"><i class="fa fa-check"></i><b>6.8.2</b> Uniform: Direct Integration</a></li>
<li class="chapter" data-level="6.8.3" data-path="moments.html"><a href="moments.html#geometric-series-convergence"><i class="fa fa-check"></i><b>6.8.3</b> Geometric: Series Convergence</a></li>
<li class="chapter" data-level="6.8.4" data-path="moments.html"><a href="moments.html#binomial-kernel-technique-series-version"><i class="fa fa-check"></i><b>6.8.4</b> Binomial: Kernel Technique, Series Version</a></li>
<li class="chapter" data-level="6.8.5" data-path="moments.html"><a href="moments.html#negative-binomial-and-hypergeometric-computing-exx-1"><i class="fa fa-check"></i><b>6.8.5</b> Negative Binomial and Hypergeometric: Computing <span class="math inline">\(E(X(X-1))\)</span></a></li>
<li class="chapter" data-level="6.8.6" data-path="moments.html"><a href="moments.html#poisson-exponential-taylor-series"><i class="fa fa-check"></i><b>6.8.6</b> Poisson: Exponential Taylor Series</a></li>
<li class="chapter" data-level="6.8.7" data-path="moments.html"><a href="moments.html#exponential-integration-by-parts"><i class="fa fa-check"></i><b>6.8.7</b> Exponential: Integration By Parts</a></li>
<li class="chapter" data-level="6.8.8" data-path="moments.html"><a href="moments.html#gamma-and-beta-kernel-technique-integration-version"><i class="fa fa-check"></i><b>6.8.8</b> Gamma and Beta: Kernel Technique, Integration Version</a></li>
<li class="chapter" data-level="6.8.9" data-path="moments.html"><a href="moments.html#normal-location-scale-trick-and-polar-integration"><i class="fa fa-check"></i><b>6.8.9</b> Normal: Location-Scale Trick and Polar Integration</a></li>
</ul></li>
<li class="chapter" data-level="6.9" data-path="moments.html"><a href="moments.html#other-moments-for-reference"><i class="fa fa-check"></i><b>6.9</b> Other Moments (for reference)</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="statistics.html"><a href="statistics.html"><i class="fa fa-check"></i><b>7</b> Statistics</a>
<ul>
<li class="chapter" data-level="7.1" data-path="statistics.html"><a href="statistics.html#sufficient-stats"><i class="fa fa-check"></i><b>7.1</b> Sufficient Statistics</a>
<ul>
<li class="chapter" data-level="7.1.1" data-path="statistics.html"><a href="statistics.html#techniques-for-finding-sufficient-statistics"><i class="fa fa-check"></i><b>7.1.1</b> Techniques for Finding Sufficient Statistics</a></li>
<li class="chapter" data-level="7.1.2" data-path="statistics.html"><a href="statistics.html#exp-fam-ss"><i class="fa fa-check"></i><b>7.1.2</b> Exponential Family Sufficient Statistics</a></li>
<li class="chapter" data-level="7.1.3" data-path="statistics.html"><a href="statistics.html#a-note-on-distributions-of-sufficient-statistics"><i class="fa fa-check"></i><b>7.1.3</b> A Note on Distributions of Sufficient Statistics</a></li>
<li class="chapter" data-level="7.1.4" data-path="statistics.html"><a href="statistics.html#moments-of-the-sufficient-statistic"><i class="fa fa-check"></i><b>7.1.4</b> Moments of the Sufficient Statistic</a></li>
<li class="chapter" data-level="7.1.5" data-path="statistics.html"><a href="statistics.html#table-of-sufficient-statistics"><i class="fa fa-check"></i><b>7.1.5</b> Table of Sufficient Statistics</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="statistics.html"><a href="statistics.html#minimal-sufficiency"><i class="fa fa-check"></i><b>7.2</b> Minimal Sufficiency</a></li>
<li class="chapter" data-level="7.3" data-path="statistics.html"><a href="statistics.html#ancillary-stats"><i class="fa fa-check"></i><b>7.3</b> Ancillary Statistics</a>
<ul>
<li class="chapter" data-level="7.3.1" data-path="statistics.html"><a href="statistics.html#why-are-we-interested-in-ancillary-statistics"><i class="fa fa-check"></i><b>7.3.1</b> Why are we interested in ancillary statistics?</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="statistics.html"><a href="statistics.html#complete-stats"><i class="fa fa-check"></i><b>7.4</b> Complete Statistics</a>
<ul>
<li class="chapter" data-level="7.4.1" data-path="statistics.html"><a href="statistics.html#techniques-for-finding-css"><i class="fa fa-check"></i><b>7.4.1</b> Techniques for Finding CSS</a></li>
<li class="chapter" data-level="7.4.2" data-path="statistics.html"><a href="statistics.html#basus-theorem"><i class="fa fa-check"></i><b>7.4.2</b> Basu’s Theorem</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="point-estimators-finite-samples.html"><a href="point-estimators-finite-samples.html"><i class="fa fa-check"></i><b>8</b> Point Estimators: Finite Samples</a>
<ul>
<li class="chapter" data-level="8.1" data-path="point-estimators-finite-samples.html"><a href="point-estimators-finite-samples.html#identifiability"><i class="fa fa-check"></i><b>8.1</b> Identifiability</a></li>
<li class="chapter" data-level="8.2" data-path="point-estimators-finite-samples.html"><a href="point-estimators-finite-samples.html#method-of-moments"><i class="fa fa-check"></i><b>8.2</b> Method of Moments</a></li>
<li class="chapter" data-level="8.3" data-path="point-estimators-finite-samples.html"><a href="point-estimators-finite-samples.html#mle-theory"><i class="fa fa-check"></i><b>8.3</b> MLE Theory</a></li>
<li class="chapter" data-level="8.4" data-path="point-estimators-finite-samples.html"><a href="point-estimators-finite-samples.html#unbiasedness"><i class="fa fa-check"></i><b>8.4</b> Unbiasedness</a></li>
<li class="chapter" data-level="8.5" data-path="point-estimators-finite-samples.html"><a href="point-estimators-finite-samples.html#minimum-variance-cramer-rao-lower-bound"><i class="fa fa-check"></i><b>8.5</b> Minimum Variance (Cramer-Rao Lower Bound)</a></li>
<li class="chapter" data-level="8.6" data-path="point-estimators-finite-samples.html"><a href="point-estimators-finite-samples.html#uniform-minimum-variance-unbiased-estimators-umvues"><i class="fa fa-check"></i><b>8.6</b> Uniform Minimum Variance Unbiased Estimators (UMVUEs)</a></li>
<li class="chapter" data-level="8.7" data-path="point-estimators-finite-samples.html"><a href="point-estimators-finite-samples.html#inferential-properties-of-exponential-families-distributions"><i class="fa fa-check"></i><b>8.7</b> Inferential Properties of Exponential Families Distributions</a>
<ul>
<li class="chapter" data-level="8.7.1" data-path="point-estimators-finite-samples.html"><a href="point-estimators-finite-samples.html#bernoulli-1"><i class="fa fa-check"></i><b>8.7.1</b> Bernoulli</a></li>
<li class="chapter" data-level="8.7.2" data-path="point-estimators-finite-samples.html"><a href="point-estimators-finite-samples.html#binomial-1"><i class="fa fa-check"></i><b>8.7.2</b> Binomial</a></li>
<li class="chapter" data-level="8.7.3" data-path="point-estimators-finite-samples.html"><a href="point-estimators-finite-samples.html#geometric"><i class="fa fa-check"></i><b>8.7.3</b> Geometric</a></li>
<li class="chapter" data-level="8.7.4" data-path="point-estimators-finite-samples.html"><a href="point-estimators-finite-samples.html#negative-binomial-1"><i class="fa fa-check"></i><b>8.7.4</b> Negative Binomial</a></li>
<li class="chapter" data-level="8.7.5" data-path="point-estimators-finite-samples.html"><a href="point-estimators-finite-samples.html#poisson-1"><i class="fa fa-check"></i><b>8.7.5</b> Poisson</a></li>
<li class="chapter" data-level="8.7.6" data-path="point-estimators-finite-samples.html"><a href="point-estimators-finite-samples.html#normal-1"><i class="fa fa-check"></i><b>8.7.6</b> Normal</a></li>
<li class="chapter" data-level="8.7.7" data-path="point-estimators-finite-samples.html"><a href="point-estimators-finite-samples.html#exponential-1"><i class="fa fa-check"></i><b>8.7.7</b> Exponential</a></li>
<li class="chapter" data-level="8.7.8" data-path="point-estimators-finite-samples.html"><a href="point-estimators-finite-samples.html#gamma-1"><i class="fa fa-check"></i><b>8.7.8</b> Gamma</a></li>
<li class="chapter" data-level="8.7.9" data-path="point-estimators-finite-samples.html"><a href="point-estimators-finite-samples.html#pareto-1"><i class="fa fa-check"></i><b>8.7.9</b> Pareto</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="point-estimators-asymptotics.html"><a href="point-estimators-asymptotics.html"><i class="fa fa-check"></i><b>9</b> Point Estimators: Asymptotics</a>
<ul>
<li class="chapter" data-level="9.1" data-path="point-estimators-asymptotics.html"><a href="point-estimators-asymptotics.html#consistency"><i class="fa fa-check"></i><b>9.1</b> Consistency</a>
<ul>
<li class="chapter" data-level="9.1.1" data-path="point-estimators-asymptotics.html"><a href="point-estimators-asymptotics.html#technique-weak-law-of-large-numbers"><i class="fa fa-check"></i><b>9.1.1</b> Technique: Weak Law of Large Numbers</a></li>
<li class="chapter" data-level="9.1.2" data-path="point-estimators-asymptotics.html"><a href="point-estimators-asymptotics.html#technique-direct-proof-via-convergence-in-probability"><i class="fa fa-check"></i><b>9.1.2</b> Technique: Direct Proof via Convergence in Probability</a></li>
<li class="chapter" data-level="9.1.3" data-path="point-estimators-asymptotics.html"><a href="point-estimators-asymptotics.html#technique-continuous-mapping-theorem."><i class="fa fa-check"></i><b>9.1.3</b> Technique: Continuous Mapping Theorem.</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="point-estimators-asymptotics.html"><a href="point-estimators-asymptotics.html#asymptotic-efficiency"><i class="fa fa-check"></i><b>9.2</b> Asymptotic Efficiency</a>
<ul>
<li class="chapter" data-level="9.2.1" data-path="point-estimators-asymptotics.html"><a href="point-estimators-asymptotics.html#central-limit-theorems"><i class="fa fa-check"></i><b>9.2.1</b> Central Limit Theorems</a></li>
<li class="chapter" data-level="9.2.2" data-path="point-estimators-asymptotics.html"><a href="point-estimators-asymptotics.html#the-delta-method"><i class="fa fa-check"></i><b>9.2.2</b> The Delta Method</a></li>
<li class="chapter" data-level="9.2.3" data-path="point-estimators-asymptotics.html"><a href="point-estimators-asymptotics.html#cramer-wold-device"><i class="fa fa-check"></i><b>9.2.3</b> Cramer-Wold Device</a></li>
<li class="chapter" data-level="9.2.4" data-path="point-estimators-asymptotics.html"><a href="point-estimators-asymptotics.html#asymptotic-distribution-in-practice."><i class="fa fa-check"></i><b>9.2.4</b> Asymptotic Distribution in Practice.</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="point-estimators-asymptotics.html"><a href="point-estimators-asymptotics.html#asymptotic-properties-of-mles"><i class="fa fa-check"></i><b>9.3</b> Asymptotic Properties of MLEs</a></li>
<li class="chapter" data-level="9.4" data-path="point-estimators-asymptotics.html"><a href="point-estimators-asymptotics.html#variance-stabilizing-transformations"><i class="fa fa-check"></i><b>9.4</b> Variance Stabilizing Transformations</a></li>
<li class="chapter" data-level="9.5" data-path="point-estimators-asymptotics.html"><a href="point-estimators-asymptotics.html#asymptotic-confidence-intervals"><i class="fa fa-check"></i><b>9.5</b> Asymptotic Confidence Intervals</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="hypothesis-tests-finite-samples.html"><a href="hypothesis-tests-finite-samples.html"><i class="fa fa-check"></i><b>10</b> Hypothesis Tests: Finite Samples</a></li>
<li class="chapter" data-level="11" data-path="hypothesis-tests-asymptotics.html"><a href="hypothesis-tests-asymptotics.html"><i class="fa fa-check"></i><b>11</b> Hypothesis Tests: Asymptotics</a>
<ul>
<li class="chapter" data-level="11.1" data-path="hypothesis-tests-asymptotics.html"><a href="hypothesis-tests-asymptotics.html#wald-test"><i class="fa fa-check"></i><b>11.1</b> Wald Test</a></li>
<li class="chapter" data-level="11.2" data-path="hypothesis-tests-asymptotics.html"><a href="hypothesis-tests-asymptotics.html#score-test"><i class="fa fa-check"></i><b>11.2</b> Score Test</a></li>
<li class="chapter" data-level="11.3" data-path="hypothesis-tests-asymptotics.html"><a href="hypothesis-tests-asymptotics.html#likelihood-ratio-test"><i class="fa fa-check"></i><b>11.3</b> Likelihood Ratio Test</a></li>
<li class="chapter" data-level="11.4" data-path="hypothesis-tests-asymptotics.html"><a href="hypothesis-tests-asymptotics.html#composite-null-hypotheses"><i class="fa fa-check"></i><b>11.4</b> Composite Null Hypotheses</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="generating-random-variables.html"><a href="generating-random-variables.html"><i class="fa fa-check"></i><b>12</b> Generating Random Variables</a></li>
<li class="chapter" data-level="13" data-path="random-processes.html"><a href="random-processes.html"><i class="fa fa-check"></i><b>13</b> Random Processes</a>
<ul>
<li class="chapter" data-level="13.1" data-path="random-processes.html"><a href="random-processes.html#poisson-processes"><i class="fa fa-check"></i><b>13.1</b> Poisson Processes</a>
<ul>
<li class="chapter" data-level="13.1.1" data-path="random-processes.html"><a href="random-processes.html#memorylessness-of-the-exponential"><i class="fa fa-check"></i><b>13.1.1</b> Memorylessness of the Exponential</a></li>
<li class="chapter" data-level="13.1.2" data-path="random-processes.html"><a href="random-processes.html#count-time-duality"><i class="fa fa-check"></i><b>13.1.2</b> Count-Time Duality</a></li>
<li class="chapter" data-level="13.1.3" data-path="random-processes.html"><a href="random-processes.html#poisson-distribution"><i class="fa fa-check"></i><b>13.1.3</b> Poisson Distribution</a></li>
<li class="chapter" data-level="13.1.4" data-path="random-processes.html"><a href="random-processes.html#exponential-distribution"><i class="fa fa-check"></i><b>13.1.4</b> Exponential Distribution</a></li>
<li class="chapter" data-level="13.1.5" data-path="random-processes.html"><a href="random-processes.html#example"><i class="fa fa-check"></i><b>13.1.5</b> Example</a></li>
</ul></li>
<li class="chapter" data-level="13.2" data-path="random-processes.html"><a href="random-processes.html#branching-processes"><i class="fa fa-check"></i><b>13.2</b> Branching Processes</a>
<ul>
<li class="chapter" data-level="13.2.1" data-path="random-processes.html"><a href="random-processes.html#random-variables"><i class="fa fa-check"></i><b>13.2.1</b> Random Variables</a></li>
<li class="chapter" data-level="13.2.2" data-path="random-processes.html"><a href="random-processes.html#probability-generating-function"><i class="fa fa-check"></i><b>13.2.2</b> Probability Generating Function</a></li>
<li class="chapter" data-level="13.2.3" data-path="random-processes.html"><a href="random-processes.html#criticality-theorem"><i class="fa fa-check"></i><b>13.2.3</b> Criticality Theorem</a></li>
<li class="chapter" data-level="13.2.4" data-path="random-processes.html"><a href="random-processes.html#example-1"><i class="fa fa-check"></i><b>13.2.4</b> Example</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Solving Statistical Problems</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="point-estimators-asymptotics" class="section level1 hasAnchor" number="9">
<h1><span class="header-section-number">Chapter 9</span> Point Estimators: Asymptotics<a href="point-estimators-asymptotics.html#point-estimators-asymptotics" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>Evaluating the asymptotic properties of point estimators.</p>
<div id="consistency" class="section level2 hasAnchor" number="9.1">
<h2><span class="header-section-number">9.1</span> Consistency<a href="point-estimators-asymptotics.html#consistency" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>An important asymptotic property of an estimator is that it converges in probability to the true value being estimated as <span class="math inline">\(n \rightarrow \infty\)</span>. This is called <strong>consistency</strong>. It is generally the most basic asymptotic property that an estimator can have.</p>
<div id="technique-weak-law-of-large-numbers" class="section level3 hasAnchor" number="9.1.1">
<h3><span class="header-section-number">9.1.1</span> Technique: Weak Law of Large Numbers<a href="point-estimators-asymptotics.html#technique-weak-law-of-large-numbers" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The most common strategy for proving consistency is to use the Weak Law of Large Numbers.</p>
<div class="theorem">
<p><span id="thm:wlln" class="theorem"><strong>Theorem 9.1  (Weak Law of Large Numbers) </strong></span><strong>iid version</strong>: If <span class="math inline">\(Z_i\)</span> are iid with finite mean, then</p>
<p><span class="math display">\[\bar{X} = \frac{1}{n}\sum_{i=1}^nX_i\overset{p}{\rightarrow}E(X_i)\]</span></p>
<p><strong>Non-iid version</strong>: Alternatively, we can drop the iid assumption; if <span class="math inline">\(X_i\)</span> have finite mean and variance, <span class="math inline">\(Cov(X_i, X_j) = 0\)</span>, and <span class="math inline">\(\lim_{n\rightarrow\infty}\sum_{i=1}^n\frac{\sigma_i^2}{n^2} = 0\)</span>, then</p>
<p><span class="math display">\[\frac{1}{n}\sum_{i=1}^nX_i - \frac{1}{n}\sum_{i=1}^nE(X_i) \overset{\mathcal{p}}{\rightarrow} 0\]</span>
This theorem guarantees that Method of Moments estimators are consistent.</p>
</div>
<p>One hint that the WLLN (or later, the Central Limit Theorem) may be employed is that the expression to be proven contains a sum. If this is true, you can multiply and divide by <span class="math inline">\(n\)</span> to form a</p>
</div>
<div id="technique-direct-proof-via-convergence-in-probability" class="section level3 hasAnchor" number="9.1.2">
<h3><span class="header-section-number">9.1.2</span> Technique: Direct Proof via Convergence in Probability<a href="point-estimators-asymptotics.html#technique-direct-proof-via-convergence-in-probability" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Sometimes, a statistic may not consistent of a sum of random variables. This often arises in the case of <a href="probability.html#order-statistics">order statistics</a>. In this case, the definition of convergence in probability must be used directly. To do this, a variety of arguments may be employed.</p>
<div class="example">
<p><span id="exm:unlabeled-div-100" class="example"><strong>Example 9.1  (How To Prove Convergence of a Sample Maximum) </strong></span>Let <span class="math inline">\(X_i \overset{iid}{\sim} U(a, b)\)</span>. We can prove that <span class="math inline">\(X_{(n)}\overset{p}{\rightarrow}b\)</span> via a <a href="probability.html#direct-manipulation">direct probability</a> argument using <a href="probability.html#disjointify">disjointification</a>. By the definition of the order statistic cdf,</p>
<p><span class="math display">\[
P(|X_{(n)} - b| &gt; \varepsilon) = P(X_{(n)} &gt; \varepsilon + b) + P(X_{(n)} &lt; b - \varepsilon) \\
= 0 + (F_{X_i}(x))^n = (\frac{b - \varepsilon - a}{b - a})^n = (1 -\frac{\varepsilon}{b - a})^n
\]</span>
The 0 arises because <span class="math inline">\(X_{(n)} &lt; b\)</span> by the definition of the Uniform. As <span class="math inline">\(\varepsilon\)</span> is taken to be small, <span class="math inline">\(\lim_{n\rightarrow \infty}(1 -\frac{\varepsilon}{b - a})^n = 0\)</span>. Therefore, since <span class="math inline">\(P(|X_{(n)} - b| &gt; \varepsilon) = 0\)</span>, we have proven <span class="math inline">\(X_{(n)}\overset{p}{\rightarrow}b\)</span></p>
</div>
</div>
<div id="technique-continuous-mapping-theorem." class="section level3 hasAnchor" number="9.1.3">
<h3><span class="header-section-number">9.1.3</span> Technique: Continuous Mapping Theorem.<a href="point-estimators-asymptotics.html#technique-continuous-mapping-theorem." class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>If you can prove that a particular estimator <span class="math inline">\(T\)</span> converges in probability to some value not equal to <span class="math inline">\(\theta\)</span>, chances are that some transformation can be applied to make it consistent by the <a href="#cmt">Continuous Mapping Theorem</a>. In fact, any continuous function of a consistent estimator is also consistent; that is <span class="math inline">\(T_n(X) \overset{p}{\rightarrow} \theta\)</span>, then <span class="math inline">\(g(T_n(X)) \overset{p}{\rightarrow} g(\theta)\)</span></p>
<p>For example, if <span class="math inline">\(T_n(X) \overset{p}{\rightarrow} \frac{\theta + \zeta_1}{\zeta_2}\)</span>, then by the Continuous Mapping Theorem, <span class="math inline">\(W_n(X) = \zeta_2T(X) - \zeta_1\overset{p}{\rightarrow}\theta\)</span>. As such, one strategy to finding a consistent estimator is to start with a sample mean, which is consistent by the WLLN, then transform it to obtain the desired result.</p>
</div>
</div>
<div id="asymptotic-efficiency" class="section level2 hasAnchor" number="9.2">
<h2><span class="header-section-number">9.2</span> Asymptotic Efficiency<a href="point-estimators-asymptotics.html#asymptotic-efficiency" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In addition to checking that an estimator converges to the correct value, we are also often concerned with the estimator’s <em>variance</em> as <span class="math inline">\(n \rightarrow \infty\)</span>. Often, estimators converge asymptotically to a normal distribution. If an estimator <span class="math inline">\(T_n(X)\)</span> has the property <span class="math inline">\(k_n(T_n(X) - \tau(\theta)) \overset{\mathcal{D}}{\rightarrow} N(0,\sigma^2)\)</span>, then <span class="math inline">\(\sigma^2\)</span> is called the <em>asymptotic variance</em> <span class="citation">(<a href="#ref-Casella1990">Casella and Berger 1990</a>)</span>.</p>
<p>Furthermore, <span class="math inline">\(T_n(X)\)</span> is <strong>asymptotically efficient* if <span class="math inline">\(\sigma^2\)</span> achieves the Cramer-Rao Lower Bound. If <span class="math inline">\(T_n(X)\)</span> is one-dimensional, then the </strong>asymptotic efficiency** of <span class="math inline">\(T_n(X)\)</span> can be computed as the ratio</p>
<p><span class="math display">\[AE(\theta, T_n) = \frac{(\tau&#39;(\theta))^2}{\mathcal{I}(\theta)\sigma^2}\]</span>
If <span class="math inline">\(T_n(X)\)</span> is <span class="math inline">\(k\)</span>-dimensional, let <span class="math inline">\(d = \begin{bmatrix}\frac{\partial}{\partial\theta_1}\tau(\theta),...,\frac{\partial}{\partial\theta_k}\tau(\theta)\end{bmatrix}\)</span>. Then, the asymptotic efficiency is</p>
<p><span class="math display">\[AE(\theta, T_n) = \frac{d&#39;\mathcal{I}(\theta)^{-1}d}{\sigma^2}\]</span></p>
<p>We can also compare estimators via their <strong>asymptotic relative efficiency (ARE)</strong>, which for estimators <span class="math inline">\(S_n\)</span> and <span class="math inline">\(T_n\)</span> is</p>
<p><span class="math display">\[ARE(\theta, S_n, T_n) = \frac{\sigma_T^2}{\sigma_S^2}\]</span></p>
<p>Finally, <span class="math inline">\(T_n(X)\)</span> is said to be <em>asymptotically normal</em> if <span class="math inline">\(\sqrt{n}(\hat{\theta}_n - \theta_0) \overset{\mathcal{D}}{\rightarrow} N(0, I(\theta_0))\)</span>.</p>
<p>Let’s explore first how we might prove that an estimator converges asymptotically to a normal distribution to show this directly.</p>
<div id="central-limit-theorems" class="section level3 hasAnchor" number="9.2.1">
<h3><span class="header-section-number">9.2.1</span> Central Limit Theorems<a href="point-estimators-asymptotics.html#central-limit-theorems" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>These theorems are used to show asymptotic normality. There are many different types; let us focus on the three most common.</p>
<div class="theorem">
<p><span id="thm:clt" class="theorem"><strong>Theorem 9.2  (Central Limit Theorem (iid)) </strong></span>If <span class="math inline">\(X_i\)</span> are iid with finite first and second moments (<span class="math inline">\(E(X_i), Var(X_i) &lt; \infty\)</span>) then</p>
<p><span class="math display">\[\sqrt{n}\Big(\frac{1}{n}\sum_{i=1}^nX_i - E(X_i)\Big) \overset{\mathcal{D}}{\rightarrow} N(0,Var(X_i))\]</span></p>
</div>
<div class="theorem">
<p><span id="thm:clt" class="theorem"><strong>Theorem 9.2  (Lyapunov's Central Limit Theorem (non-iid)) </strong></span>Consider <span class="math inline">\(X_i\)</span> that are independent with finite first and second moments (<span class="math inline">\(E(X_i), Var(X_i) &lt; \infty\)</span>). Let <span class="math inline">\(S_n^2 = \sum_{i=1}^n Var(X_i)\)</span>. Then, for some <span class="math inline">\(\delta &gt; 0\)</span>, if the condition</p>
<p><span class="math display">\[\lim_{n\rightarrow \infty} \frac{1}{S_n^{2 + \delta}}\sum_{i=1}^n E(|X_i - E(X_i)|^{2 + \delta}) = 0\]</span></p>
<p>holds, then we know</p>
<p><span class="math display">\[\frac{1}{S_n}\sum_{i=1}^n (X_i - E(X_i)) \overset{\mathcal{D}}{\rightarrow} N(0,1)\]</span></p>
</div>
<p>In practice, to prove that Lyapunov’s CLT holds true, we typically take <span class="math inline">\(\delta = 1\)</span> and compute the third moments contained in the Lyapunov condition. Alternatively, we can use Lindeberg’s CLT for non-iid data:</p>
<div class="theorem">
<p><span id="thm:clt" class="theorem"><strong>Theorem 9.2  (Lindeberg's Central Limit Theorem (non-iid)) </strong></span>Like the Lyapunov CLT, consider <span class="math inline">\(X_i\)</span> that are independent with finite first and second moments (<span class="math inline">\(E(X_i), Var(X_i) &lt; \infty\)</span>). Let <span class="math inline">\(S_n^2 = \sum_{i=1}^n Var(X_i)\)</span>. Then if</p>
<p><span class="math display">\[\lim_{n\rightarrow \infty} \frac{1}{S_n^{2}}\sum_{i=1}^n E((X_i - E(X_i))^{2})\cdot I(|X_i - E(X_i) &gt; \varepsilon S_n) = 0\]</span>
holds, then we know</p>
<p><span class="math display">\[\frac{1}{S_n}\sum_{i=1}^n (X_i - E(X_i)) \overset{\mathcal{D}}{\rightarrow} N(0,1)\]</span></p>
</div>
</div>
<div id="the-delta-method" class="section level3 hasAnchor" number="9.2.2">
<h3><span class="header-section-number">9.2.2</span> The Delta Method<a href="point-estimators-asymptotics.html#the-delta-method" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>What if our estimator is not a sample mean? In this case, if it is a function of a sample mean, we can still use the Delta Method to prove that it converges asymptotically to either a Normal or a <span class="math inline">\(\chi^2\)</span> distribution.</p>
<div class="theorem">
<p><span id="thm:delta-method" class="theorem"><strong>Theorem 9.3  (Delta Method) </strong></span>If <span class="math inline">\(\sqrt{n}(\hat{\theta}_n - \theta) \overset{\mathcal{D}}{\rightarrow} N(0,\sigma^2)\)</span>, then for a continuous <span class="math inline">\(g\)</span> with continuous nonzero derivative in an interval containing <span class="math inline">\(\theta\)</span>…</p>
<ul>
<li><span class="math inline">\(\sqrt{n}\Big(g(\hat{\theta}_n) - g(\theta)\Big) \overset{\mathcal{D}}{\rightarrow} N\Big(0, \sigma^2\cdot (\frac{d}{d\theta}g(\theta))^2\Big)\)</span> (First-Order Delta Method)</li>
<li><span class="math inline">\(n\Big(g(\hat{\theta}_n) - g(\theta)\Big) \overset{\mathcal{D}}{\rightarrow} \frac{1}{2}\sigma^2\cdot g&#39;&#39;(\theta)\cdot\chi^2(1)\)</span> (Second-Order Delta Method)</li>
</ul>
</div>
<p>Generally, the Second-Order Delta Method is required if <span class="math inline">\(g&#39;(\theta) = 0\)</span>.</p>
<div class="theorem">
<p><span id="thm:multivariate-delta-method" class="theorem"><strong>Theorem 9.4  (Multivariate Delta Method) </strong></span>If <span class="math inline">\(\sqrt{n}(\hat{\theta}_n - \theta) \overset{\mathcal{D}}{\rightarrow} N(0,V)\)</span>, where <span class="math inline">\(V\)</span> is a <span class="math inline">\(k \times k\)</span> matrix, then for a continuous real-valued function <span class="math inline">\(g(x)\)</span> of <span class="math inline">\(k\)</span> variables with continuous nonzero first partial derivatives,</p>
<p><span class="math display">\[\sqrt{n}\Big(g(\hat{\theta}_n) - g(\theta)\Big) \overset{\mathcal{D}}{\rightarrow} MVN_k\Big(0, u&#39;Vu\Big)\]</span></p>
<p>where <span class="math inline">\(u = \begin{bmatrix}\frac{\partial}{\partial \theta_1}g(\theta),...,\frac{\partial}{\partial \theta_k}g(\theta)\end{bmatrix}\)</span></p>
</div>
</div>
<div id="cramer-wold-device" class="section level3 hasAnchor" number="9.2.3">
<h3><span class="header-section-number">9.2.3</span> Cramer-Wold Device<a href="point-estimators-asymptotics.html#cramer-wold-device" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>One final technique for proving convergence in distribution is to use the Cramer-Wold Device.</p>
<div class="theorem">
<p><span id="thm:cramer-wold-device" class="theorem"><strong>Theorem 9.5  (Cramer-Wold Device) </strong></span>For a sequence of random vectors <span class="math inline">\(X_n\)</span>, a random vector <span class="math inline">\(X\)</span>, and a vector <span class="math inline">\(a\)</span> of constants,</p>
<p><span class="math display">\[X_n \overset{\mathcal{D}}{\rightarrow} X \iff a&#39;X_n \overset{\mathcal{D}}{\rightarrow}a&#39;X, \forall a\]</span>
In other words, we can show convergence in distribution of a random vector by showing that every linear combination of that random vector converges in distribution to the same linear combination of <span class="math inline">\(X\)</span>.</p>
</div>
<p>The Cramer-Wold Device allows us to convert a problem involving convergence of random vectors into a problem involving convergence of a single random variable.</p>
<p>::: {.example name=“Using the Cramer-Wold Device}
Suppose <span class="math inline">\(X_{1n} \overset{\mathcal{D}}{\rightarrow} X_1 \sim N(\mu_1, \sigma^2_1)\)</span> and <span class="math inline">\(X_{2n} \overset{\mathcal{D}}{\rightarrow} X_2 \sim N(\mu_2, \sigma^2_2)\)</span>. Then, by the Cramer-Wold Device, <span class="math inline">\((X_{1n}, X_{2n}) \overset{\mathcal{D}}{\rightarrow} MVN(\begin{bmatrix}\mu_1 \\ \mu_2\end{bmatrix}, \Sigma)\)</span> where</p>
<p><span class="math display">\[\Sigma = \begin{bmatrix}\sigma_1^2 &amp; \rho\\\rho &amp; \sigma_2^2\end{bmatrix}\]</span>
where <span class="math inline">\(\rho = Cov(X_1, X_2)\)</span>.</p>
<p><em>Proof</em>: To use the Cramer-Wold Device, we need to hypothesize a distribution to which the vector <span class="math inline">\((X_{1n}, X_{2n})\)</span> will converge. Fortunately, we’re given one - <span class="math inline">\(MVN(\begin{bmatrix}\mu_1 \\ \mu_2\end{bmatrix}, \Sigma)\)</span>. Now, we show what happens when we apply an arbitrary linear transformation.</p>
<p>By <a href="known-distributions.html#normal">additivity of the normal</a>,</p>
<p><span class="math display">\[
a_1X_1 + a_2X_2 = N(a_1\mu_1 + a_2\mu_2, a_1^2\sigma_1^2 + a_2^2\sigma_2^2 + 2\rho a_1a_2\sigma_1\sigma_2)\\
= \begin{bmatrix}a_1 \\ a_2\end{bmatrix}\cdot MVN(\begin{bmatrix}\mu_1 \\ \mu_2\end{bmatrix}, \Sigma)
\]</span></p>
<p>By the Cramer-Wold Device, this implies that <span class="math inline">\((X_{1n}, X_{2n}) \overset{\mathcal{D}}{\rightarrow} MVN(\begin{bmatrix}\mu_1 \\ \mu_2\end{bmatrix}, \Sigma)\)</span></p>
<p>:::</p>
<p>The above example can be generalized to a <span class="math inline">\(k\)</span>-vector of random variables <span class="math inline">\(X_i\)</span> which converge in distribution to a normal.</p>
</div>
<div id="asymptotic-distribution-in-practice." class="section level3 hasAnchor" number="9.2.4">
<h3><span class="header-section-number">9.2.4</span> Asymptotic Distribution in Practice.<a href="point-estimators-asymptotics.html#asymptotic-distribution-in-practice." class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Statistical problems are often more complicated than simply applying the CLT or Delta Method. Often, we may need to consider the convergences of other random variables. To do this, we can combine the CLT/Delta Method with Slutsky’s Theorem or the Continuous Mapping Theorem to prove desired results. Or, we may need to employ a more direct proof. Here are some examples.</p>
</div>
</div>
<div id="asymptotic-properties-of-mles" class="section level2 hasAnchor" number="9.3">
<h2><span class="header-section-number">9.3</span> Asymptotic Properties of MLEs<a href="point-estimators-asymptotics.html#asymptotic-properties-of-mles" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>While the CLT and Delta Method are extremely useful, if you are working with MLEs, it can often be faster to rely on their known properties. For <span class="math inline">\(X_i\)</span> satisfying a certain set of <a href="https://myweb.uiowa.edu/pbreheny/7110/wiki/regularity-conditions.html">regularity conditions</a> (where? better link), the MLE $_n has the two import properties discussed above:</p>
<ul>
<li><strong>Consistency</strong>: <span class="math inline">\(\hat{\theta}_n \overset{p}{\rightarrow} \theta\)</span></li>
<li><strong>Asymptotic Efficiency</strong>: <span class="math inline">\(\sqrt{n}(\hat{\theta}_n - \theta) \overset{\mathcal{D}}{\rightarrow} N(0, \mathcal{I}(\theta_0)^{-1})\)</span>, the <a href="#crlb">Cramer-Rao Lower Bound</a></li>
</ul>
<p>This holds true even for multi-parameter MLEs. Note that the regularity conditions are met by the MLE of all exponential families for which <span class="math inline">\(\nu \in \Theta \subset \mathbb{R}\)</span> is an open set, which can be useful in problem-solving.</p>
<p>Estimating <span class="math inline">\(\mathcal{I}(\theta_0)\)</span> can be performed in two ways: (is this correct?)</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(-\frac{1}{n} \frac{\partial^2}{\partial\theta^2}\ell(\hat{\theta}_n | X_n) \overset{p}{\rightarrow} \mathcal{I}(\theta_0)\)</span></li>
<li><span class="math inline">\(\frac{1}{n} I_n(\hat{\theta}_n) \overset{p}{\rightarrow} \mathcal{I}(\theta_0)\)</span></li>
</ol>
</div>
<div id="variance-stabilizing-transformations" class="section level2 hasAnchor" number="9.4">
<h2><span class="header-section-number">9.4</span> Variance Stabilizing Transformations<a href="point-estimators-asymptotics.html#variance-stabilizing-transformations" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Sometimes, the asymptotic variance of a distribution may depend on the value of a particular parameter. This may be undesirable because the parameter might be unknown. In this case, we can perform a <strong>variance-stabilizing transformation</strong> to remove the variance.</p>
<p>This is done by setting <span class="math inline">\(f&#39;(\theta) = \frac{c}{\tau(\theta)}\)</span>. Then, for <span class="math inline">\(T_n(X)\)</span> such that <span class="math inline">\(\sqrt{n}(T_n - \theta) \overset{\mathcal{D}}{\rightarrow} N(0, \tau^2(\theta))\)</span>, by the Delta Method we get</p>
<p><span class="math display">\[\sqrt{n}(f(T_n) - f(\theta)) \overset{\mathcal{D}}{\rightarrow} N(0, f&#39;(\theta)^2 \cdot\tau^2(\theta)) = N(0, c)\]</span></p>
</div>
<div id="asymptotic-confidence-intervals" class="section level2 hasAnchor" number="9.5">
<h2><span class="header-section-number">9.5</span> Asymptotic Confidence Intervals<a href="point-estimators-asymptotics.html#asymptotic-confidence-intervals" class="anchor-section" aria-label="Anchor link to header"></a></h2>

</div>
</div>
<h3>References<a href="references.html#references" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-Casella1990" class="csl-entry">
Casella, G C, and Roger L Berger. 1990. <em>Statistical Inference</em>. 2nd ed. The Wadsworth &amp; Brooks/Cole Statistics/Probability Series. Florence, KY: Brooks/Cole.
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="point-estimators-finite-samples.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="hypothesis-tests-finite-samples.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown-demo/edit/master/09-point-estimators-asymptotics.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["Solving-Statistical-Problems.pdf", "Solving-Statistical-Problems.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
