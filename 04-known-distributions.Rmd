# Known Distributions {#known-distributions}

This will cover proofs and useful properties of commonly used distributions, as well as location-scale and exponential families.

## Families of Distributions

Many so-called "distributions" are actually *families* of distributions, meaning that their pdf involves one or more parameters. That is, their pdfs represent a [family of curves](https://en.wikipedia.org/wiki/Family_of_curves), a **set** of pdfs with variable parameters.

For example, the $\text{Normal}(\mu, \sigma^2)$ distribution contains two parameters, $\mu$ (the mean), and $\sigma^2$ (the variance). These are also examples of two types of parameters with special properties - called *location* and *scale* parameters, respectively - that can be used to simply calculations.

## Location and Scale Families (#location-scale)

### Location Families

::: {.definition name="Location Family"}
Let $Z \sim f_Z(z)$. Given a constant **location parameter** $b$, $X$ is a location family if $X \sim f_Z(z - b)$ or if $X = Z + b$.
:::

The two above definitions are equivalent because if $X = Z + b$, then $P(X < z) = P(Z + b < z) = P(Z < z - b)$, so the cdf of $Z$ is $F_Z(z - b)$ and therefore $X \sim f_Z(z - b)$ (note this makes use of a [direct probability argument](#probability))

### Scale Families

::: {.definition name="Scale Family"}
Let $Z \sim f_Z(Z)$. Given a constant **scale parameter** $a$, $X$ is a scale family if...  

- $X \sim \frac{1}{a}f_Z(\frac{z}{a})$  
- $X \sim F_Z(\frac{z}{a})$   
- or $X = aZ$  
:::

All of the above definitions are equivalent because if $X = aZ$, then $P(X < z)$ = $P(aZ < z) = P(Z < \frac{z}{a} = F_Z(z)$. Also, $f_X(x) = \frac{d}{dx}F_X(x) = \frac{d}{dx}F_Z(\frac{z}{a}) = \frac{1}{a}f_Z(\frac{z}{a})$

### Properties of Location-Scale Families

We can compute moments by using general properties of expectation (see Moments)

- $E(X) = aE(Z) + b$, by linearity of expectation. If the support of $Z$ includes $0$, then we typically define $Z$ such that $E(Z) = 0$ so that $E(X) = b$. 

- $Var(X) = a^2Var(Z)$, since $Var(Z + b) = Var(Z)$. We typically define $Z$ such that $Var(Z) = 1$, so that $Var(X) = a^2Var(Z) = a^2$. An example of this is the standard normal.

- $\mathcal{M}_X(t) = e^{tb} \mathcal{M}_Z(at)$

It may seem like the sum of a scale family should also follow the same family - indeed, this is true for a number of distributions include the Normal, Poisson, and Gamma. However, it is not true always. For instance, $X_i \sim \text{Uniform}(0,a)$ is a scale family, but $X_1 + X_2$ does not follow a uniform distribution:

```{r}
X1 = runif(1000, 0, 1)
X2 = runif(1000, 0, 1)
hist(X1 + X2)
```

## Exponential Families {#exponential-family}

::: {.definition name="Exponential Family"}
$X \sim f_X(x|\theta)$ is an exponential family if its pdf can be written in the form

$$f_X(x|\theta) = h(x)c(\theta)\exp\Big(\sum_{i=1}^k w_i(\theta)t_i(x)\Big)$$
:::

How do we prove that a pdf *can* be written in the above form? Often, the easiest way is to use a simple trick to get the necessary $\exp$ function: $f(x) = \exp(\log(f(x)))$. Then, we algebraically manipulate to obtain this form.

How do we prove that a pdf *cannot* be written in the above form?

FORTHCOMING

### Properties

Exponential families have a number of incredibly useful properties:

- [Leibniz's rule](#leibniz-rule) holds, meaning that the [Cramer-Rao Lower Bound](#point-estimators-finite-samples) provides a lower bound on the variance of estimators. 
- Among most common families, **only** exponential families admit sufficient statistics with dimension bounded in $n$. This is proven by the [Pitman-Koopman-Darmois theorem](http://yaroslavvb.com/papers/koopman-on.pdf) for families with smooth, nowhere-vanishing pdfs whose domain does not depend on the parameter being estimated.
- If $X$ is an exponential family, $$T(X) = \Big(\sum_{i=1}^n t_1(x), ..., \sum_{i=1}^n t_k(x)\Big)$$ is a minimal [sufficient statistic](#statistics).
- Furthermore, if $\{w_1(\theta),...,w_k(\theta)\}$ contains an open set, then $T(X)$ is a [complete sufficient statistic](#statistics), which we can use to compute an UMVUE.
- The Method of Moments (MOM) estimator is equal to the Maximum Likelihood Estimator (MLE)
- The regularity conditions required for [consistency and asymptotic normality of the MLE](#point-estimators-asymptotics) are guaranteed to hold.
- The family must have a Monotone Likelihood Ratio, meaning that the [Karlin-Rubin Theorem may be employed to construct an UMP test](#hypothesis-tests-finite-samples).

### Natural Exponential Families

::: {.definition name="Natural Exponential Family"}
$X \sim f_X(x|\theta)$ is a natural exponential family if its pdf can be written in the form

$$f_X(x|\theta) = h(x)c^*(\boldsymbol{\eta})\exp\Big(\sum_{i=1}^k \eta_i t_i(x)\Big)$$
:::




## Known Exponential Families

Many common distributions follow exponential families. As you will come to find, virtually all of them arise in order to model variations on a common idea: the Bernoulli trial. Let's discuss this distribution, and the situations in which it arises.

### Bernoulli

The Bernoulli distribution, represented mathematically as $\text{Bernoulli}(p)$, describes the outcome of a random variable $X$ that takes only two possible values, 0 and 1. Such an event is often termed a "Bernoulli trial". 


|               Description               |    Parameters     |     Support      |       pmf        |
|:----------------:|:----------------:|:----------------:|:----------------:|
| Any random variables whose value can be either 0 or 1 | $0 \leq p \leq 1$ | $x \in \{0, 1\}$ | $p^x(1-p)^{1-x}$ |

The Bernoulli distribution occurs very commonly because many situations can be described in terms of 0 or 1 outcomes. For example, all indicator functions $I(A)$ of random variables, where $A$ is a statement about the random variable (for instance, $A = \{x: x > 1\}$) are Bernoulli random variables with $p = P(A)$. 


- The Bernoulli is a special case of the Binomial distribution: $\text{Bernoulli}(p) = \text{Binomial}(1, p)$
- As we discuss regarding the Binomial distribution, the Bernoulli has an additive property: $\sum_{i=1}^n \sim \text{Binomial}(n, p)$. This can be proven by the [additivity technique](#additivity) discussed imminently.
- If $X\sim \text{Bernoulli}$ then $E(X) = P(Y = 1)$, a fact which is often useful for [computing moments](#moments) as well as [finding UMVUEs](#point-estimators-finite-samples).

### Binomial

What happens when we repeat a Bernoulli trial many times and count how many 1's occur? The Binomial distribution is represented mathematically as $\text{Bernoulli}(n, p)$. It describes the *number of successes* in a series of Bernoulli trials. 


|               Description               |    Parameters     |     Support      |       pmf        |
|:----------------:|:----------------:|:----------------:|:----------------:|
| The number of times an event was successful out of $n$ attempts  | $0 \leq p \leq 1$, $n \in \mathbb{N}$ | $x \in \mathbb{N}$ | ${n\choose x}p^x(1-p)^{n-x}$ |



::: {.example #additivity name="Proving Additive Properties of Distributions"}  

The Binomial distribution is **additive**: if $X \sim \text{Binomial}(n,p)$ and $X \sim \text{Binomial}(m,p)$, then $X + Y \sim \text{Binomial}(m + n, p)$. One can prove the additivity of any distribution, not just the Binomial, by relying on the convolution property of mgfs: $\mathcal{M}_{X + Y}(t) = \mathcal{M}_X(t)\cdot\mathcal{M}_Y(t)$. 

Here's an example with the Binomial: if $X \sim \text{Binomial}(n, p)$ and $Y \sim \text{Binomial}(m, p)$, then 

$$\mathcal{M}_{X + Y}(t) = ((1-p) + pe^t)^n\cdot ((1-p) + pe^t)^m = ((1-p) + pe^t)^{mn}$$
which we can recognize as the mgf of a $\text{Binomial}(n + m, p)$ distribution. Since, like the cdf and the pdf, the mgf fully characterizes a probability distribution, we've proven the additive property mentioned above.

Here's another example with the Binomial, this time generalizing it to an arbitrary summation: If $X_i \overset{iid}{\sim} \text{Binomial}(m, p)$, and $Y = \sum_{i=1}^nX_i$, then 

$$
\mathcal{M}_Y(t) = \prod_{i=1}^n\mathcal{M}_{X_i}(t) = (\mathcal{M}_{X_i}(t))^n \\
= (((1-p) + pe^t)^m)^n = ((1-p) + pe^t)^{mn}
$$ 

proving the generalized additive property that if $X_i \sim \text{Binomial}(m, p)$, then $\sum_{i=1}^n X_i \sim \text{Binomial}(nm, p)$.

:::

### Geometric {#geometric-distribution}

Suppose instead of counting the number of successes, we wish to count the number of attempts until a single success occurs? The Geometric distribution is represented mathematically as $\text{Geo}(p)$. It describes the **number of trials before a success occurs** in a series of Bernoulli trials.

Note that the parametrization below does not include the final success in the number of trials, but alternatives exist in which it may.

|               Description               |    Parameters     |     Support      |       pmf        |
|:----------------:|:----------------:|:----------------:|:----------------:|
| The number of Bernoulli trials attempted before a success occurs  | $0 < p \leq 1$ | $x \in \mathbb{N}$ | $p(1-p)^{x}$ |

- The Geometric is a special case of the Negative Binomial: $\text{Geo}(p) = \text{NegBin}(1, p)$
- Just like the Bernoulli, the Geometric has an additive property: $\sum_{i=1}^n X_i \sim \text{NegBin}(n, p)$, proven via the same [addivity technique](#additivity) discussed previously.

### Negative Binomial

The Negative Binomial distribution generalizes the Geometric distribution to instead represent the number of Bernoulli trials until $r$ successes have occurred. It is represented mathematically as $\text{NegBin}(r, p)$. 

|               Description               |    Parameters     |     Support      |       pmf        |
|:----------------:|:----------------:|:----------------:|:----------------:|
| The number of Bernoulli trials attempted before a success occurs  | $0 < p \leq 1$, $r \in \mathbb{N}$ | $x \in \mathbb{N}$ | ${x+r-1\choose x}p^r(1-p)^{x}$ |



- The Negative Binomial is additive: If $X_i \sim \text{NegBin(r, p)}$, then $\sum_{i=1}^n X_i \sim \text{NegBin(nr, p)}$

### Poisson

The Poisson distribution describes one possible behavior of a **count** random variable. It describes the probability that a certain number of events occur within a fixed interval, such as a time period, distance or area. It is mathematically represented as $\text{Poisson}(\lambda)$. 


|               Description               |    Parameters     |     Support      |       pmf        |
|:----------------:|:----------------:|:----------------:|:----------------:|
| The number of events occurring in a fixed interval | $\lambda \in (0, \infty)$ | $x \in \mathbb{N}_0$ | $\frac{1}{x!}\lambda^xe^{-\lambda}$ |


- Like the Binomial, the Poisson is formulated by counting the number of successes within a set of Bernoulli trials. The distribution describes the asymptotic behavior of the Binomial distribution as $n \rightarrow \infty$ and $np \rightarrow \lambda$, a fixed *rate* parameter.
- The Poisson is **additive**. If $X_i \sim \text{Poisson}(\lambda)$, then $\sum_{i=1}^n X_i \sim \text{Poisson}(n\lambda)$

### Normal

Often denoted $N(\mu, \sigma^2)$, the Normal distribution

|               Description               |    Parameters     |     Support      |       pdf        |
|:----------------:|:----------------:|:----------------:|:----------------:|
| Describes the asymptotic behavior of sample means and many distributions | $\mu \in \mathbb{R}$, $\sigma^2 \in (0, \infty)$ | $x \in \mathbb{R}$ | $\frac{1}{\sqrt{2\pi\sigma^2}}\exp\Big(-\frac{1}{2}\frac{(x-\mu)^2}{\sigma^2}\Big)$ |


### Exponential

The Poisson process that models the number of events occuring in an interval also gives rise to another distribution: the Exponential. This distribution models the size of the interval (time, distance, etc.) between events.

The Exponential is described mathematically as $\text{Exp}(\lambda)$. It can be parametrized in two ways: with $\lambda$ as a rate parameter, describing how often events occur; or with $\lambda$ as a scale parameter (yes, the same scale parameter discussed in [Location Scale Families](#location-scale)) that is the inverse of the rate.

Note that the parametrization below describes the distribution in terms of the scale parameter. The scale parameter version simply replaces $\lambda$ with $\frac{1}{\lambda}$.


|               Description               |    Parameters     |     Support      |       pdf        |
|:----------------:|:----------------:|:----------------:|:----------------:|
| Models the size of the interval between events in a Poisson process | $\lambda \in (0, \infty)$ | $x \in (0, \infty)$ | $\lambda e^{-\lambda x}$ |


- The Exponential is a special case of the Gamma distribution: $\text{Exp}(\lambda) = \text{Gamma}(1, \lambda)$
- By extension, the exponential has an additive property: If $X_i\sim Exp(\lambda) $, then $\sum_{i=1}^n X_i \sim \text{Gamma}(n, \lambda)$.

### Gamma

Similar to how the Negative Binomial generalizes the Geometric to multiple successes, the Gamma generalizes the Exponential to multiple events. The Gamma is useful for modeling random variables that are known to be greater than 0. 

It is represented mathematically as $\text{Gamma}(k, \lambda)$, where $k$ is a **shape** parameter and $\lambda$ is a [scale](#location-scale) parameter. It is called "Gamma" because it involves the [gamma function](#gamma-function).


|               Description               |    Parameters     |     Support      |       pdf        |
|:----------------:|:----------------:|:----------------:|:----------------:|
| Generalization of the exponential distribution | $k \in (0, \infty)$, $\lambda \in (0, \infty)$ | $x \in (0, \infty)$ | $\frac{1}{\Gamma(k)\lambda^k}x^{k-1}\exp(-\frac{x}{\theta})$ |


- The Gamma is **additive**: If $X_i\sim \text{Gamma}(k, \lambda) $, then $\sum_{i=1}^n X_i \sim \text{Gamma}(nk, \lambda)$.

### Beta

The Beta distribution models proportions. It is mathematically denoted $\text{Beta}(\alpha, \beta)$ where $\alpha$ and $\beta$ are two shape parameters. It is known as "Beta" because its pdf contains the beta function: $\Beta(\alpha, \beta) = \frac{\Gamma(\alpha)\Gamma(\beta)}{\Gamma(\alpha + \beta)}$


|               Description               |    Parameters     |     Support      |       pdf        |
|:----------------:|:----------------:|:----------------:|:----------------:|
| Model of a proportion | $\lambda \in (0, \infty)$ | $x \in (0, 1)$ | $\frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha)\Gamma(\beta)}x^{\alpha-1}(1-x)^{\beta-1}$ |



### Chi-squared

The Chi-squared distribution describes the distribution of the sum of squared stadnard Normal ($N(0,1)$) random variables.


|               Description               |    Parameters     |     Support      |       pdf        |
|:----------------:|:----------------:|:----------------:|:----------------:|
| Squared standard normals | $\nu \in \mathbb{N}$ | $x \in (0, \infty)$ | $\frac{1}{\Gamma(\nu/2)2^{\nu / 2}}x^{\nu/2 - 1}\exp(-\nu/2)$ |


- If $X_i \sim N(0,1)$, then $\sum_{i=1}^nX_i^2\sim \chi^2(n)$


## Exponential families with certain parameters fixed

Some families are exponential, but only when one or more parameters are fixed.

### Weibull

#### Definition


|               Description               |    Parameters     |     Support      |       pdf        |
|:----------------:|:----------------:|:----------------:|:----------------:|
|  | $\lambda \in (0, \infty)$, $k \in (0, \infty)$ | $x \in [0, \infty)$ | $\frac{k}{\lambda}\Big(\frac{x}{\lambda}\Big)^{k-1}\exp(-(x/\lambda)^k)$ |


### Pareto

$\alpha \in (0, \infty)$ is a shape parameter, while $x_m$ is a scale parameter.


|               Description               |    Parameters     |     Support      |       pdf        |
|:----------------:|:----------------:|:----------------:|:----------------:|
|  | $\alpha \in (0, \infty)$, $k \in (0, \infty)$ | $x \in [0, \infty)$ | $\frac{\alpha x_m^\alpha}{x^{\alpha+1}}$ |


## Non-exponential families

The following families are not exponential, but still commonly arise.  

### Uniform


|               Description               |    Parameters     |     Support      |       pdf        |
|:----------------:|:----------------:|:----------------:|:----------------:|
|  | $-\infty < a < b < \infty$ | $x \in [a, b]$ | $\frac{1}{b-a}$ |


### Cauchy


|               Description               |    Parameters     |     Support      |       pdf        |
|:----------------:|:----------------:|:----------------:|:----------------:|
|  | $x_0 \in \mathbb{R}$, $\gamma \in (0, \infty)$ | $x \in \mathbb{R}$ | $\frac{1}{\pi \gamma\Big(1 + \Big(\frac{x - x_0}{\gamma}\Big)^2\Big)}$ |


### t-distribution


|               Description               |    Parameters     |     Support      |       pdf        |
|:----------------:|:----------------:|:----------------:|:----------------:|
|  | $x_0 \in \mathbb{R}$, $\gamma \in (0, \infty)$ | $x \in \mathbb{R}$ | $\frac{1}{\pi \gamma\Big(1 + \Big(\frac{x - x_0}{\gamma}\Big)^2\Big)}$ |

### F-distribution


|               Description               |    Parameters     |     Support      |       pdf        |
|:----------------:|:----------------:|:----------------:|:----------------:|
|  | $n \in \mathbb{N}$, $m \in \mathbb{N}$ | $x \in (0, \infty)$ | $\sqrt{\frac{(nx)^nm^m}{(nx + m)^{n + m}}}\frac{\Gamma(n + m)}{x\Gamma(n)\Gamma(m)}$ |


### Hypergeometric


|               Description               |    Parameters     |     Support      |       pmf        |
|:----------------:|:----------------:|:----------------:|:----------------:|
|  | $N \in \mathbb{N}_0$, $K \in \{0, 1, ..., N\}$, $n \in \{0,1,...,N\}$ | $x \in \{\max(0, n+K-N),..., \min(n,K)\}$ | $\frac{{K\choose x}{N - K \choose n - x -1}}{N \choose n}$ |


