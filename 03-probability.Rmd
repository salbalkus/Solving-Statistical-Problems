# Probability {#probability}

## Basic Axioms

How do we reason about events that may or may not occur? The field of *probability* provides the mathematical foundations for reasoning under uncertainty.

The foundation of probability is set theory. When you think about "something that could happen", you can conceptualize that "something" as an experiment. We can define all of the details of that experiment in terms of sets:

- Any given outcome of the experiment we represent as a set - call it $A$.
- We represent the set of all possible that could have occurred - the *sampling space* - as $\Omega$. Hence, $A \subseteq \Omega$

Now suppose we could repeat this experiment infinitely. The *probability* of an event A is defined as the proportion of experiments in which that event will occurs as the number of experiments increases towards infinity. Mathematically, we represent probability as a function: $P(A)$. Then,

$$P(A) = \lim_{n\rightarrow\infty}\frac{\text{# of times A is drawn}}{n}$$

In the simplest case, we can compute this proportion directly as $P(A) = \frac{\text{# of outcomes in }A}{\text{# of outcomes in }\Omega}$ - a typical grade-school exercise. Furthermore, from this definition, three properties (called Kolmogorov's axioms) arise:

:::{.definition name="Kolmogorov's Axioms of Probability"}

For all probability functions $P(\cdot)$ defined on a sampling space $\Omega$:

1. $0 \leq P(A) \leq 1$ for all $A\subseteq \Omega$
2. $P(\Omega) = 1$.
3. If $A_1, A_2, ..., $ are pairwise disjoint, then 

$$P(\cup_{i=1}^\infty A_i) = \sum_{i=1}^\infty P(A_i)$$

:::

Two sets $A$ and $B$ are *pairwise disjoint* if $A$ does not share any elements in common with $B$. This means $A \cup B = \emptyset$.  

Consequently, to find a probability involving multiple events, you can use any knowledge of set theory you might have in conjunction with these axioms. Here's one example:

::: {.example name=Probability of a Set Complement"}

$P(A^c) = 1 - P(A)$. Why?
  - Since $A^c$ and $A$ are disjoint, and naturally $A^c \cup A = \Omega$ we first note, by axiom 3, that $P(\Omega) = P(A^c) + P(A)$.
  - Then, by axiom 2, $P(\Omega) = 1$, so $P(A^c) + P(A) = 1$. Rearrange to complete the proof.
  
:::

These axioms are the foundation of all probability. Learn to recognize them when working directly with probability functions $P(\cdot)$. For instance, any time you see a sum, you should think "Aha! Kolmogorov Axiom 3!"

## Basic Probability Solving Techniques

### Disjointification {#disjointify}
  
Kolmogorov's Axiom 3 is by far the most useful, but to use it requires a trick called *disjointification*, or "partitioning into disjoint subsets". In this technique, we rewrite a set into a union of disjoint subsets:

$$A = (A \cap B) \cup (A \cap B^c)$$

Then, if we know the probability of the intersections, we apply Axiom 3 to solve: $P(A) = P(A \cap B) + P(A \cap B^c)$. Let's prove three classic equalities by disjointification:

::: {#intersection-complement .example name="Probability of Set Intersection with Complement" }

To compute the probability of set intersection, use the following:

$$P(B \cap A^c) = P(B) - P(A \cap B)$$

*Proof*:

1. Disjointify: Note $B = (B \cap A) \cup (B \cap A^c)$. Therefore, $P(B) = P(B \cap A^c) + P(A \cap B)$ by Probability Axiom 3
2. Rearrange: $P(B \cap A^c) = P(B) - P(A \cap B)$

:::

::: {#subset-inequality .example name="Subset Inequality"}

Probability inequalities can be constructed using subsets:

$$A \subset B \implies P(A) \leq P(B)$$

*Proof*:

1. Disjointify: If $A \subset B$, then $B = A \cup (A^c \cap B)$, with $A$ and $(A^c \cap B)$ disjoint (since $(A^c \cap B)$ is the part of $B$ not contained by $A$).
2. Consequently, $P(B) = P(A) + P(A^c \cap B)$.
3. By Kolmogorov Axiom 1, $P(A^c \cap B) \geq 0$ which implies that $P(A) \leq P(B)$, completing the proof.

:::

Of course, these examples are famous properties that you can simply memorize and apply directly on an example. But, in case you forget, now you know how to derive it!

### DeMorgan's Laws {#demorgan}

If you ever need to turn a union into an intersection or vice versa, first take the complement $P(A) = 1 - P(A^c)$. Then, apply DeMorgan's Laws from set theory:

1. $(A \cup B)^c = A^c \cap B^c$
2. $(A \cap B)^c = A^c \cup B^c$

Here's an example:

::: {.example name="Probability of a Set Union"}

$$P(A \cup B) = P(A) + P(B) - P(A \cap B)$$

1. Start by taking the complement: $P(A \cup B) = 1 - P((A\cup B)^c$
2. Apply DeMorgan's Laws: $1 - P((A\cup B)^c = 1 - P(A^c \cap B^c)$.
3. Now, we could *disjointify* $A^c$, but a faster way is to recall the [previous set intersection example](#intersection-complement): $1 - P(A^c \cap B^c) = 1 - (P(A^c) - P(A^c \cap B))$
4. Revert the complement: $1 - (P(A^c) - P(A^c \cap B)) = P(A) + P(A^c \cap B)$
5. Repeat Step 3 on the other set intersection: $P(A) + P(A^c \cap B) = P(A) + P(B) - P(A \cap B)$, completing the proof.

:::


### Proving Inequalities: Subsetting

The [Subset Inequality](#subset-inequality) proven above - $A \subset B \implies P(A) \leq P(B)$ can be applied to prove other, more famous inequalities in probability as well. We'll look at two.

::: {.theorem name="Boole's Inequality"}

Boole's Inequality is a general inequality for *unions* of events. For a set of events $A_1,...,A_n$,

$$P(\cup_{i=1}^n A_i) \leq \sum_{i=1}^n P(A_i)$$
*Proof*:

We want to use the subset inequality, so we need to construct a set of subsets $\{B_i\}$. This subset must have the properties:

- $\cup_{i=1}^n A_i = \cup_{i=1}^n B_i$, so we can replace this on the LHS of Boole's inequality above
- $B_i \subset A_i \subset B_i$, so we can use the subset inequality.
- $\{B_i\}$ is disjoint, so we can construct the sum on the RHS using Kolmogorov's Axiom 3.


We can accomplish this by [disjointifying](#disjointify). Let

\begin{align}
B_1 = A_1 \\
B_2 = A_2 \cap A_1^c \\
B_3 = A_3 \cap A_2^c \cap A_1^c \\
... \\
B_n = A_n \cap A_{n-1}^c \cap ... \cap A_1^c
\end{align}

Since $\{B_i\}$ are all disjoint, by Kolmogorov Axiom 3, $P(\cup_{i=1}^n A_i) = \sum_{i=1}^nP(B_i)$.

Finally, use the [subset inequality](#subset-inequality) to note that $B_i \subseteq A_i \implies P(B_i) \leq P(A_i)$ and therefore $P(\cup_{i=1}^n A_i) \leq \sum_{i=1}^n P(B_i) = \sum_{i=1}^n P(A_i)$


:::

::: {.theorem name="Bonferroni's Inequality"}
Bonferroni's Inequality is the *set intersection* counterpart of Boole's Inequality. In general, it states

$$P(\cap_{i=1}^n A_i) \geq \sum_{i=1}^n P(A_i) - (n-1)$$
*Proof*:
1. We need the LHS. Start by taking the complement $P(\cap_{i=1}^n A_i) = 1 - P((\cap_{i=1}^n)^c)$.
2. Apply [DeMorgan's Laws](#demorgan): $1 - P((\cap_{i=1}^n)^c) = 1 - P(\cup_{i=1}^nA_i^c)$.
3. This is Boole's Inequality which we already proved. $1 - P(\cup_{i=1}^nA_i^c) \geq 1 - \sum_{i=1}^n P(A_i^c)$
4. Undo the complement: 

$$
1 - \sum_{i=1}^n P(A_i^c) = 1 - \sum_{i=1}^n (1 - P(A_i)) \\
= \sum_{i=1}^n P(A_i) - (n-1)
$$
:::

Next, we'll discuss conditional probability, where computing set intersections and their bounds is critical for problem-solving.


## Conditional Probability {#conditional-probability}

Sometimes in a problem, we are given additional information about an event. For example, we might wish to compute the probability of $A$ given that we know another event $B$ has already occurred. In this case, we must compute conditional probabilities.

:::{.definition name="Conditional Probability"}
The probability of event $A$ occurring given that we know event $B$ has occurred is given by

$$P(A|B) = \frac{P(A\cap B)}{P(B)}$$

:::

When solving problems involving conditional probabilities, we often convert them via the above definition, and then manipulate the intersection $A\cap B$ using set theory to rewrite into the given form.

:::{.definition name="Bayes' Theorem"}

If we need to invert the order of $A$ and $B$ in a conditional probability express, we can use the property

$$P(A|B) = \frac{P(B|A)P(A)}{P(B)}$$
More generally, let $A_1, A_2, ...$ be a partition of the sample space and let $B$ be any set

$$P(A_i|B) = \frac{P(B|A_i)P(A_i)}{\sum_{j=1}^{\infty} P(B|A_j)P(A_j)}$$
:::

Bayes Theorem often arises in word problems involving given probabilities

:::{.definition name="Law of Total Probability"}

Let $A_1, A_2, ..., A_n$ be a partition of the sample space and let $B$ be any set. 

$$P(B) = \sum_{i=1}^{n} P(B \cap A_i) = \sum_{i=1}^{n} P(B|A_i) P(A_i)$$ 

:::

### Conditional Probability in Practice

Public health research often relies on the task of comparison. There are several standard measures of association that are widely used in public health research. Here we present two of them. 

:::{.definition name="Relative Risk, RR"}

Let $D$ be an outcome event and $S$ be an exposure event. The relative risk (or risk ratio), denoted by $RR$ is given by

$$RR = \frac{P(D|S)}{P(D|\bar{S})}$$
where $\bar{S}$ is the complement of $S$. 

:::

:::{.definition name="Odds Ratio, OR"}

Let $D$ be an outcome event and $S$ be an exposure event. The odds ratio, denoted by $OR$, is given by

$$OR = \frac{P(D|S)/P(\bar{D}|S)}{P(D|\bar{S})/P(\bar{D}|\bar{S})}$$
Note that $P(\bar{D}|S) = 1 - P(D|S)$. 

- The odds ratio is invariant to switching $D$ and $S$, ie. 
$$\frac{P(D|S)/P(\bar{D}|S)}{P(D|\bar{S})/P(\bar{D}|\bar{S})} = \frac{P(S|D)/P(\bar{S}|D)}{P(S|\bar{D})/P(\bar{S}|\bar{D})}.$$
*Proof.*

$$
\begin{aligned}
\frac{P(D|S)/P(\bar{D}|S)}{P(D|\bar{S})/P(\bar{D}|\bar{S})} &= P(D|S) \cdot \frac{1}{P(\bar{D}|S)} \cdot \frac{1}{P(D|\bar{S})} \cdot P(\bar{D}|\bar{S})\\
&= \frac{P(S|D) P(D)}{P(S)} \cdot \frac{P(S)}{P(S|\bar{D})P(\bar{D})} \cdot \frac{P(\bar{S})}{P(\bar{S}|D)P(D)} \cdot \frac{P(\bar{S}|\bar{D})P(\bar{D})}{P(\bar{S})} \\
&= \frac{P(S|D)/P(\bar{S}|D)}{P(S|\bar{D})/P(\bar{S}|\bar{D})}
\end{aligned}
$$
The second step plugs in the following equalities which follow from Bayes' Rule

$$P(D|S) = \frac{P(S|D) P(D)}{P(S)}, P(\bar{D}|S) = \frac{P(S|\bar{D})P(\bar{D})}{P(S)},$$
$$P(D|\bar{S}) = \frac{P(\bar{S}|D)P(D)}{P(\bar{S})}, \text{and } P(\bar{D}|\bar{S}) = \frac{P(\bar{S}|\bar{D})P(\bar{D})}{P(\bar{S})}$$
## Random Variables

### Definition

Statisticians study data, which typically comes not in the form of "events", but rather, *random variables*. A random variable is a real-valued function which maps an event $A$ to some numeric quantity - that is, for an event $A$, the random variable $X(A) \mapsto x \in \mathbb{R}$. Even though $X$ is a function, the $A$ is usually omitted, and probability texts usually refer to it as one would a number (i.e. saying it "takes on" a particular value).

Suppose $A$ represents the event where a person contracts heart disease. Then, we can "turn this into a number" by defining a random variable:

$X = \begin{cases}1 \text{ if }A\text{ occurs}\\0 \text{ if }A\text{ does not occur}\end{cases}$.

Since the output of a random variable is real, we can describe the probability of a given numerical output from a random variable in many ways. Here are the most common:


### Functions Describing the Distribution

::: {.definition name="Probability Mass Function"}

If $X$ is countable, then the probability mass function (pmf) is

$$f_X(x) = P(X = x)$$.

Properties arising from Kolmogorov's Axioms:
- $0 \leq f_X(x) \leq 1$ (by Axiom 1)
- $\sum_{x} f_X(x) = 1$ (by Axiom 3)

:::


:::{.definition name="Cumulative Distribution Function"}

The cumulative distribution function (cdf) of $X$ is $F_X(x) = P(X \leq x)$.

- For discrete RVs with $x \geq 0$, $F_X(x) = \sum_{k = 0}^xP(X = x)$
- For continuous RVs, $F_X(x) = \int_{-\infty}^xf(t)dt$

A function $F(x)$ is a cdf iff the following all hold:

1. $\lim_{x\rightarrow -\infty}F(x) = 0$ and $\lim_{x\rightarrow \infty}F(x) = 1$.
2. $F(x)$ is nondecreasing.
3. $F(x)$ is right continuous: $\lim_{x\rightarrow c^+}F(x) = F(c)$.

:::

We can prove a particular function is a cdf by proving each necessary property. Prove...
- (1) by taking the limit directly
- (2) by taking the derivative of $F_X(x)$ and showing it is positive
- (3) by arguing that $F_X(x)$ is continuous, showing it has no undefined points (continuity implies right-continuity).

In general, we can show two random variables have the same distribution by showing they have the same cdf (*example*?)


:::{.definition name="Probability Density Function"}

The probability density function (pdf) is the continuous analogue of a pmf. One way to define the pdf of $X$ is as the derivative of the cdf:

$$f_X(x) = \frac{d}{dx}F_X(x)$$

Therefore, $F_X(x) = \int_{-\infty}^xf_X(t)dt$

(It is **not** P(X = x); $X$ is uncountable, so this is technically 0).


It satisfies two properties from Kolmogorov's Axiom's:
- $0 \leq f_X(x) \leq 1$ (by Axiom 1)
- $\sum_{x} f_X(x) = 1$ (by Axiom 3)

:::


### Technique: Direct Probability Manipulation {#direct-manipulation}

Functions of random variables are also random variables. Suppose $Y$ and $X$ are both random variables, and $Y = g(X)$. We often want to find $P(Y\in A)$ where $A$ is some set such as an interval or even a single value. If we know the distribution of $X$, we can find the distribution of $Y$ like so:

$$
P(Y \in A) = P(g(X) \in A) = P(X \in g^{-1}(A))
$$
where $g^{-1}(A) = \{x \in \mathcal{X}: g(X) \in A\}$ (note $\mathcal{X}$ is the sample space of $X$)


Since this is such a fundamental technique arising throughout statistics, in lieu of an example, we will point out where this technique is used in future chapters. We can, however, note that if $X$ and $Y$ are both discrete, then

$$f_Y(y) = P(Y = y) = \sum_{x \in g^{-1}(y)}P(X=x) = \sum_{x \in g^{-1}(y)}f_X(x)$$

meaning the pmf of $Y$ is just the sum of the probabilities of $X$ over all $y \in \mathcal{Y}$ (where $\mathcal{Y}$ is the sample space of $Y$). For more information, see Chapter 2 of @Casella1990. **This is very important! When in doubt, fall back on direct manipulation of the probability function.**


## Independence {#independence}

:::{.definition name="Independence"}
Two events or random variables are **statistically independent** if the occurrence of one has no impact on the other. Mathematically,

$$P(A | B) = P(A)$$

By Bayes' Theorem,

$$P(A \cap B) = P(A)P(B)$$
:::

Often, we prefer to use the second definition because it is independent and easier to generalize to multiple events or random variables. For example, if we know that the random variables $X$ and $Y$ are independent, then

$$f_{X,Y}(x, y) = f_X(x)f_Y(y)$$

This property is the foundation of statistical inference for iid random variables discussed in Chapters 8 and 10.


## Order Statistics {#order-statistics}

::: {.definition name="Order statistic"} 

The $k$th order statistic of a random sample from a random variable $X$ is the $k$th smallest value. We denote the $k$th order statistic as $X_{(k)}$. 

Let $X_1, X_2, ..., X_n$ be a random sample from $X  \sim f_X(x)$ with cumulative distribution $F_X(x)$. The order statistics are random variables themselves that satisfy $X_{(1)} \leq X_{(2)}\leq ... \leq X_{(n)}$. In particular, $X_{(1)} = min(X_1, X_2, ..., X_n)$ and  $X_{(n)} = max(X_1, X_2, ..., X_n)$. 

- The **sample range**, $R = X_{(n)} - X_{(1)}$,  is the distance between the smallest and the largest order statistics of the sample. 
- The **sample median** is a number $M$ such that approximately half of the observations in the sample are less than $M$ and the other half are greater. 

$$M = \begin{cases} 
      X_{((n+1)/2)}& \text{if } n  \text{ is odd} \\
      (X_{(n/2)} + X_{(n/2 + 1)})/2 & \text{if } n  \text{ is even.}
   \end{cases}$$
:::

::: {.theorem} 

Let $X_1, X_2, ..., X_n$ be a random sample from a discrete distribution with pmf $f_X(x_i) = p_i$, where $x_1<x_2< \cdots$ are the possible values of $X$ in ascending order. Define $P_i = p_1 + p_2 + \cdots p_i$. Let $X_{(1)}, X_{(2)}, ... ,X_{(n)}$ denote the order statistics from the sample. Then 

$$P(X_{(j)}\leq x_i) = \sum_{k=j}^{n} {n \choose k} P_i^k(1-P_i)^{n-k}$$
and 

$$P(X_{(j)}= x_i) = \sum_{k=j}^{n} {n \choose k} [P_i^k(1-P_i)^{n-k} - P_{i-1}^k(1-P_{i-1})^{n-k}].$$
:::


::: {.theorem} 

Let $X_1, X_2, ..., X_n$ be a random sample from a continuous distribution with pdf $f_X(x_i)$ and cdf $F_X(x)$. Then the pdf of $X_{(j)}$ is 

$$f_{X_{(j)}}(x) = \frac{n!}{(j-1)!(n-j)!} f_X(x)[F_X(x)]^{j-1}[1-F_X(x)]^{n-j}.$$

:::

::: {.theorem} 
Let $X_1, X_2, ..., X_n$ be a random sample from a continuous distribution with pdf $f_X(x_i)$ and cdf $F_X(x)$. Then the joint pdf of  $X_{(i)}$ and $X_{(j)}, 1\leq i \leq j \leq n$, is 

$$f_{X_{(i)}, X_{(j)}} (u,v) = \frac{n!}{(i-1)!(j-1-i)!(n-j)!}f_X(u)f_X(v)[F_X(u)]^{i-1} [F_X(v)-F_X(u)]^{j-1-i}[1-F_X(v)]^{n-j}.$$

The joint pdf of all order statistics is 

$$f_{X_{(1)}, X_{(2)}, ..., X_{(n)}} (x_1,x_2, ..., x_n) = \begin{cases} 
n!f_X(x_1)f_X(x_2) \cdots f_X(x_n) & -\infty< x_1<x_2<\cdots<x_n<\infty\\
0 & otherwise
\end{cases}$$
::: 

::: {.example name="Joint Density of Order Statistic Differences"}
A useful trick for finding the joint of $D_i = X_{(i)} - X_{(i-1)}$ and proving that they are independent for a given distribution is by rewriting the random variable as

$$
X_{(1)} = D_1 + 0\\
X_{(2)} = D_2 + X_{(1)} = D_2 + D_1\\
X_{(3)} = D_3 + X_{(2)} = D3 + D2 + D1\\

$$
and so on. By induction, $X_{(i)} = \sum_{j=1}^{i-1}X_{(j)}$. Therefore, we can rewrite the joint density of all order statistics as $n!f(d_1)f(d_2 + d_1)\cdot...\cdot f(d_1 + ... + d_n)I(d_i \geq 0, \forall i)$, which we can then compute and factorize to show independence.
:::


## Convergence

### Borel-Cantelli Lemmas

Let $E_1$, $E_2$,... be a sequence of events. 

::: {.lemma name="First Borel-Cantelli"}
If $\sum_{n=1}^\infty P(E_n) \leq \infty$, then 

$$P\Big(\limsup_{n\rightarrow\infty}E_n\Big) = P\Big(\bigcap_{n=1}^\infty\bigcup_{k=n}^\infty E_k\Big) = 0$$

:::

::: {.lemma name="Second Borel-Cantelli"}

If $\sum_{n=1}^\infty P(E_n) = \infty$ and $E_k$ are independent, then 


$$P\Big(\limsup_{n\rightarrow\infty}E_n\Big) = P\Big(\bigcap_{n=1}^\infty\bigcup_{k=n}^\infty E_k\Big) = 1$$

:::


### Convergence in Probability {#convergence-in-probability}

What happens when, instead of a set of events or random variables, we observe a *sequence* of $n$ random variables? There exist two major types of **convergence** of random variables with which we are typically concerned: **convergence in probability** and **convergence in distribution**. Let's start with the former.

:::{.definition name="Convergence in Probability"}

If $Z$ is a random variable and $Z_n$ is a sequence of random variables, then

$$Z_n \overset{p}{\rightarrow} Z \iff\lim_{n\rightarrow\infty}P(|Z_n - Z| > \epsilon) = 0$$
:::

If $Z$ is degenerate (meaning it equals a constant value, or $Z = c$), we can show convergence in probability by [disjointifying](#disjointify) the absolute value into two disjoint events, and using Kolmogorov Axiom 3. For example, 

$$
P(|Z_n - c| > \epsilon) = P(( Z_n - c > \varepsilon) \cup (Z_n - c <-\varepsilon)) \text{ with } ( Z_n - c > \varepsilon), (Z_n - c <-\varepsilon) \text{ disjoint} \\
= P(Z_n > c + \epsilon) + P(Z_n < c - \epsilon) = 1 - P(Z_n < c + \epsilon) + P(Z_n < c - \epsilon)

$$
From here, we can use the CDF of $Z_n$ to compute the necessary probability. But be careful - this method may not work if the convergence of the CDF cannot be evaluated easily.

Convergence in probability has several properties - if we *know* $Z_n$ convergence in probability, we can use these for solving other problems, especially in [Chapter 9 - Point Estimator Asymptotic](#point-estimators-asymptotics)  If $A_n \overset{p}{\rightarrow} a$ and $B_n\overset{p}{\rightarrow}b$, then

1. $A_n + B_n \overset{p}{\rightarrow} a + b$
2. $A_n - B_n \overset{p}{\rightarrow} a - b$
3. $A_n \cdot B_n \overset{p}{\rightarrow} a \cdot b$
4. $A_n / B_n \overset{p}{\rightarrow} a / b$

Convergence in probability can also be extended to the multivariate setting, where it takes on a slightly different meaning.

:::{.definition name="Multivariate Convergence in Probability"}
If $X$ is random vector and $X_n$ is a sequence of random vectors, then

\begin{align}
X_n \overset{p}{\rightarrow} X \iff\lim_{n\rightarrow\infty}P(||X_n - X|| > \epsilon) = 0 \\
\iff X_{jn} \overset{p}{\rightarrow}X_j, \forall j \in 1,...,k
\end{align}

:::

### Convergence in Distribution

:::{.definition name="Convergence in Probability"}

$$Z_n \overset{\mathcal{D}}{\rightarrow} Z \iff\lim_{n\rightarrow\infty}F_n(Z) = F(z), \forall\text{ continuity points of }F$$
:::

Note that convergence in probability implies convergence in distribution, but not the converse. That is,

$$Z_n \overset{p}{\rightarrow} Z \implies Z_n \overset{\mathcal{D}}{\rightarrow} Z$$
Unless, that is, the random variable converges in distribution to a *constant* - then, convergence in distribution *does* imply convergence in probability! That is,

$$Z_n \overset{\mathcal{D}}{\rightarrow} c \implies Z_n \overset{p}{\rightarrow} c $$
:::{.definition name="Multivariate Convergence in Distribution"}
If $X$ is random vector and $X_n$ is a sequence of random vectors, then

$$X_n \overset{\mathcal{D}}{\rightarrow} X \iff \lim_{n\rightarrow\infty}F_n(X_1, ..., X_k) = F(X_1,...,X_k), \forall\text{ continuity points of }F$$
:::

How do we prove $X_n$ converges in distribution to $X$? There are three chief strategies:

1. Derive the limiting CDF of $X_n$ and show its limit as $n\rightarrow \infty$ equals the $F_X(x)$. 
2. Derive the MGF or CF of $X_n$ and show its limit as $n\rightarrow \infty$ equals the $\mathcal{M}_X(t)$.
3. Use the CLT with Slutsky's Theorem or the CMT (will be discussed later in [Chapter 9 - Point Estimator Asymptotics](#point-estimators-asymptotics))



### Important Theorems

:::{.theorem name="Slutsky's Theorem"}

If $Z_n \overset{\mathcal{D}}{\rightarrow} Z$ and $Y_n \overset{p}{\rightarrow} c$, then

1. $Z_n + Y_n \overset{\mathcal{D}}{\rightarrow} Z + c$
2. $Z_nY_n \overset{\mathcal{D}}{\rightarrow} cZ$
3. $\frac{Z_n}{Y_n}\overset{\mathcal{D}}{\rightarrow}\frac{Z}{c}$. 

:::

For problem-solving, Slutsky's theorem is generally applied whenever we deal with both convergence in probability and convergence in distribution together.

:::{#cmt .theorem name="Continuous Mapping Theorem"}

Suppose $Y_n$ is a sequence of random variables (possibly vectors), $Y$ is a random variable (or vector the same length as $Y_n$), $c$ is a constant, and $g$ is a function. Then,

1. If $Y_n \overset{p}{\rightarrow} c$, and $g$ is continuous at $c$, then $g(Y_n) \overset{p}{\rightarrow} g(c)$

2. If $Y_n \overset{\mathcal{D}}{\rightarrow} Y$, and $g$ is continuous (with $g: \mathbb{R}^k \mapsto \mathbb{R}^m$ for vectors), then $g(Y_n) \overset{\mathcal{D}}{\rightarrow} g(Y)$ (in $\mathbb{R}^m$ for vectors)

:::
