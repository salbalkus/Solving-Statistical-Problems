# Probability {#probability}

## Basic Axioms

How do we reason about events that may or may not occur? The field of *probability* provides the mathematical foundations for reasoning under uncertainty.

The foundation of probability is set theory. When you think about "something that could happen", you can conceptualize that "something" as an experiment. We can define all of the details of that experiment in terms of sets:

- Any given outcome of the experiment we represent as a set - call it $A$.
- We represent the set of all possible that could have occurred - the *sampling space* - as $\Omega$. Hence, $A \subseteq \Omega$

Now suppose we could repeat this experiment infinitely. The *probability* of an event A is defined as the proportion of experiments in which that event will occurs as the number of experiments increases towards infinity. Mathematically, we represent probability as a function: $P(A)$. Then,

$$P(A) = \lim_{n\rightarrow\infty}\frac{\text{# of times A is drawn}}{n}$$

In the simplest case, we can compute this proportion directly as $P(A) = \frac{\text{# of outcomes in }A}{\text{total number of outcomes}}$ - a typical grade-school exercise. Furthermore, from this definition, three properties (called Kolmogorov's axioms) arise:

:::{.definition name="Kolmogorov's Axioms of Probability"}

For all probability functions $P(\cdot)$ defined on a sampling space $\Omega$:

1. $0 \leq P(A) \leq 1$ for all $A\subseteq \Omega$
2. $P(\Omega) = 1$.
3. If $A_1, A_2, ..., $ are pairwise disjoint, then 

$$P(\cup_{i=1}^\infty A_i) = \sum_{i=1}^\infty P(A_i)$$

:::

Two sets $A$ and $B$ are *pairwise disjoint* if $A$ does not share any elements in common with $B$. This means $A \cup B = \emptyset$.  

Consequently, to find a probability involving multiple events, you can use any knowledge of set theory you might have in conjunction with these axioms. Here's one example:

::: {.example name=Probability of a Set Complement"}

$P(A^c) = 1 - P(A)$. Why?
  - Since $A^c$ and $A$ are disjoint, and naturally $A^c \cup A = \Omega$ we first note, by axiom 3, that $P(\Omega) = P(A^c) + P(A)$.
  - Then, by axiom 2, $P(\Omega) = 1$, so $P(A^c) + P(A) = 1$. Rearrange to complete the proof.
  
:::

These axioms are the foundation of all probability. Learn to recognize them when working directly with probability functions $P(\cdot)$. For instance, any time you see a sum, you should think "Aha! Kolmogorov Axiom 3!"

## Basic Probability Solving Techniques

### Disjointification {#disjointify}
  
Kolmogorov's Axiom 3 is by far the most useful, but to use it requires a trick called *disjointification*, or "partitioning into disjoint subsets". In this technique, we rewrite a set into a union of disjoint subsets:

$$A = (A \cap B) \cup (A \cap B^c)$$

Then, if we know the probability of the intersections, we apply Axiom 3 to solve: $P(A) = P(A \cap B) + P(A \cap B^c)$. Let's prove three classic equalities by disjointification:

::: {#intersection-complement .example name="Probability of Set Intersection with Complement" }

To compute the probability of set intersection, use the following:

$$P(B \cap A^c) = P(B) - P(A \cap B)$$

*Proof*:

1. Disjointify: Note $B = (B \cap A) \cup (B \cap A^c)$. Therefore, $P(B) = P(B \cap A^c) + P(A \cap B)$ by Probability Axiom 3
2. Rearrange: $P(B \cap A^c) = P(B) - P(A \cap B)$

:::

::: {#subset-inequality .example name="Subset Inequality"}

Probability inequalities can be constructed using subsets:

$$A \subset B \implies P(A) \leq P(B)$$

*Proof*:

1. Disjointify: If $A \subset B$, then $B = A \cup (A^c \cap B)$, with $A$ and $(A^c \cap B)$ disjoint (since $(A^c \cap B)$ is the part of $B$ not contained by $A$).
2. Consequently, $P(B) = P(A) + P(A^c \cap B)$.
3. By Kolmogorov Axiom 1, $P(A^c \cap B) \geq 0$ which implies that $P(A) \leq P(B)$, completing the proof.

:::

Of course, these examples are famous properties that you can simply memorize and apply directly on an example. But, in case you forget, now you know how to derive it!

### DeMorgan's Laws {#demorgan}

If you ever need to turn a union into an intersection or vice versa, first take the complement $P(A) = 1 - P(A^c)$. Then, apply DeMorgan's Laws from set theory:

1. $(A \cup B)^c = A^c \cap B^c$
2. $(A \cap B)^c = A^c \cup B^c$

Here's an example:

::: {.example name="Probability of a Set Union"}

$$P(A \cup B) = P(A) + P(B) - P(A \cap B)$$

1. Start by taking the complement: $P(A \cup B) = 1 - P((A\cup B)^c$
2. Apply DeMorgan's Laws: $1 - P((A\cup B)^c = 1 - P(A^c \cap B^c)$.
3. Now, we could *disjointify* $A^c$, but a faster way is to recall the [previous set intersection example](#intersection-complement): $1 - P(A^c \cap B^c) = 1 - (P(A^c) - P(A^c \cap B))$
4. Revert the complement: $1 - (P(A^c) - P(A^c \cap B)) = P(A) + P(A^c \cap B)$
5. Repeat Step 3 on the other set intersection: $P(A) + P(A^c \cap B) = P(A) + P(B) - P(A \cap B)$, completing the proof.

:::


### Proving Inequalities: Subsetting

The [Subset Inequality](#subset-inequality) proven above - $A \subset B \implies P(A) \leq P(B)$ can be applied to prove other, more famous inequalities in probability as well. We'll look at two.

::: {.theorem name="Boole's Inequality"}

Boole's Inequality is a general inequality for *unions* of events. For a set of events $A_1,...,A_n$,

$$P(\cup_{i=1}^n A_i) \leq \sum_{i=1}^n P(A_i)$$
*Proof*:

We want to use the subset inequality, so we need to construct a set of subsets $\{B_i\}$. This subset must have the properties:

- $\cup_{i=1}^n A_i = \cup_{i=1}^n B_i$, so we can replace this on the LHS of Boole's inequality above
- $B_i \subset A_i \subset B_i$, so we can use the subset inequality.
- $\{B_i\}$ is disjoint, so we can construct the sum on the RHS using Kolmogorov's Axiom 3.


We can accomplish this by [disjointifying](#disjointify). Let

\begin{align}
B_1 = A_1 \\
B_2 = A_2 \cap A_1^c \\
B_3 = A_3 \cap A_2^c \cap A_1^c \\
... \\
B_n = A_n \cap A_{n-1}^c \cap ... \cap A_1^c
\end{align}

Since $\{B_i\}$ are all disjoint, by Kolmogorov Axiom 3, $P(\cup_{i=1}^n A_i) = \sum_{i=1}^nP(B_i)$.

Finally, use the [subset inequality](#subset-inequality) to note that $B_i \subseteq A_i \implies P(B_i) \leq P(A_i)$ and therefore $P(\cup_{i=1}^n A_i) \leq \sum_{i=1}^n P(B_i) = \sum_{i=1}^n P(A_i)$


:::

::: {.theorem name="Bonferroni's Inequality"}
Bonferroni's Inequality is the *set intersection* counterpart of Boole's Inequality. In general, it states

$$P(\cap_{i=1}^n A_i) \geq \sum_{i=1}^n P(A_i) - (n-1)$$
*Proof*:
1. We need the LHS. Start by taking the complement $P(\cap_{i=1}^n A_i) = 1 - P((\cap_{i=1}^n)^c)$.
2. Apply [DeMorgan's Laws](#demorgan): $1 - P((\cap_{i=1}^n)^c) = 1 - P(\cup_{i=1}^nA_i^c)$.
3. This is Boole's Inequality which we already proved. $1 - P(\cup_{i=1}^nA_i^c) \geq 1 - \sum_{i=1}^n P(A_i^c)$
4. Undo the complement: 

$$
1 - \sum_{i=1}^n P(A_i^c) = 1 - \sum_{i=1}^n (1 - P(A_i)) \\
= \sum_{i=1}^n P(A_i) - (n-1)
$$
:::

Next, we'll discuss conditional probability, where computing set intersections and their bounds is critical for problem-solving.


## Conditional Probability

Sometimes in a problem, we are given additional information about an event. For example, we might wish to compute the probability of $A$ given that we know another event $B$ has already occurred. In this case, must compute conditional probabilities.

:::{.definition name="Conditional Probability"}
The probability of event $A$ occurring given that we know event $B$ has occurred is given by

$$P(A|B) = \frac{P(A\cap B)}{P(B)}$$

:::

:::{.definition name="Bayes' Theorem"}

If we need to invert the order of $A$ and $B$ in a conditional probability express, we can use the property

$$P(A|B) = \frac{P(B|A)P(A)}{P(B)}$$
More generally, let $A_1, A_2, ...$ be a partition of the sample space and let $B$ be any set

$$P(A_i|B) = \frac{P(B|A_i)P(A_i)}{\sum_{j=1}^{\infty} P(B|A_j)P(A_j)}$$
:::

:::{.definition name="Law of Total Probability"}

Let $A_1, A_2, ..., A_n$ be a partition of the sample space and let $B$ be any set. 

$$P(B) = \sum_{i=1}^{n} P(B \cap A_i) = \sum_{i=1}^{n} P(B|A_i) P(A_i)$$ 

:::

### Conditional Probability in Practice

Public health research often relies on the task of comparison. There are several standard measures of association that are widely used in public health research. Here we present two of them. 

:::{.definition name="Relative Risk, RR"}

Let $D$ be an outcome event and $S$ be an exposure event. The relative risk (or risk ratio), denoted by $RR$ is given by

$$RR = \frac{P(D|S)}{P(D|\bar{S})}$$
where $\bar{S}$ is the complement of $S$. 

:::

:::{.definition name="Odds Ratio, OR"}

Let $D$ be an outcome event and $S$ be an exposure event. The odds ratio, denoted by $OR$, is given by

$$OR = \frac{P(D|S)/P(\bar{D}|S)}{P(D|\bar{S})/P(\bar{D}|\bar{S})}$$
Note that $P(\bar{D}|S) = 1 - P(D|S)$. 

- The odds ratio is invariant to switching $D$ and $S$, ie. 
$$\frac{P(D|S)/P(\bar{D}|S)}{P(D|\bar{S})/P(\bar{D}|\bar{S})} = \frac{P(S|D)/P(\bar{S}|D)}{P(S|\bar{D})/P(\bar{S}|\bar{D})}.$$
*Proof.*

$$
\begin{aligned}
\frac{P(D|S)/P(\bar{D}|S)}{P(D|\bar{S})/P(\bar{D}|\bar{S})} &= P(D|S) \cdot \frac{1}{P(\bar{D}|S)} \cdot \frac{1}{P(D|\bar{S})} \cdot P(\bar{D}|\bar{S})\\
&= \frac{P(S|D) P(D)}{P(S)} \cdot \frac{P(S)}{P(S|\bar{D})P(\bar{D})} \cdot \frac{P(\bar{S})}{P(\bar{S}|D)P(D)} \cdot \frac{P(\bar{S}|\bar{D})P(\bar{D})}{P(\bar{S})} \\
&= \frac{P(S|D)/P(\bar{S}|D)}{P(S|\bar{D})/P(\bar{S}|\bar{D})}
\end{aligned}
$$
The second step plugs in the following equalities which follow from Bayes' Rule

$$P(D|S) = \frac{P(S|D) P(D)}{P(S)}, P(\bar{D}|S) = \frac{P(S|\bar{D})P(\bar{D})}{P(S)},$$
$$P(D|\bar{S}) = \frac{P(\bar{S}|D)P(D)}{P(\bar{S})}, \text{and } P(\bar{D}|\bar{S}) = \frac{P(\bar{S}|\bar{D})P(\bar{D})}{P(\bar{S})}$$

## Independence {#independence}

:::{.definition name="Independence"}
Two events are **statistically independent** if the occurrence of one has no impact on the other. Mathematically,

$$P(A | B) = P(A)$$

By Bayes' Theorem,

$$P(A \cap B) = P(A)P(B)$$
:::

Often, we prefer to use the second definition because it is independent and easier to generalize to multiple events or random variables. For example, if we know that the random variables $X$ and $Y$ are independent, then

$$f_{X,Y}(x, y) = f_X(x)f_Y(y)$$

This property is the foundation of statistical inference for iid random variables discussed in Chapters 8 and 10.


## Order Statistics

::: {.definition name="Order statistic"} 

The $k$th order statistic of a random sample from a random variable $X$ is the $k$th smallest value. We denote the $k$th order statistic as $X_{(k)}$. 

Let $X_1, X_2, ..., X_n$ be a random sample from $X  \sim f_X(x)$ with cumulative distribution $F_X(x)$. The order statistics are random variables themselves that satisfy $X_{(1)} \leq X_{(2)}\leq ... \leq X_{(n)}$. In particular, $X_{(1)} = min(X_1, X_2, ..., X_n)$ and  $X_{(n)} = max(X_1, X_2, ..., X_n)$. 

- The **sample range**, $R = X_{(n)} - X_{(1)}$,  is the distance between the smallest and the largest order statistics of the sample. 
- The **sample median** is a number $M$ such that approximately half of the observations in the sample are less than $M$ and the other half are greater. 

$$M = \begin{cases} 
      X_{((n+1)/2)}& \text{if } n  \text{ is odd} \\
      (X_{(n/2)} + X_{(n/2 + 1)})/2 & \text{if } n  \text{ is even.}
   \end{cases}$$
:::

::: {.theorem} 

Let $X_1, X_2, ..., X_n$ be a random sample from a discrete distribution with pmf $f_X(x_i) = p_i$, where $x_1<x_2< \cdots$ are the possible values of $X$ in ascending order. Define $P_i = p_1 + p_2 + \cdots p_i$. Let $X_{(1)}, X_{(2)}, ... ,X_{(n)}$ denote the order statistics from the sample. Then 

$$P(X_{(j)}\leq x_i) = \sum_{k=j}^{n} {n \choose k} P_i^k(1-P_i)^{n-k}$$
and 

$$P(X_{(j)}= x_i) = \sum_{k=j}^{n} {n \choose k} [P_i^k(1-P_i)^{n-k} - P_{i-1}^k(1-P_{i-1})^{n-k}].$$
:::


::: {.theorem} 

Let $X_1, X_2, ..., X_n$ be a random sample from a continuous distribution with pdf $f_X(x_i)$ and cdf $F_X(x)$. Then the pdf of $X_{(j)}$ is 

$$f_{X_{(j)}}(x) = \frac{n!}{(j-1)!(n-j)!} f_X(x)[F_X(x)]^{j-1}[1-F_X(x)]^{n-j}.$$

:::

::: {.theorem} 
Let $X_1, X_2, ..., X_n$ be a random sample from a continuous distribution with pdf $f_X(x_i)$ and cdf $F_X(x)$. Then the joint pdf of  $X_{(i)}$ and $X_{(j)}, 1\leq i \leq j \leq n$, is 

$$f_{X_{(i)}, X_{(j)}} (u,v) = \frac{n!}{(i-1)!(j-1-i)!(n-j)!}f_X(u)f_X(v)[F_X(u)]^{i-1} [F_X(v)-F_X(u)]^{j-1-i}[1-F_X(v)]^{n-j}.$$

The joint pdf of all order statistics is 

$$f_{X_{(1)}, X_{(2)}, ..., X_{(n)}} (x_1,x_2, ..., x_n) = \begin{cases} 
n!f_X(x_1)f_X(x_2) \cdots f_X(x_n) & -\infty< x_1<x_2<\cdots<x_n<\infty\\
0 & otherwise
\end{cases}$$
::: 

## Convergence

What happens when, instead of a set of events or random variables, we observe a *sequence* of $n$ random variables? There exist two major types of **convergence** of random variables with which we are typically concerned: **convergence in probability** and **convergence in distribution**

### Convergence in Probability

:::{.definition name="Convergence in Probability"}

If $Z$ is a random variable and $Z_n$ is a sequence of random variables, then

$$Z_n \overset{p}{\rightarrow} Z \iff\lim_{n\rightarrow\infty}P(|Z_n - Z| > \epsilon) = 0$$
:::

Convergence in probability has several properties. If $A_n \overset{p}{\rightarrow} a$ and $B_n\overset{p}{\rightarrow}b$, then

1. $A_n + B_n \overset{p}{\rightarrow} a + b$
2. $A_n - B_n \overset{p}{\rightarrow} a - b$
3. $A_n \cdot B_n \overset{p}{\rightarrow} a \cdot b$
4. $A_n / B_n \overset{p}{\rightarrow} a / b$

Convergence in probability can also be extended to the multivariate setting, where it takes on a slightly different meaning.

:::{.definition name="Multivariate Convergence in Probability"}
If $X$ is random vector and $X_n$ is a sequence of random vectors, then

\begin{align}
X_n \overset{p}{\rightarrow} X \iff\lim_{n\rightarrow\infty}P(||X_n - X|| > \epsilon) = 0 \\
\iff X_{jn} \overset{p}{\rightarrow}X_j, \forall j \in 1,...,k
\end{align}

:::



### Convergence in Distribution

:::{.definition name="Convergence in Probability"}

$$Z_n \overset{\mathcal{D}}{\rightarrow} Z \iff\lim_{n\rightarrow\infty}F_n(Z) = F(z), \forall\text{ continuity points of }F$$
:::

Note that convergence in probability implies convergence in distribution, but not the converse. That is,

$$Z_n \overset{p}{\rightarrow} Z \implies Z_n \overset{\mathcal{D}}{\rightarrow} Z$$
Unless, that is, the random variable converges in distribution to a *constant* - then, convergence in distribution *does* imply convergence in probability! That is,

$$Z_n \overset{\mathcal{D}}{\rightarrow} c \implies Z_n \overset{p}{\rightarrow} c $$
:::{.definition name="Multivariate Convergence in Distribution"}
If $X$ is random vector and $X_n$ is a sequence of random vectors, then

$$X_n \overset{\mathcal{D}}{\rightarrow} X \iff \lim_{n\rightarrow\infty}F_n(X_1, ..., X_k) = F(X_1,...,X_k), \forall\text{ continuity points of }F$$


:::


### Important Theorems

:::{.theorem name="Slutsky's Theorem"}

If $Z_n \overset{\mathcal{D}}{\rightarrow} Z$ and $Y_n \overset{p}{\rightarrow} c$, then

1. $Z_n + Y_n \overset{\mathcal{D}}{\rightarrow} Z + c$
2. $Z_nY_n \overset{\mathcal{D}}{\rightarrow} cZ$
3. $\frac{Z_n}{Y_n}\overset{\mathcal{D}}{\rightarrow}\frac{Z}{c}$. 

:::

For problem-solving, Slutsky's theorem is generally applied whenever we deal with both convergence in probability and convergence in distribution together.

:::{.theorem name="Continuous Mapping Theorem"}

Suppose $Y_n$ is a sequence of random variables (possibly vectors), $Y$ is a random variable (or vector the same length as $Y_n$), $c$ is a constant, and $g$ is a function. Then,

1. If $Y_n \overset{p}{\rightarrow} c$, and $g$ is continuous at $c$, then $g(Y_n) \overset{p}{\rightarrow} g(c)$

2. If $Y_n \overset{\mathcal{D}}{\rightarrow} Y$, and $g$ is continuous (with $g: \mathbb{R}^k \mapsto \mathbb{R}^m$ for vectors), then $g(Y_n) \overset{\mathcal{D}}{\rightarrow} g(Y)$ (in $\mathbb{R}^m$ for vectors)

:::
