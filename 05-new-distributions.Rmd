# New Distributions {#new-distributions}

This chapter will demonstrate how to derive the PDF or CDF of a random variable that is a function of other random variables, including hierarchical models.

## Transformation Theorems

We'll start by introducing two useful theorems for finding the pdf of a transformed random variable $Y=g(X)$ where $g(X)$ is a monotone, one-to-one function over the domain of interest.

&nbsp;

>**Theorem 2.1.5** Let $X$ have $p d f f_X(x)$ and let $Y=g(X)$, where $g$ is a monotone function. Let $\mathcal{X}$ and $\mathcal{Y}$ be defined by (2.1.7). Suppose that $f_X(x)$ is continuous on $\mathcal{X}$ and that $g^{-1}(y)$ has a continuous derivative on $\mathcal{Y}$. Then the pdf of $Y$ is given by
$$
f_Y(y)= \begin{cases}f_X\left(g^{-1}(y)\right)\left|\frac{d}{d y} g^{-1}(y)\right| & y \in \mathcal{Y} \\ 0 & \text { otherwise}\end{cases}
$$

&nbsp;

In some cases, $g(X)$ will only be a monotone, one-to-one function over subsets of the domain of interest. Below, **Theorem 2.1.8** provides a generalization of **Theorem 2.1.5** that can be applied in these situations. 

&nbsp;

>**Theorem 2.1.8** Let $X$ have pdf $f_X(x)$, let $Y=g(X)$, and define the sample space $\mathcal{X}$ as in (2.1.7). Suppose there exists a partition, $A_0, A_1, \ldots, A_k$, of $\mathcal{X}$ such that $P\left(X \in A_0\right)=0$ and $f_X(x)$ is continuous on each $A_i$. Further, suppose there exist functions $g_1(x), \ldots, g_k(x)$, defined on $A_1, \ldots, A_k$, respectively, satisfying 
>
1. $g(x)=g_i(x)$, for $x \in A_i$,
2. $g_i(x)$ is monotone on $A_i$,
3. the set $\mathcal{Y}=\left\{y: y=g_i(x)\right.$ for some $\left.x \in A_i\right\}$ is the same for each $i=1, \ldots, k$, and
4. $g_i^{-1}(y)$ has a continuous derivative on $\mathcal{Y}$, for each $i=1, \ldots, k$.
Then
$$
f_Y(y)= \begin{cases}\sum_{i=1}^k f_X\left(g_i^{-1}(y)\right)\left|\frac{d}{d y} g_i^{-1}(y)\right| & y \in \mathcal{Y} \\ 0 & \text { otherwise }\end{cases}
$$

&nbsp;

For transformations of multiple random variables, recall the bivariate Jacobian:

$$
J=\left|\begin{array}{ll}
\frac{\partial x}{\partial u} & \frac{\partial x}{\partial v} \\
\frac{\partial y}{\partial u} & \frac{\partial y}{\partial v}
\end{array}\right|=\frac{\partial x}{\partial u} \frac{\partial y}{\partial v}-\frac{\partial y}{\partial u} \frac{\partial x}{\partial v}
$$

## Probability Integral Transform

The probability integral transform is a powerful technique that allows for the simulation of random variables from any distribution, given that you have access to a source of uniformly distributed random numbers. This method is particularly useful because it provides a straightforward way to transform uniform random variables into random variables following a desired distribution. We'll start with the general idea:

When you plug any continuous random variable $X$ into its own CDF, you get $U \sim$ Uniform(0,1):
$$
F_X(X) \sim \text{Uniform} (0,1)
$$

When you plug $U \sim$ Uniform(0,1) into an inverse CDF, you get a continuous random variable $X$ with that CDF:
$$
F_X^{-1}(U) \text{ has CDF } F_X(x)
$$

The general procedure for constructing a random variable $X$ with a particular CDF $F_X(x)$ is as follows:

1. Generate $U \sim$ Uniform(0,1)
2. Plug $U$ into the inverse CDF: $X=F^{-1}_X(U)$
3. $X$ is distributed according to the CDF $F_X(x)$

*[For a visual explanation, click here](https://www.youtube.com/watch?v=TzKANDzAXnQ)*


## Hiearchical Models (Iterated Moments)

>**Theorem 4.4 .3** and **Theorem 4.4 .7** If $X$ and $Y$ are any two random variables and the relevant expectations exist, then
$$
\begin{aligned}
E_X(X) & =E_Y\left(E_{X \mid Y}(X \mid Y)\right) \\
\operatorname{Var}_X(X) & =E_Y\left(\operatorname{Var}_{X \mid Y}(X \mid Y)\right)+\operatorname{Var}_Y\left(E_{X \mid Y}(X \mid Y)\right)
\end{aligned}
$$

## Convolutions

$$
\begin{array}{l|l}
f_{X_i} & f_{\Sigma X_i}\left(X_i \text { independent }\right) \\
\hline \operatorname{Ber}(p) & \operatorname{Bin}(n, p) \\
\operatorname{Bin}\left(n_i, p\right) & \operatorname{Bin}\left(\sum_i n_i, p\right) \\
\operatorname{Geo}(p) & \operatorname{NBin}(n, p) \\
\operatorname{Exp}(\beta) & \operatorname{Gam}(n, \beta) \quad \quad \text { Note: } E\left(X_i\right)=\frac{1}{\beta}, E\left(\sum X_i\right)=\frac{n}{\beta} \\
\operatorname{Gam}\left(n_i, \beta\right) & \operatorname{Gam}\left(\sum_i n_i, \beta\right) \\
\operatorname{Pois}\left(\lambda_i\right) & \operatorname{Pois}\left(\sum_i \lambda_i\right) \\
\chi^2(1) & \chi^2(n) \\
\chi^2\left(n_i\right) & \chi^2\left(\sum_i n_i\right) \\
\operatorname{Norm}\left(\mu_i, \sigma_i^2\right) & \operatorname{Norm}\left(\sum_i \mu_i, \sum_i \sigma_i^2\right)
\end{array}
$$
