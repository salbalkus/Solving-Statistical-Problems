# New Distributions {#new-distributions}

This chapter will demonstrate how to derive the PDF or CDF of a random variable that is a function of other random variables, including hierarchical models.

## Transformations

### Theorems

We'll start by introducing two useful theorems for finding the PDF of a transformed random variable $Y=g(X)$ where $g(X)$ is a monotone, one-to-one function over the domain of interest.

&nbsp;

::: {.definition name="Theorem 2.1.5"}
Let $X$ have pdf $f_X(x)$ and let $Y=g(X)$, where $g$ is a monotone function. Let $\mathcal{X}$ and $\mathcal{Y}$ be defined by (2.1.7). Suppose that $f_X(x)$ is continuous on $\mathcal{X}$ and that $g^{-1}(y)$ has a continuous derivative on $\mathcal{Y}$. Then the PDF of $Y$ is given by
$$
f_Y(y)= \begin{cases}f_X\left(g^{-1}(y)\right)\left|\frac{d}{d y} g^{-1}(y)\right| & y \in \mathcal{Y} \\ 0 & \text { otherwise}\end{cases}
$$
:::

A non-rigorous sketch of the proof of this follows from the [Direction Probability Manipulation](#direct-manipulation) of the CDF described in Chapter 2. Note

$$
F(Y < y) = P(Y < y) = P(g(X) < y) \\
= P(X < g^{-1}(y)) = F_x(g^{-1}(y))
$$
Then, $f(y) = \frac{d}{dy}F_Y(y) = f_X(g^{-1}(y)\cdot|\frac{d}{dy}g^{-1}(y)|$ by the chain rule (the absolute value is necessary to handle the case where $g$ is decreasing)

---

In some cases, $g(X)$ will only be a monotone, one-to-one function over subsets of the domain of interest. Below, **Theorem 2.1.8** provides a generalization of **Theorem 2.1.5** that can be applied in these situations. 

::: {.definition name="Theorem 2.1.8"}
Let $X$ have PDF $f_X(x)$, let $Y=g(X)$, and define the sample space $\mathcal{X}$ as in (2.1.7). Suppose there exists a partition, $A_0, A_1, \ldots, A_k$, of $\mathcal{X}$ such that $P\left(X \in A_0\right)=0$ and $f_X(x)$ is continuous on each $A_i$. Further, suppose there exist functions $g_1(x), \ldots, g_k(x)$, defined on $A_1, \ldots, A_k$, respectively, satisfying

1. $g(x)=g_i(x)$, for $x \in A_i$,
2. $g_i(x)$ is monotone on $A_i$,
3. the set $\mathcal{Y}=\left\{y: y=g_i(x)\right.$ for some $\left.x \in A_i\right\}$ is the same for each $i=1, \ldots, k$, and
4. $g_i^{-1}(y)$ has a continuous derivative on $\mathcal{Y}$, for each $i=1, \ldots, k$.
Then

$$
f_Y(y)= \begin{cases}\sum_{i=1}^k f_X\left(g_i^{-1}(y)\right)\left|\frac{d}{d y} g_i^{-1}(y)\right| & y \in \mathcal{Y} \\ 0 & \text { otherwise }\end{cases}
$$
:::

### Practical Strategy

Now, we will outline a practical strategy for solving problems that involve transformations. Given a transformation $y=g(x)$ (univariate) or $u,v=g(x,y)$ (multivariate), perform a transformation by following these steps:

**Step 1**: Define PMF/PDF of the untransformed random variable(s).

* Univariate: $f(x)$ (given) 
* Multivariate: calculate joint density $f(x,y)$ (not always given)

**Step 2**: Find the inverse.

* Univariate: $y=g(x) \quad \Rightarrow \quad x=g^{-1}(y)$
* Multivariate: $u=g(x,y), v=g(x,y) \quad \Rightarrow \quad x=g^{-1}(u,v),y=g^{-1}(u,v)$ 

*(Note: for multivariate case, solve system of equations)*

**Step 3**: Calculate jacobian of the transformation.

* For transformations of multiple random variables, recall bivariate Jacobian:
$$
|J|=\left|\begin{array}{ll}
\frac{\partial x}{\partial u} & \frac{\partial x}{\partial v} \\
\frac{\partial y}{\partial u} & \frac{\partial y}{\partial v}
\end{array}\right|=\left| \frac{\partial x}{\partial u} \frac{\partial y}{\partial v}-\frac{\partial y}{\partial u} \frac{\partial x}{\partial v} \right|
$$

**Step 4**: Calculate PMF/PDF of the transformed random variable.

$$
f_Y(y)= f_X\left(g^{-1}(y)\right)\left|\frac{d}{d y} g^{-1}(y)\right|
$$
$$
f_{U,V}(u,v)= f_{X,Y}\left(g^{-1}(u,v)\right) \left|\begin{array}{ll}
\frac{\partial x}{\partial u} & \frac{\partial x}{\partial v} \\
\frac{\partial y}{\partial u} & \frac{\partial y}{\partial v}
\end{array}\right|
$$


::: {.example name="Chi-squared density"}

Suppose $Z \sim N(0,1)$. What is the distribution of $Y = Z^2$?  

Here, $g$ is not monotone. If we want to proceed using the Transformation Theorem, we need split up the domain into areas where $g$ is monotone. This, trivially, is $z < 0$ and $z > 0$. Then,

$$g(z) = z^2 \implies z = g^{-1}(y) = \begin{cases}-\sqrt{y} & z < 0\\\sqrt{y} & z>0\end{cases}$$.

An intuition for why we need to sum the pdfs for non-monotone functions is that $Y = Z^2$ maps multiple values of $Z$ to the same value of $Y$. For example, $g(z)$ maps both $Z = -1$ and $Z = 1$ to $Y = 1$. Therefore, $f_Y(1)$ needs to include both the probability of $Z = -1$ and $Z = 1$.

Hence, we apply the Non-Monotone Transformation Theorem to obtain

$$
f_Y(y) = f_Z(\sqrt{y})\Big|\frac{d}{dy}\sqrt{y}\Big| + f_Z(-\sqrt{y})\Big|\frac{d}{dy}-\sqrt{y}\Big|\\
= \frac{1}{\sqrt{2\pi}}\cdot \exp(-\frac{1}{2}(\sqrt{y})^2\cdot\frac{1}{2\sqrt{y}} + \frac{1}{\sqrt{2\pi}}\cdot \exp(-\frac{1}{2}(-\sqrt{y})^2\cdot\frac{1}{2\sqrt{y}}\\
= \frac{1}{\sqrt{2\pi}y^\frac{1}{2}}\exp(-\frac{y}{2})
$$
which is a $\chi^2(1)$ distribution.

Alternatively, we could have proceeded directly using [Direct Probability Manipulation](#direct-manipulation) like so:

$$
F(y) = P(Y \leq y) = P(Z^2 \leq y)\\
= P(-\sqrt{y}< Z < \sqrt{y}) = \Phi(\sqrt{y}) - \Phi(-\sqrt{y})
$$
where $\Phi(z)$ is the normal cdf. Then,

$$
f_Y(y) = \frac{d}{dy}F_Y(y) = f_y(\sqrt{y})\cdot\frac{d}{dy}{\sqrt{y}} - f_y(\sqrt{y})\cdot(\frac{d}{dy}{-\sqrt{y}})\\
= \frac{1}{\sqrt{2\pi}y^\frac{1}{2}}\exp\Big(-\frac{y}{2}\Big)
$$
the same result as the theorem.

:::

### Proving Independence From a Joint Transformation

Some problems may involve proving that two transformations $U = g_1(x,y)$ and $V = g_2(x,y)$ are independent. To do this, we can first compute the joint distribution using the Transformation Theorem, and then factorize the joint distribution into components containing only $u$ and $v$ respectively.

::: {.example name="Independence From a Transformation"}

test

:::

## Probability Integral Transform

The probability integral transform is a powerful technique that allows for the simulation of random variables from any distribution, given that you have access to a source of uniformly distributed random numbers. This method is particularly useful because it provides a straightforward way to transform uniform random variables into random variables following a desired distribution. The general idea is as follows:

When you plug any continuous random variable $X$ into its own CDF, you get $U \sim$ Uniform(0,1):
$$
F_X(X) \sim \text{Uniform} (0,1)
$$

When you plug $U \sim$ Uniform(0,1) into an inverse CDF, you get a continuous random variable $X$ with that CDF:
$$
F_X^{-1}(U) \text{ has CDF } F_X(x)
$$

**Procedure**: Construct a random variable $X$ with a particular CDF $F_X(x)$

1. Generate $U \sim$ Uniform(0,1)
2. Plug $U$ into the inverse CDF: $X=F^{-1}_X(U)$
3. $X$ is distributed according to the CDF $F_X(x)$

&nbsp;

*[Click Here: Visual Explanation of Universality of the Uniform](https://www.youtube.com/watch?v=TzKANDzAXnQ)*

### Hiearchical Models (Iterated Moments)

::: {.definition name="Theorem 4.4.3 and Theorem 4.4.7"}
If $X$ and $Y$ are any two random variables and the relevant expectations exist, then
$$
\begin{aligned}
E_X(X) & =E_Y\left(E_{X \mid Y}(X \mid Y)\right) \\
\operatorname{Var}_X(X) & =E_Y\left(\operatorname{Var}_{X \mid Y}(X \mid Y)\right)+\operatorname{Var}_Y\left(E_{X \mid Y}(X \mid Y)\right)
\end{aligned}
$$
::: 

### Convolutions

Convolutions arise when we calculate the probability distribution of the sum or linear combination of independent random variables. In the following table, we provide a compilation of convolutions involving random variables from well-established probability distributions. Acquainting yourself with these convolutions can significantly improve the efficiency of computing moment generating functions and other statistical expressions of interest.

$$
\begin{array}{l|l}
f_{X_i} & f_{\Sigma X_i}\left(X_i \text { independent }\right) \\
\hline \operatorname{Ber}(p) & \operatorname{Bin}(n, p) \\
\operatorname{Bin}\left(n_i, p\right) & \operatorname{Bin}\left(\sum_i n_i, p\right) \\
\operatorname{Geo}(p) & \operatorname{NBin}(n, p) \\
\operatorname{Exp}(\beta) & \operatorname{Gam}(n, \beta) \quad \quad \text { Note: } E\left(X_i\right)=\frac{1}{\beta}, E\left(\sum X_i\right)=\frac{n}{\beta} \\
\operatorname{Gam}\left(n_i, \beta\right) & \operatorname{Gam}\left(\sum_i n_i, \beta\right) \\
\operatorname{Pois}\left(\lambda_i\right) & \operatorname{Pois}\left(\sum_i \lambda_i\right) \\
\chi^2(1) & \chi^2(n) \\
\chi^2\left(n_i\right) & \chi^2\left(\sum_i n_i\right) \\
\operatorname{Norm}\left(\mu_i, \sigma_i^2\right) & \operatorname{Norm}\left(\sum_i \mu_i, \sum_i \sigma_i^2\right)
\end{array}
$$

&nbsp;

Convolutions can also be computed directly as follows:

::: {.definition name="Theorem"}
Let $X$ and $Y$ be two independent continuous random variables with possible values $\mathcal{X}$ and $\mathcal{Y}$ and let $Z=X+Y$. Then, the probability density function of $Z$ is given by
$$
\begin{aligned}
f_Z(z) & =\int_{-\infty}^{+\infty} f_X(z-y) f_Y(y) \mathrm{d} y \\
\text { or } \quad f_Z(z) & =\int_{-\infty}^{+\infty} f_Y(z-x) f_X(x) \mathrm{d} x
\end{aligned}
$$
where $f_X(x), f_Y(y)$ and $f_Z(z)$ are the probability density functions of $X, Y$ and $Z$.
:::

*Tip: For convolutions, this theorem provides an alternative to the multivariate Jacobian above.*

&nbsp;

Lastly, we can apply the below theorem to **easily** compute the moment generating function and characteristic function of a convolution:

::: {.definition name="Theorem 4.2.12"}
Let $X$ and $Y$ be independent random variables with moment generating $M_X(t)$ and $M_Y(t)$. Then the moment generating and characteristic functions of the random variable $Z=X+Y$ are given by
$$
M_Z(t)=M_X(t) \cdot M_Y(t)
$$
$$
\varphi_Z(t) =\varphi_X(t) \cdot \varphi_Y(t) 
$$

For the random variable $Z = X-Y$, the moment generating and characteristic functions are given by:
$$
M_Z(t)=M_X(t) \cdot M_Y(-t)
$$
$$
\varphi_Z(t) =\varphi_X(t) \cdot \varphi_Y(-t) 
$$
:::















